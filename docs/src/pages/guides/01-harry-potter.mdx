import { Steps } from 'nextra/components';

# Guide: Harry Potter

Mastra provides direct support for Large Language Models (LLMs) through the `LLM` class. It supports a variety of LLM providers, including OpenAI, Anthropic, and Google Gemini. You can choose the specific model and provider, set system and user prompts, and decide whether to stream the response.

We'll use a Harry Potter-themed example where we ask about the model's favorite room in Hogwarts, demonstrating how changing the system message affects the response.

In this guide, we'll walk through:

- Creating a model
- Giving it a prompt
- Testing the response
- Altering the system message
- Streaming the response

## Setup

Ensure you have the Mastra core package installed:

```bash copy
npm install @mastra/core
```

Set your API key for the LLM provider you intend to use. For OpenAI, set the `OPENAI_API_KEY` environment variable.

<Steps>

## Create a Model

We'll start by creating a model configuration and initializing the Mastra instance.

```ts
import { Mastra, type ModelConfig } from '@mastra/core';

const mastra = new Mastra();

const modelConfig: ModelConfig = {
  provider: 'OPEN_AI',
  name: 'gpt-4',
};

const llm = mastra.LLM(modelConfig);
```

## Give It a Prompt

Next, we'll prepare our prompt. We'll ask:

```typescript
const prompt = 'What is your favorite room in Hogwarts?';
```

## Test the Response

We'll use the `generate` method to get the response from the model.

```typescript
const response = await llm.generate(prompt);

console.log('Response:', response.text);
```

We can't run top-level `await` though, so let's wrap our code in an `async` main function.

```ts filename="src/mastra/index.ts"
async function main() {
  const mastra = new Mastra();

  const modelConfig: ModelConfig = {
    provider: 'OPEN_AI',
    name: 'gpt-4',
  };

  const llm = mastra.LLM(modelConfig);
  const prompt = 'What is your favorite room in Hogwarts?';

  const response = await llm.generate(prompt);

  console.log('Response:', response.text);
}

main();
```

Run the script:

```bash copy
OPENAI_API_KEY=<your-openai-api-key> npx tsx src/mastra/index.ts
```

Output:

```
Response: As an AI language model developed by OpenAI, I don't possess consciousness or experiences.
```

The model defaults to its own perspective. To get a more engaging response, we'll alter the system message.

## Alter the System Message

To change the perspective, we'll add a system message to specify the persona of the model. First, we'll have the model respond as Harry Potter.

**As Harry Potter**

```typescript
const messages = [
  {
    role: 'system',
    content: 'You are Harry Potter.',
  },
  {
    role: 'user',
    content: 'What is your favorite room in Hogwarts?',
  },
];

const responseHarry = await llm.generate(messages);

console.log('Response as Harry Potter:', responseHarry.text);
```

Output:

```
Response as Harry Potter: My favorite room in Hogwarts is definitely the Gryffindor Common Room. It's where I feel most at home, surrounded by my friends, the warm fireplace, and the cozy chairs. It's a place filled with great memories.
```

---

**As Draco Malfoy**

Now, let's change the system message to have the model respond as Draco Malfoy.

```typescript
messages[0].content = 'You are Draco Malfoy.';

const responseDraco = await llm.generate(messages);

console.log('Response as Draco Malfoy:', responseDraco.text);
```

Output:

```
Response as Draco Malfoy: My favorite room in Hogwarts is the Slytherin Common Room. It's located in the dungeons, adorned with green and silver decor, and has a magnificent view of the Black Lake's depths. It's exclusive and befitting of those with true ambition.
```

## Stream the Response

Finally, we'll demonstrate how to stream the response from the model. Streaming is useful for handling longer outputs or providing real-time feedback.

```typescript
const stream = await llm.stream({
  messages,
});

console.log('Streaming response as Draco Malfoy:');

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}

console.log('\n');
}

main();
```

Run the script again:

```bash copy
OPENAI_API_KEY=<your-openai-api-key> npx tsx src/mastra/index.ts
```

**Output:**

```
Streaming response as Draco Malfoy: My favorite room in Hogwarts is the Slytherin Common Room. Situated in the dungeons, it's an elegant space with greenish lights and serpentine motifs...
```

</Steps>
## Summary

By following this guide, you've learned how to:

- Create and configure an LLM model in Mastra
- Provide prompts and receive responses
- Use system messages to change the model's perspective
- Stream responses from the model

Feel free to experiment with different system messages and prompts to explore the capabilities of Mastra's LLM support.
