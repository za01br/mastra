---
title: "LLM Class Overview | Mastra LLM Models"
description: Overview of the LLM class in Mastra, detailing its capabilities for interacting with various language models.
---

import { Callout } from "nextra/components";

# LLM Class Overview

Mastra provides direct support for Large Language Models (LLMs) through the `LLM` class. The `LLM` class allows you to interact with various language models seamlessly, enabling you to generate text, handle conversations, and more. This guide covers:

- How to initialize the LLM class.
- Supported models and providers.
- Using the `generate` function.
- Message formats in `generate`.
- Output formats in `generate`.

---

## Initializing the LLM Class

To start using the `LLM` class, you need to initialize it with the desired model configuration. Here's how you can do it:

```typescript
import { Mastra } from "@mastra/core";

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: "OPEN_AI",
  name: "gpt-4o-mini",
});
```

This initialization allows telemetry to pass through to the LLM, providing insights into model usage and performance.

**Note:** You can find more details about the model configuration options in the [Providers and Models reference](../reference/llm/providers-and-models.mdx).

---

## Supported Models and Providers

Mastra supports major LLM providers out of the box, plus additional providers through AI SDK integrations. Custom providers can also be added via the Portkey service.

### Most Popular Providers and Models

| Provider          | Supported Models                                                                                                                                                         |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **OpenAI**        | `gpt-4`, `gpt-4-turbo`, `gpt-3.5-turbo`, `gpt-4o`, `gpt-4o-mini`                                                                                                         |
| **Anthropic**     | `claude-3-5-sonnet-20241022`, `claude-3-5-sonnet-20240620`, `claude-3-5-haiku-20241022`, `claude-3-opus-20240229`, `claude-3-sonnet-20240229`, `claude-3-haiku-20240307` |
| **Google Gemini** | `gemini-1.5-pro-latest`, `gemini-1.5-pro`, `gemini-1.5-flash-latest`, `gemini-1.5-flash`                                                                                 |

A full list of supported models can be found [here](../reference/llm/providers-and-models.mdx).

<Callout>
  {" "}
  If you don't want to pay for an LLM provider, Google Gemini has a generous
  free tier for its API.
</Callout>

---

## The `generate` Function

The main function you'll use with the `LLM` class is `generate`. It allows you to send messages to the language model and receive responses. The `generate` function takes:

- **messages**: The first parameter, which can be a string, an array of strings, or an array of message objects.
- **options**: The second parameter, which includes additional configurations like streaming, schemas for structured output, etc.

This design covers all potential use cases and is extensible to multi-modal interactions in the future.

---

## Message Formats in `generate`

The `generate` function supports three types of message formats:

### 1. Simple String

You can pass a single string as the message:

```typescript
const response = await llm.generate("Tell me a joke.");
```

### 2. Array of Strings

You can provide an array of strings, which will be converted into user messages:

```typescript
const response = await llm.generate([
  "Hello!",
  "Can you explain quantum mechanics?",
]);
```

### 3. Detailed Message Objects

For finer control, you can pass an array of message objects, specifying the role and content:

```typescript
const response = await llm.generate([
  { role: "system", content: "You are a helpful assistant." },
  { role: "user", content: "What is the meaning of life?" },
]);
```

---

## Output Formats in `generate`

The `generate` function supports four types of output formats:

### 1. Simple Text Generation

Receive a basic text response from the model:

```typescript
const response = await llm.generate("What is AI?");

console.log(response.text);
```

### 2. Structured Output

Request a structured response by providing a schema. This is useful when you need the output in a specific format:

```typescript
import { z } from "zod";

const mySchema = z.object({
  definition: z.string(),
  examples: z.array(z.string()),
});

const response = await llm.generate(
  "Define machine learning and give examples.",
  {
    output: mySchema,
  },
);

console.log(response.object);
```

### 3. Streaming Text

Stream the response in real-time, which is useful for handling longer outputs or providing immediate feedback to users:

```typescript
const stream = await llm.stream("Tell me a story about a brave knight.");

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
```

### 4. Streaming Structured Output

Stream a structured response using a schema:

```typescript
const stream = await llm.stream("Provide live weather data.", {
  output: mySchema,
});

for await (const chunk of stream.textStream) {
  console.log(chunk);
}
```

---

## Additional Notes

- **Telemetry**: Initializing the `LLM` class through Mastra allows telemetry data to pass through, enabling better monitoring and debugging.
- **Extensibility**: The design of the `generate` function and message formats makes it future-proof and extensible for multi-modal interactions.
