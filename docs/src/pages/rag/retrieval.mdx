## Step 4: Querying the Knowledge Base

When a user asks a question, embed their query and retrieve relevant chunks:

```typescript showLineNumbers copy
import { embed } from '@mastra/rag';

const { embedding } = await embed('What are the main points in the article?', {
  provider: 'openai',
  model: 'text-embedding-ada-002',
});

const results = await PgVector.query({
  connectionString: 'postgresql://localhost:5432/mydb',
  tableName: 'embeddings',
  queryEmbedding: embedding,
  topK: 5,
});

console.log(results);
// Example structure:
// [
//   { text: "A chunk related to main points...", metadata: { title: "...", ... } },
//   ...
// ]

// Incorporate these results into your LLM prompt
const prompt = `
  Using the context below, answer the user's question.

  Context:
  ${results.map(r => r.text).join('\n')}

  User's question: What are the main points in the article?
`;

// Send `prompt` to your LLM model for a more informed and grounded response.
```
