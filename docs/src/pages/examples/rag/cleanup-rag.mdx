---
title: "Example: Optimizing Information Density | RAG | Mastra Docs"
description: Example of implementing a RAG system in Mastra to optimize information density and deduplicate data using LLM-based processing.
---

import { GithubLink } from "../../../components/github-link";

# Optimizing Information Density

This example demonstrates how to implement a Retrieval-Augmented Generation (RAG) system using Mastra, OpenAI embeddings, and PGVector for vector storage.
The system uses an agent to clean the initial chunks to optimize information density and deduplicate data.

## Overview

The system implements RAG using Mastra and OpenAI, this time optimizing information density through LLM-based processing. Here's what it does:

1. Sets up two Mastra agents with gpt-4o-mini for response generation
2. Sets up another Mastra agent to handle cleaning up chunk data before vector storage
3. Creates a vector query tool to manage vector store interactions
4. Create a document chunking tool for agent to use to get chunks
5. Chunks text documents into smaller segments
6. Takes those chunks and filters them to remove irrelevant or duplicate information
7. Creates embeddings for both the initial chunks and the updated chunks
8. Stores them both in a PostgreSQL vector database
9. Retrieves relevant chunks based on queries using vector query tool
10. Generates context-aware responses using the Mastra agents

## Setup

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
OPENAI_API_KEY=your_openai_api_key_here
POSTGRES_CONNECTION_STRING=your_connection_string_here
```

### Dependencies

Then, import the necessary dependencies:

```typescript copy showLineNumbers filename="src/mastra/index.ts"
import { openai } from '@ai-sdk/openai';
import { Mastra } from "@mastra/core";
import { Agent } from "@mastra/core/agent";
import { PgVector } from "@mastra/pg";
import {
  MDocument,
  createVectorQueryTool,
  createDocumentChunkerTool,
} from "@mastra/rag";
import { embedMany } from "ai";
```

## Tool Creation

### Vector Query Tool

Using createVectorQueryTool imported from @mastra/rag, you can create a tool that can query the vector database.

```typescript copy showLineNumbers{12} filename="src/mastra/index.ts"
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: "pgVector",
  indexName: "embeddings",
  model: openai.embedding('text-embedding-3-small'),
});

const cleanedVectorQueryTool = createVectorQueryTool({
  vectorStoreName: "pgVector",
  indexName: "cleanedEmbeddings",
  model: openai.embedding('text-embedding-3-small'),
});
```

### Chunk Tool

Using createDocumentChunkerTool imported from @mastra/rag, you can create a tool that chunks the document and sends the chunks to your agent.

```typescript copy showLineNumbers{24} filename="src/mastra/index.ts"
const doc = MDocument.fromText(yourText);

const documentChunkerTool = createDocumentChunkerTool({
  doc,
  params: {
    strategy: "recursive",
    size: 256,
    overlap: 50,
    separator: "\n",
  },
});
```

## Agent Configuration

Set up three Mastra agents:

```typescript copy showLineNumbers{36} filename="src/mastra/index.ts"
export const ragAgentOne = new Agent({
  name: "RAG Agent One",
  instructions:
    "You are a helpful assistant that answers questions based on the provided context. Keep your answers concise and relevant.",
  model: openai('gpt-4o-mini'),
  tools: {
    vectorQueryTool,
  },
});

export const ragAgentTwo = new Agent({
  name: "RAG Agent Two",
  instructions:
    "You are a helpful assistant that answers questions based on the provided context. Keep your answers concise and relevant.",
  model: openai('gpt-4o-mini'),
  tools: {
    cleanedVectorQueryTool,
  },
});

export const ragAgentThree = new Agent({
  name: "RAG Agent Three",
  instructions:
    "You are a helpful assistant that processes, cleans, and labels data before storage.",
  model: openai('gpt-4o-mini'),
  tools: { documentChunkerTool },
});
```

## Instantiate PgVector and Mastra

Instantiate PgVector and Mastra with all components:

```typescript copy showLineNumbers{64} filename="src/mastra/index.ts"
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);

export const mastra = new Mastra({
  agents: { ragAgentOne, ragAgentTwo, ragAgentThree },
  vectors: { pgVector },
});
const dataAgentOne = mastra.getAgent("ragAgentOne");
const dataAgentTwo = mastra.getAgent("ragAgentTwo");
const processAgent = mastra.getAgent("ragAgentThree");
```

## Document Processing

Chunk the initial document clean them using the processAgent.

```typescript copy showLineNumbers{74} filename="src/mastra/index.ts"
const chunks = await doc.chunk({
  strategy: "recursive",
  size: 256,
  overlap: 50,
  separator: "\n",
});

const chunkPrompt = `Take the chunks returned from the tool and clean them up according to the instructions provided. Make sure to filter out irrelevant information and remove duplicates.`;

const newChunks = await processAgent.generate(chunkPrompt);
const updatedDoc = MDocument.fromText(newChunks.text);

const updatedChunks = await updatedDoc.chunk({
  strategy: "recursive",
  size: 256,
  overlap: 50,
  separator: "\n",
});
```

## Creating and Storing Embeddings

Generate and store both raw and cleaned embeddings:

```typescript copy showLineNumbers{93} filename="src/mastra/index.ts"
const { embeddings } = await embedMany({
  model: openai.embedding('text-embedding-3-small'),
  values: chunks.map(chunk => chunk.text),
});

const { embeddings: cleanedEmbeddings } = await embedMany({
  model: openai.embedding('text-embedding-3-small'),
  values: updatedChunks.map(chunk => chunk.text),
});

const vectorStore = mastra.getVector("pgVector");
await vectorStore.createIndex({
  indexName: "embeddings",
  dimension: 1536,
});
await vectorStore.createIndex({
  indexName: "cleanedEmbeddings",
  dimension: 1536,
});

await vectorStore.upsert({
  indexName: "embeddings",
  vectors: embeddings,
  metadata: chunks?.map((chunk: any) => ({ text: chunk.text })),
});

await vectorStore.upsert({
  indexName: "cleanedEmbeddings",
  vectors: cleanedEmbeddings,
  metadata: updatedChunks?.map((chunk: any) => ({ text: chunk.text })),
});
```

## Response Generation

Function to generate responses with index selection:

```typescript copy showLineNumbers{121} filename="src/mastra/index.ts"
async function generateResponse(query: string, agent: Agent) {
  const prompt = `
      Please answer the following question:
      ${query}

      Please base your answer only on the context provided in the tool with this index ${index}. 
      If the context doesn't contain enough information to fully answer the question, 
      please state that explicitly. 
      `;
  // Call the agent to generate a response
  const completion = await agent.generate(prompt);
  return completion.text;
}
```

## Example Usage

```typescript copy showLineNumbers{135} filename="src/mastra/index.ts"
async function answerQueries(queries: string[], agent: Agent) {
  for (const query of queries) {
    try {
      const answer = await generateResponse(query, agent);
      console.log("\nQuery:", query);
      console.log("Response:", answer);
    } catch (error) {
      console.error(`Error processing query "${query}":`, error);
    }
  }
}

const queries = [
  "What is the average temperature on Mars?",
  "What technologies are used in modern spacecraft?",
  "What are all the requirements for space settlements?",
  "What are all the dates mentioned related to space stations?",
  "What are all the mentions of sustainability in space settlements?",
];

// Compare responses between raw and cleaned embeddings
await answerQueries(queries, dataAgentOne);
await answerQueries(queries, dataAgentTwo);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/cleanup-rag"
  }
/>
