Combined MDX Documentation
=========================
Total files processed: 124

Files included:
- src/pages/docs/08-running-evals.mdx
- src/pages/docs/agents/00-overview.mdx
- src/pages/docs/agents/01-agent-memory.mdx
- src/pages/docs/agents/02-adding-tools.mdx
- src/pages/docs/deployment/deployment.mdx
- src/pages/docs/deployment/logging-and-tracing.mdx
- src/pages/docs/getting-started/installation.mdx
- src/pages/docs/getting-started/project-structure.mdx
- src/pages/docs/guides/01-harry-potter.mdx
- src/pages/docs/guides/02-chef-michel.mdx
- src/pages/docs/guides/03-stock-agent.mdx
- src/pages/docs/guides/04-recruiter.mdx
- src/pages/docs/index.mdx
- src/pages/docs/llm-models/00-overview.mdx
- src/pages/docs/local-dev/engine.mdx
- src/pages/docs/local-dev/integrations.mdx
- src/pages/docs/local-dev/mastra-dev.mdx
- src/pages/docs/local-dev/mastra-init.mdx
- src/pages/docs/rag/chunking-and-embedding.mdx
- src/pages/docs/rag/overview.mdx
- src/pages/docs/rag/retrieval.mdx
- src/pages/docs/rag/vector-databases.mdx
- src/pages/docs/reference/agents/createTool.mdx
- src/pages/docs/reference/agents/generate.mdx
- src/pages/docs/reference/agents/getAgent.mdx
- src/pages/docs/reference/agents/stream.mdx
- src/pages/docs/reference/cli/deploy.mdx
- src/pages/docs/reference/cli/dev.mdx
- src/pages/docs/reference/cli/engine.mdx
- src/pages/docs/reference/cli/init.mdx
- src/pages/docs/reference/core/mastra-class.mdx
- src/pages/docs/reference/llm/generate.mdx
- src/pages/docs/reference/llm/providers-and-models.mdx
- src/pages/docs/reference/llm/stream.mdx
- src/pages/docs/reference/mcp/client.mdx
- src/pages/docs/reference/observability/combine-loggers.mdx
- src/pages/docs/reference/observability/create-logger.mdx
- src/pages/docs/reference/observability/logger.mdx
- src/pages/docs/reference/observability/otel-config.mdx
- src/pages/docs/reference/observability/providers/braintrust.mdx
- src/pages/docs/reference/observability/providers/index.mdx
- src/pages/docs/reference/observability/providers/laminar.mdx
- src/pages/docs/reference/observability/providers/langfuse.mdx
- src/pages/docs/reference/observability/providers/langsmith.mdx
- src/pages/docs/reference/observability/providers/langwatch.mdx
- src/pages/docs/reference/observability/providers/new-relic.mdx
- src/pages/docs/reference/observability/providers/signoz.mdx
- src/pages/docs/reference/observability/providers/traceloop.mdx
- src/pages/docs/reference/rag/chunk.mdx
- src/pages/docs/reference/rag/document.mdx
- src/pages/docs/reference/rag/embeddings.mdx
- src/pages/docs/reference/rag/graph-rag.mdx
- src/pages/docs/reference/rag/pgstore.mdx
- src/pages/docs/reference/rag/pinecone.mdx
- src/pages/docs/reference/rag/qdrant.mdx
- src/pages/docs/reference/rag/reranker.mdx
- src/pages/docs/reference/tools/document-chunker-tool.mdx
- src/pages/docs/reference/tools/graph-rag-tool.mdx
- src/pages/docs/reference/tools/vector-query-tool.mdx
- src/pages/docs/reference/tts/generate.mdx
- src/pages/docs/reference/tts/providers-and-models.mdx
- src/pages/docs/reference/tts/stream.mdx
- src/pages/docs/reference/workflows/after.mdx
- src/pages/docs/reference/workflows/commit.mdx
- src/pages/docs/reference/workflows/createRun.mdx
- src/pages/docs/reference/workflows/resume.mdx
- src/pages/docs/reference/workflows/start.mdx
- src/pages/docs/reference/workflows/step-class.mdx
- src/pages/docs/reference/workflows/step-condition.mdx
- src/pages/docs/reference/workflows/step-function.mdx
- src/pages/docs/reference/workflows/step-options.mdx
- src/pages/docs/reference/workflows/then.mdx
- src/pages/docs/reference/workflows/workflow.mdx
- src/pages/docs/workflows/00-overview.mdx
- src/pages/docs/workflows/control-flow.mdx
- src/pages/docs/workflows/steps.mdx
- src/pages/docs/workflows/suspend-and-resume.mdx
- src/pages/examples/agents/agentic-workflows.mdx
- src/pages/examples/agents/bird-checker.mdx
- src/pages/examples/agents/hierarchical-multi-agent.mdx
- src/pages/examples/agents/multi-agent-workflow.mdx
- src/pages/examples/agents/system-prompt.mdx
- src/pages/examples/agents/using-a-tool.mdx
- src/pages/examples/index.mdx
- src/pages/examples/llms/call-claude.mdx
- src/pages/examples/llms/call-google-gemini.mdx
- src/pages/examples/llms/describe-an-image.mdx
- src/pages/examples/llms/generate-object-with-structured-output.mdx
- src/pages/examples/llms/generate-text-from-pdf.mdx
- src/pages/examples/llms/generate-text.mdx
- src/pages/examples/llms/stream-object-with-structured-output.mdx
- src/pages/examples/llms/stream-text.mdx
- src/pages/examples/llms/use-a-system-prompt.mdx
- src/pages/examples/rag/adjust-chunk-delimiters.mdx
- src/pages/examples/rag/adjust-chunk-size.mdx
- src/pages/examples/rag/basic-rag.mdx
- src/pages/examples/rag/chunk-html.mdx
- src/pages/examples/rag/chunk-json.mdx
- src/pages/examples/rag/chunk-markdown.mdx
- src/pages/examples/rag/chunk-text.mdx
- src/pages/examples/rag/cleanup-rag.mdx
- src/pages/examples/rag/cot-rag.mdx
- src/pages/examples/rag/cot-workflow-rag.mdx
- src/pages/examples/rag/embed-chunk-array.mdx
- src/pages/examples/rag/embed-text-chunk.mdx
- src/pages/examples/rag/embed-text-with-cohere.mdx
- src/pages/examples/rag/filter-rag.mdx
- src/pages/examples/rag/graph-rag.mdx
- src/pages/examples/rag/insert-embedding-in-pgvector.mdx
- src/pages/examples/rag/insert-embedding-in-pinecone.mdx
- src/pages/examples/rag/rerank-rag.mdx
- src/pages/examples/rag/reranking-with-cohere.mdx
- src/pages/examples/rag/retrieve-results.mdx
- src/pages/examples/workflows/branching-paths.mdx
- src/pages/examples/workflows/calling-agent.mdx
- src/pages/examples/workflows/calling-llm.mdx
- src/pages/examples/workflows/creating-a-workflow.mdx
- src/pages/examples/workflows/cyclical-dependencies.mdx
- src/pages/examples/workflows/parallel-steps.mdx
- src/pages/examples/workflows/sequential-steps.mdx
- src/pages/examples/workflows/suspend-and-resume.mdx
- src/pages/examples/workflows/using-a-tool-as-a-step.mdx
- src/pages/showcase/index.mdx

================================================================================


================================================================================
Source: src/pages/docs/08-running-evals.mdx
================================================================================

# Running Evals

Evals are automated tests that evaluate LLM outputs using model-graded, rule-based, and statistical methods. Each eval returns a normalized score between 0-1 that can be logged and compared. Evals can be customized with your own prompts and scoring functions.

Evals suites run in the cloud, but as tests, it's logical to store them in your codebase. Because LLMs are non-deterministic, you might not get a 100% pass rate every time.

Mastra recommends using Braintrust's eval framework, [autoevals](https://github.com/braintrustdata/autoevals), to run evals. They have a free tier that should be enough for most use cases.

Other open-source eval frameworks:

- [Laminar](https://www.npmjs.com/package/@lmnr-ai/lmnr)
- [PromptFoo](https://www.npmjs.com/package/promptfoo)


================================================================================
Source: src/pages/docs/agents/00-overview.mdx
================================================================================

# Agents Overview

Agents in Mastra are systems where the language model can autonomously decide on a sequence of actions to perform tasks. They have access to tools, workflows, and synced data, enabling them to perform complex tasks and interact with external systems. Agents can invoke your custom functions, utilize third-party APIs through integrations, and access knowledge bases you have built.

While the `LLM` class is similar to a contractor you might hire for a one-off task, agents are like employees who can be used for ongoing projects. They have names, persistent memory, consistent model configurations, and instructions across calls, as well as a set of enabled tools.

## 1. Creating an Agent

To create an agent in Mastra, you use the `Agent` class and define its properties:

```ts showLineNumbers filename="src/mastra/agents/index.ts" copy
import { Agent } from "@mastra/core";

export const myAgent = new Agent({
  name: "My Agent",
  instructions: "You are a helpful assistant.",
  model: {
    provider: "OPEN_AI",
    name: "gpt-4o-mini",
  },
});
```

**Note:** Ensure that you have set the necessary environment variables, such as your OpenAI API key, in your `.env` file:

```.env filename=".env" copy
OPENAI_API_KEY=your_openai_api_key
```

Also, make sure you have the `@mastra/core` package installed:

```bash npm2yarn copy
npm install @mastra/core
```

### Registering the Agent

Register your agent with Mastra to enable logging and access to configured tools and integrations:

```ts showLineNumbers filename="src/mastra/index.ts" copy
import { Mastra } from "@mastra/core";
import { myAgent } from "./agents";

export const mastra = new Mastra({
  agents: { myAgent },
});
```

## 2. Generating and streaming text

### Generating text

Use the `.generate()` method to have your agent produce text responses:

```ts showLineNumbers filename="src/mastra/index.ts" copy
const response = await myAgent.generate([
  { role: "user", content: "Hello, how can you assist me today?" },
]);

console.log("Agent:", response.text);
```

### Streaming responses

For more real-time responses, you can stream the agent's response:

```ts showLineNumbers filename="src/mastra/index.ts" copy
const stream = await myAgent.stream([
  { role: "user", content: "Tell me a story." },
]);

console.log("Agent:");

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
```

## **3. Structured Output**

Agents can return structured data by providing a JSON Schema or using a Zod schema.

### Using JSON Schema

```typescript
const schema = {
  type: "object",
  properties: {
    summary: { type: "string" },
    keywords: { type: "array", items: { type: "string" } },
  },
  additionalProperties: false,
  required: ["summary", "keywords"],
};

const response = await myAgent.generate(
  [
    {
      role: "user",
      content:
        "Please provide a summary and keywords for the following text: ...",
    },
  ],
  {
    output: schema,
  }
);

console.log("Structured Output:", response.object);
```

### Using Zod

You can also use Zod schemas for type-safe structured outputs.

First, install Zod:

```bash npm2yarn copy
npm install zod
```

Then, define a Zod schema and use it with the agent:

```ts showLineNumbers filename="src/mastra/index.ts" copy
import { z } from "zod";

// Define the Zod schema
const schema = z.object({
  summary: z.string(),
  keywords: z.array(z.string()),
});

// Use the schema with the agent
const response = await myAgent.generate(
  [
    {
      role: "user",
      content:
        "Please provide a summary and keywords for the following text: ...",
    },
  ],
  {
    output: schema,
  }
);

console.log("Structured Output:", response.object);
```

This allows you to have strong typing and validation for the structured data returned by the agent.

## **4. Running Agents**

Mastra provides a CLI command `mastra dev` to run your agents behind an API. By default, this looks for exported agents in files in the `src/mastra/agents` directory.

### Starting the Server

```bash
mastra dev
```

This will start the server and make your agent available at `http://localhost:4111/api/agents/myAgent/generate`.

### Interacting with the Agent

You can interact with the agent using `curl` from the command line:

```bash
curl -X POST http://localhost:4111/api/agents/myAgent/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      { "role": "user", "content": "Hello, how can you assist me today?" }
    ]
  }'
```

## Next Steps

- Learn about Agent Memory in the [Agent Memory](./01-agent-memory.mdx) guide.
- Learn about Agent Tools in the [Agent Tools](./02-adding-tools.mdx) guide.
- See an example agent in the [Chef Michel](./02-chef-michel.mdx) example.


================================================================================
Source: src/pages/docs/agents/01-agent-memory.mdx
================================================================================

# Agent Memory

Agents in Mastra have a memory that stores conversation history and contextual information. This memory allows agents to maintain state across interactions, enabling more coherent and context-aware responses.

There are two main ways to implement agent memory in Mastra:

1. **Using KV Upstash**
2. **Using External Storage (Redis or PostgreSQL)**

## 1. Using KV Upstash

KV Upstash provides a serverless Redis-compatible key-value store that's perfect for agent memory storage. Here's how to set it up:

First, install the necessary package:

```bash
npm install @mastra/memory
```

Create an Upstash account and database:

1. Go to https://upstash.com/ and sign up for an account
2. Create a new database in your preferred region
3. Copy your REST URL and TOKEN from the database details page

Then, configure your agent to use Upstash KV:

```typescript
// src/mastra/index.ts
import { Mastra } from "@mastra/core";
import { UpstashKVMemory } from "@mastra/memory";

import { myAgent } from "./agents";

// Create UpstashKVMemory
const upstashKVMemory = new UpstashKVMemory({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!,
});

export const mastra = new Mastra({
  memory: upstashKVMemory,
  agents: { myAgent },
});
```

Add your Upstash credentials to your environment variables:

```env
# .env
UPSTASH_REDIS_REST_URL=https://your-url.upstash.io
UPSTASH_REDIS_REST_TOKEN=your_token
```

## 2. Using External Storage

For production environments where you need persistent memory across restarts and scalability, you can use a external storage backend like PostgreSQL.

### Using PostgreSQL for Agent Memory

First, install the necessary packages:

```bash npm2yarn copy
npm install @mastra/memory
```

Then, configure your agent to use PostgreSQL for memory storage:

```typescript
// src/mastra/index.ts
import { Mastra } from "@mastra/core";
import { PostgresMemory } from "@mastra/memory";

import { myAgent } from "./agents";

// Create a PgMemory using connectionString
const pgMemory = new PgMemory({
  connectionString: process.env.POSTGRES_CONNECTION_STRING!,
});

export const mastra = new Mastra({
  memory: pgMemory,
  agents: { myAgent },
});
```

Ensure you have your PostgreSQL connection string set in your environment variables:

```env
# .env
POSTGRES_CONNECTION_STRING=postgresql://user:password@localhost:5432/your_database
```

## Managing Agent Memory

Once your agent is configured with a memory backend, you can manage its memory using the following methods.

### Creating a Thread

First, create a new conversation thread for your agent:

```typescript
const thread = await mastra.memory.createThread({
  resourceId: "memory_id"
});
```

This creates a new thread and assigns it a unique identifier. The `resourceId` parameter helps identify which memory context this thread belongs to.

### Save Messages on Generate

Generate responses while maintaining conversation context. 
The `threadId` parameter links messages to a specific conversation thread, while `resourceId` associates the conversation with a particular memory context or resource:

```typescript
const responseOne = await myAgent.generate("Tell me about the project requirements.", {
  resourceId: "memory_id",
  threadId: thread.id,
});

const responseTwo = await myAgent.generate("What are the next steps?", {
  resourceId: "memory_id",
  threadId: thread.id,
});
```
Each generate call automatically preserves the conversation context in memory.

### Saving Messages to Memory

You can explicitly save messages to the thread from the agent.

```typescript
await myAgent.saveMemory({
  threadId: thread.id,
  resourceId: "memory_id",
  userMessages: [
    {
      role: "user",
      content: "What are the main performance bottlenecks?",
    },
  ],
});
```

This method enables you to manually preserve specific messages in the thread's memory. 

### Retrieving Messages From Memory

To check the current messages in memory at any point:

```typescript
const messages = await mastra.memory.getMessages({
  threadId: thread.id,
});
```

Retrieves all messages from the specified thread in chronological order.

### Retrieving Context Window

You can use memory to retrieve messages within a specific time range:

```typescript
const messages = await mastra.memory.getContextWindow({
  threadId: thread.id,
  startDate: new Date('2024-01-01'),
  endDate: new Date('2024-01-31'),
  format: 'raw'
});
```

You can optionally limit the total tokens in the context window by configuring maxTokens in your memory:

```typescript
const upstashKVMemory = new UpstashKVMemory({
  url: process.env.UPSTASH_REDIS_REST_URL!,
  token: process.env.UPSTASH_REDIS_REST_TOKEN!,
  maxTokens: 4000 // Limit total tokens in context window
});
```

When maxTokens is set, messages are filtered by date range and then processed from newest to oldest until the token limit is reached. 
This maintains the context window size while preserving the most recent conversation history.

### Deleting a Thread

To remove a thread and all its associated messages:

```typescript
await mastra.memory.deleteThread(thread.id);
```

Permanently deletes the thread and its messages.


================================================================================
Source: src/pages/docs/agents/02-adding-tools.mdx
================================================================================

# Adding Tools

Tools are typed functions that can be executed by agents or workflows, with built-in integration access and parameter validation. Each tool has a schema that defines its inputs, an executor function that implements its logic, and access to configured integrations.

## Creating Tools

In this section, we'll walk through the process of creating a tool that can be used by your agents. Let's create a simple tool that fetches current weather information for a given city.

```typescript filename="src/mastra/tools/weatherInfo.ts" copy
import { createTool } from "@mastra/core";
import { z } from "zod";

const getWeatherInfo = async (city: string) => {
  // Replace with an actual API call to a weather service
  const data = await fetch(`https://api.example.com/weather?city=${city}`).then(
    (r) => r.json(),
  );
  return data;
};

export const weatherInfo = createTool({
  id "Get Weather Information",
  inputSchema: z.object({
    city: z.string(),
  }),
  description: `Fetches the current weather information for a given city`,
  execute: async ({ context: { city } }) => {
    console.log("Using tool to fetch weather information for", city);
    return await getWeatherInfo(city);
  },
});
```

## Adding Tools to an Agent

Now we'll add the tool to an agent. We'll create an agent that can answer questions about the weather and configure it to use our `weatherInfo` tool.

```typescript filename="src/mastra/agents/weatherAgent.ts"
import { Agent } from "@mastra/core";
import * as tools from "../tools/weatherInfo";

export const weatherAgent = new Agent<typeof tools>({
  name: "Weather Agent",
  instructions:
    "You are a helpful assistant that provides current weather information. When asked about the weather, use the weather information tool to fetch the data.",
  model: {
    provider: "OPEN_AI",
    name: "gpt-4",
    toolChoice: "required",
  },
  tools: {
    weatherInfo: tools.weatherInfo,
  },
});
```

## Registering the Agent

We need to initialize Mastra with our agent.

```typescript filename="src/index.ts"
import { Mastra } from "@mastra/core";
import { weatherAgent } from "./agents/weatherAgent";

export const mastra = new Mastra({
  agents: { weatherAgent },
});
```

This registers your agent with Mastra, making it available for use.

## Debugging Tools

You can test tools using Vitest or any other testing framework. Writing unit tests for your tools ensures they behave as expected and helps catch errors early.

## Calling an Agent with a Tool

Now we can call the agent, and it will use the tool to fetch the weather information.

## Example: Interacting with the Agent

```typescript filename="src/index.ts"
import { mastra } from "./index";

async function main() {
  const agent = mastra.getAgent("weatherAgent");
  const response = await agent.generate(
    "What's the weather like in New York City today?",
  );

  console.log(response.text);
}

main();
```

The agent will use the `weatherInfo` tool to get the current weather in New York City and respond accordingly.


================================================================================
Source: src/pages/docs/deployment/deployment.mdx
================================================================================

# Deployment

In this guide, we'll cover how to deploy your Mastra agents and workflows using the `mastra deploy` command. Once deployed, your agents and workflows will be available via REST endpoints.

Currently, the Mastra CLI supports deploying to **Vercel** and **Cloudflare Workers**.

## Prerequisites

Before you begin, ensure you have the following:

- **Node.js** installed (version 18 or higher is recommended).
- **Mastra CLI** installed globally:

```bash copy
npm install -g mastra
```

- An existing Mastra project set up. If not, initialize one using:

```bash copy
mastra init
```

- A Vercel account (if deploying to Vercel) or a Cloudflare account (if deploying to Cloudflare Workers).

---

## Deploying to Vercel

[Vercel](https://vercel.com/) is a cloud platform for static sites and serverless functions. Mastra can be easily deployed to Vercel using the CLI. 

### Steps to Deploy

1. **Install the Vercel CLI** (if you haven't already):

```bash copy
npm install -g vercel
```

2. **Login to Vercel**:

```bash copy
vercel login
```

3. **Deploy with Mastra CLI**:

Navigate to your project directory and run:

```bash copy
mastra deploy vercel
```

The CLI will guide you through the deployment process, including:

- **Authentication**: If you haven't provided a Vercel token before, you'll be prompted to enter one.
- **Team Selection**: Choose the Vercel team or scope to deploy under.
- **Environment Variables**: Set any required environment variables (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`).

4. **Set Environment Variables on Vercel** (Optional):

If you didn't set environment variables during deployment, you can configure them in the Vercel dashboard:

- Go to your Vercel project.
- Navigate to **Settings > Environment Variables**.
- Add your variables (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`).

### Example

```bash copy
cd your-mastra-project
mastra deploy vercel
```

Follow the prompts provided by the CLI to complete the deployment.

---

## Deploying to Cloudflare Workers

[Cloudflare Workers](https://workers.cloudflare.com/) allow you to deploy serverless applications to Cloudflare's edge network.

### Steps to Deploy

1. **Install the Wrangler CLI**:

```bash copy
npm install -g wrangler
```

2. **Login to Cloudflare**:

```bash copy
wrangler login
```

3. **Deploy with Mastra CLI**:

Navigate to your project directory and run:

```bash copy
mastra deploy cloudflare
```

The CLI will guide you through the process, including:

- **Authentication**: If you haven't logged in, you'll be prompted to do so.
- **Project Configuration**: The CLI will generate a `wrangler.toml` file with necessary settings.
- **Environment Variables**: Set any required variables in the `wrangler.toml` file or via the Cloudflare dashboard.

4. **Set Environment Variables in `wrangler.toml`**:

Edit the `wrangler.toml` file in your project directory to include your environment variables:

```toml
[vars]
OPENAI_API_KEY = "your-openai-api-key"
ANTHROPIC_API_KEY = "your-anthropic-api-key"
```

### Example

```bash copy
cd your-mastra-project
mastra deploy cloudflare
```

Follow the prompts provided by the CLI to complete the deployment.

---

## Additional Tips

### Ensure Your `mastra` Instance is Exported Properly

Make sure that your `mastra` instance is exported in your entry file (e.g., `src/mastra/index.ts`):

```typescript:src/mastra/index.ts
import { Mastra } from '@mastra/core';

export const mastra = new Mastra({
  // Your configuration here
});
```

## Conclusion

Congratulations! You've successfully deployed your Mastra agents and workflows. They're live on the internet and ready to be used by your users.


================================================================================
Source: src/pages/docs/deployment/logging-and-tracing.mdx
================================================================================

import Image from "next/image";

# Logging and Tracing

Effective logging and tracing are crucial for understanding the behavior of your application.

Tracing is especially important for AI engineering. Teams building AI products find that visibility into inputs and outputs of every step of every run is crucial to improving accuracy. You get this with Mastra's telemetry.

## Logging

In Mastra, logs can detail when certain functions run, what input data they receive, and how they respond.

### Basic Setup

Here's a minimal example that sets up a **console logger** at the `INFO` level. This will print out informational messages and above (i.e., `INFO`, `WARN`, `ERROR`) to the console.

```typescript filename="mastra.config.ts" showLineNumbers copy
import { Mastra, createLogger } from "@mastra/core";

export const mastra = new Mastra({
  // Other Mastra configuration...
  logger: createLogger({
    name: "Mastra",
    level: "info",
  }),
});
```

In this configuration:

- `name: "Mastra"` specifies the name to group logs under.
- `level: "info"` sets the minimum severity of logs to record.

### Configuration

- For more details on the options you can pass to `createLogger()`, see the [createLogger reference documentation](/docs/reference/observability/create-logger.mdx).
- Once you have a `Logger` instance, you can call its methods (e.g., `.info()`, `.warn()`, `.error()`) in the [Logger instance reference documentation](/docs/reference/observability/logger.mdx).
- If you want to send your logs to an external service for centralized collection, analysis, or storage, you can configure other logger types such as Upstash Redis. Consult the [createLogger reference documentation](/docs/reference/observability/create-logger.mdx) for details on parameters like `url`, `token`, and `key` when using the `UPSTASH` logger type.

## Telemetry

Mastra supports the OpenTelemetry Protocol (OTLP) for tracing and monitoring your application. When telemetry is enabled, Mastra automatically traces all core primitives including agent operations, LLM interactions, tool executions, integration calls, workflow runs, and database operations. Your telemetry data can then be exported to any OTEL collector.

### Basic Configuration

Here's a simple example of enabling telemetry:

```ts filename="mastra.config.ts" showLineNumbers copy
export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "my-app",
    enabled: true,
    sampling: {
      type: "always_on",
    },
    export: {
      type: "otlp",
      endpoint: "http://localhost:4318", // SigNoz local endpoint
    },
  },
});
```

### Configuration Options

The telemetry config accepts these properties:

```ts
type OtelConfig = {
  // Name to identify your service in traces (optional)
  serviceName?: string;

  // Enable/disable telemetry (defaults to true)
  enabled?: boolean;

  // Control how many traces are sampled
  sampling?: {
    type: "ratio" | "always_on" | "always_off" | "parent_based";
    probability?: number; // For ratio sampling
    root?: {
      probability: number; // For parent_based sampling
    };
  };

  // Where to send telemetry data
  export?: {
    type: "otlp" | "console";
    endpoint?: string;
    headers?: Record<string, string>;
  };
};
```

See the [OtelConfig reference documentation](/docs/reference/observability/otel-config.mdx) for more details.

### Environment Variables

You can configure the OTLP endpoint and headers through environment variables:

```env filename=".env" copy
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_EXPORTER_OTLP_HEADERS=x-api-key=your-api-key
```

Then in your config:

```ts filename="mastra.config.ts" showLineNumbers copy
export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "my-app",
    enabled: true,
    export: {
      type: "otlp",
      // endpoint and headers will be picked up from env vars
    },
  },
});
```

### Example: SigNoz Integration

Here's what a traced agent interaction looks like in [SigNoz](https://signoz.io):

<Image
  src="/signoz-telemetry-demo.png"
  alt="Agent interaction trace showing spans, LLM calls, and tool executions"
  style={{ maxWidth: "800px", width: "100%", margin: "8px 0" }}
  className="nextra-image rounded-md"
  data-zoom
  width={800}
  height={400}
/>

### Other Supported Providers

For a complete list of supported observability providers and their configuration details, see the [Observability Providers reference](../reference/observability/providers/index.mdx).

### Next.js Configuration [Local Dev]

When developing locally with Next.js, you'll need to:

1. Install the instrumentation package:

```bash copy
npm install import-in-the-middle # or require-in-the-middle for CJS
```

2. Add it as an external dependency in your Next.js config:

```ts filename="next.config.ts" showLineNumbers copy
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  serverExternalPackages: ["import-in-the-middle"],
};

export default nextConfig;
```

This configuration is only necessary for local development to ensure proper instrumentation during hot reloading.


================================================================================
Source: src/pages/docs/getting-started/installation.mdx
================================================================================

import { Callout, Steps, Tabs } from "nextra/components";
import YouTube from "../../../components/youtube";

# Installation

To run Mastra, you need access to an LLM. Typically, you'll want to get an API key from an LLM provider such as [OpenAI](https://platform.openai.com/), [Anthropic](https://console.anthropic.com/settings/keys), or [Google Gemini](https://ai.google.dev/gemini-api/docs). You can also run Mastra with a local LLM using [Ollama](https://ollama.ai/).

## Prerequisites

- Node.js `v20.0` or higher
- Access to a [supported large language model (LLM)](/guide/reference/llm/providers-and-models.mdx).

## Automatic Installation

<YouTube id="spGlcTEjuXY" />

<Steps>

### Create a New Project

We recommend starting a new Mastra project using `create-mastra`, which will scaffold your project. To create
a project, run:

<Tabs items={["npx", "npm", "yarn", "pnpm"]}>
  <Tabs.Tab>

```bash copy
npx create-mastra@latest
```

  </Tabs.Tab>
  <Tabs.Tab>
```bash copy
npm create mastra
```
  </Tabs.Tab>
  <Tabs.Tab>
```bash copy
yarn create mastra
```
</Tabs.Tab>
  <Tabs.Tab>
```bash copy
pnpm create mastra
```
</Tabs.Tab>
</Tabs>

On installation, you'll be guided through the following prompts:

```bash
What do you want to name your project? my-mastra-app
Choose components to install:
  ‚óØ Agents (recommended)
  ‚óØ Tools
  ‚óØ Workflows
Select default provider:
  ‚óØ OpenAI (recommended)
  ‚óØ Anthropic
  ‚óØ Groq
Would you like to include example code? No / Yes
```

After the prompts, `create-mastra` will set up your project directory with TypeScript, install dependencies, and configure your selected components and LLM provider.

### Set Up your API Key

Add the API key for your configured LLM provider in your `.env` file.

```bash filename=".env" copy
OPENAI_API_KEY=<your-openai-key>
```

</Steps>
Note: If you prefer to run the command with flags (non-interactive mode) and include the example code, you can use:
```bash copy
npx create-mastra@latest --components agents,tools --llm openai --example
```
This allows you to specify your preferences upfront without being prompted.

## Manual Installation

<br/>
<Steps>

If you prefer to set up your Mastra project manually, follow these steps:

### Create a New Project

Create a project directory and navigate into it:

```bash copy
mkdir hello-mastra
cd hello-mastra
```

Then, initialize a TypeScript project including the `@mastra/core` package:

```bash copy npm2yarn
npm init -y
npm install typescript tsx @types/node mastra@alpha --save-dev
npm install @mastra/core@alpha zod
npx tsc --init
```

### Set Up your API Key

Create a `.env` file in your project root directory and add your API key:

```bash filename=".env" copy
OPENAI_API_KEY=<your-openai-key>
```

Replace your_openai_api_key with your actual API key.

### Create a Tool

Create a `weather-tool` tool file:

```bash copy
mkdir -p src/mastra/tools && touch src/mastra/tools/weather-tool.ts
```

Then, add the following code to `src/mastra/tools/weather-tool.ts`:

```ts filename="src/mastra/tools/weather-tool.ts" showLineNumbers copy
import { createTool } from "@mastra/core";
import { z } from "zod";

interface WeatherResponse {
  current: {
    time: string;
    temperature_2m: number;
    apparent_temperature: number;
    relative_humidity_2m: number;
    wind_speed_10m: number;
    wind_gusts_10m: number;
    weather_code: number;
  };
}

export const weatherTool = createTool({
  id: "get-weather",
  description: "Get current weather for a location",
  inputSchema: z.object({
    location: z.string().describe("City name"),
  }),
  outputSchema: z.object({
    temperature: z.number(),
    feelsLike: z.number(),
    humidity: z.number(),
    windSpeed: z.number(),
    windGust: z.number(),
    conditions: z.string(),
    location: z.string(),
  }),
  execute: async ({ context }) => {
    return await getWeather(context.location);
  },
});

const getWeather = async (location: string) => {
  const geocodingUrl = `https://geocoding-api.open-meteo.com/v1/search?name=${encodeURIComponent(location)}&count=1`;
  const geocodingResponse = await fetch(geocodingUrl);
  const geocodingData = await geocodingResponse.json();

  if (!geocodingData.results?.[0]) {
    throw new Error(`Location '${location}' not found`);
  }

  const { latitude, longitude, name } = geocodingData.results[0];

  const weatherUrl = `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,apparent_temperature,relative_humidity_2m,wind_speed_10m,wind_gusts_10m,weather_code`;

  const response = await fetch(weatherUrl);
  const data: WeatherResponse = await response.json();

  return {
    temperature: data.current.temperature_2m,
    feelsLike: data.current.apparent_temperature,
    humidity: data.current.relative_humidity_2m,
    windSpeed: data.current.wind_speed_10m,
    windGust: data.current.wind_gusts_10m,
    conditions: getWeatherCondition(data.current.weather_code),
    location: name,
  };
};

function getWeatherCondition(code: number): string {
  const conditions: Record<number, string> = {
    0: "Clear sky",
    1: "Mainly clear",
    2: "Partly cloudy",
    3: "Overcast",
    45: "Foggy",
    48: "Depositing rime fog",
    51: "Light drizzle",
    53: "Moderate drizzle",
    55: "Dense drizzle",
    56: "Light freezing drizzle",
    57: "Dense freezing drizzle",
    61: "Slight rain",
    63: "Moderate rain",
    65: "Heavy rain",
    66: "Light freezing rain",
    67: "Heavy freezing rain",
    71: "Slight snow fall",
    73: "Moderate snow fall",
    75: "Heavy snow fall",
    77: "Snow grains",
    80: "Slight rain showers",
    81: "Moderate rain showers",
    82: "Violent rain showers",
    85: "Slight snow showers",
    86: "Heavy snow showers",
    95: "Thunderstorm",
    96: "Thunderstorm with slight hail",
    99: "Thunderstorm with heavy hail",
  };
  return conditions[code] || "Unknown";
}
```

### Create an Agent

Create a `weather` agent file:

```bash copy
mkdir -p src/mastra/agents && touch src/mastra/agents/weather.ts
```

Then, add the following code to `src/mastra/agents/weather.ts`:

```ts filename="src/mastra/agents/weather.ts" showLineNumbers
import { Agent } from "@mastra/core";
import { weatherTool } from "../tools/weather-tool";

export const weatherAgent = new Agent({
  name: "Weather Agent",
  instructions: `You are a helpful weather assistant that provides accurate weather information.

Your primary function is to help users get weather details for specific locations. When responding:
- Always ask for a location if none is provided
- Include relevant details like humidity, wind conditions, and precipitation
- Keep responses concise but informative

Use the weatherTool to fetch current weather data.`,
  model: {
    provider: "OPEN_AI",
    name: "gpt-4o",
  },
  tools: { weatherTool },
});
```

### Register Agent

Finally, create the Mastra entry point in `src/mastra/index.ts` and register agent:

```ts filename="src/mastra/index.ts" showLineNumbers
import { Mastra } from "@mastra/core";

import { weatherAgent } from "./agents/weather";

export const mastra = new Mastra({
  agents: { weatherAgent },
});
```

This registers your agent with Mastra so that `mastra dev` can discover and serve it.

</Steps>

<Callout type="info">
  To add Mastra to an existing project, see our Local dev docs on [mastra
  init](/docs/local-dev/mastra-init).
</Callout>

## Start the Mastra Server

Mastra provides commands to serve your agents via REST endpoints

### Development Server

Run the following command to start the Mastra server:

```bash copy
npm run dev
```

If you have the mastra CLI installed, run:

```bash copy
mastra dev
```

This command creates REST API endpoints for your agents.

### Test the Endpoint

You can test the agent's endpoint using `curl` or `fetch`:

<Tabs items={['curl', 'fetch']}>
  <Tabs.Tab>
```bash copy
curl -X POST http://localhost:4111/api/agents/weatherAgent/generate \
-H "Content-Type: application/json" \
-d '{"messages": ["What is the weather in London?"]}'
```
  </Tabs.Tab>
  <Tabs.Tab>
```js copy showLineNumbers
fetch('http://localhost:4111/api/agents/weatherAgent/generate', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    messages: ['What is the weather in London?'],
  }),
})
  .then(response => response.json())
  .then(data => {
    console.log('Agent response:', data.text);
  })
  .catch(error => {
    console.error('Error:', error);
  });
```
  </Tabs.Tab>
</Tabs>

## Run from the command line

If you'd like to directly call agents from the command line, you can create a script to get an agent and call it:

```ts filename="src/index.ts" showLineNumbers
import { mastra } from "./mastra";

async function main() {
  const agent = await mastra.getAgent("weatherAgent");

  const result = await agent.generate("What is the weather in London?");

  console.log("Agent response:", result.text);
}

main();
```

Then, run the script to test that everything is set up correctly:

```bash copy
npx tsx src/index.ts
```

This should output the agent's response to your console.

---


================================================================================
Source: src/pages/docs/getting-started/project-structure.mdx
================================================================================

import { FileTree } from 'nextra/components';

# Project Structure and Organization

This page provides a guide for organizing folders and files in Mastra. Mastra is a modular framework, and you can use any of the modules separately or together.

You could write everything in a single file (as we showed in the quick start), or separate each agent, tool, and workflow into their own files.

We don't enforce a specific folder structure, but we do recommend some best practices, and the CLI will scaffold a project with a sensible structure.

## Using the CLI

`mastra init` is an interactive CLI that allows you to:

- **Choose a directory for Mastra files**: Specify where you want the Mastra files to be placed (default is `src/mastra`).
- **Select components to install**: Choose which components you want to include in your project:
  - Agents
  - Tools
  - Workflows
- **Select a default LLM provider**: Choose from supported providers like OpenAI, Anthropic, or Groq.
- **Include example code**: Decide whether to include example code to help you get started.

### Example Project Structure

Assuming you select all components and include example code, your project structure will look like this:

<FileTree>
  <FileTree.Folder name="root" defaultOpen>
    <FileTree.Folder name="src" defaultOpen>
      <FileTree.Folder name="mastra" defaultOpen>
        <FileTree.Folder name="agents" defaultOpen>
          <FileTree.File name="index.ts" />
        </FileTree.Folder>
        <FileTree.Folder name="tools" defaultOpen>
          <FileTree.File name="index.ts" />
        </FileTree.Folder>
        <FileTree.Folder name="workflows" defaultOpen>
          <FileTree.File name="index.ts" />
        </FileTree.Folder>
        <FileTree.File name="index.ts" />
      </FileTree.Folder>
    </FileTree.Folder>
    <FileTree.File name=".env" />
  </FileTree.Folder>
</FileTree>
{/* 
```
root/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ mastra/
‚îÇ       ‚îú‚îÄ‚îÄ agents/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ index.ts
‚îÇ       ‚îú‚îÄ‚îÄ tools/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ index.ts
‚îÇ       ‚îú‚îÄ‚îÄ workflows/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ index.ts
‚îÇ       ‚îú‚îÄ‚îÄ index.ts
‚îú‚îÄ‚îÄ .env
``` */}

### Top-level Folders

| Folder                 | Description                          |
| ---------------------- | ------------------------------------ |
| `src/mastra`           | Core application folder              |
| `src/mastra/agents`    | Agent configurations and definitions |
| `src/mastra/tools`     | Custom tool definitions              |
| `src/mastra/workflows` | Workflow definitions                 |

### Top-level Files

| File                  | Description                        |
| --------------------- | ---------------------------------- |
| `src/mastra/index.ts` | Main configuration file for Mastra |
| `.env`                | Environment variables              |


================================================================================
Source: src/pages/docs/guides/01-harry-potter.mdx
================================================================================

import { Steps } from "nextra/components";
import YouTube from "../../../components/youtube";

# Guide: Harry Potter

Mastra provides direct support for Large Language Models (LLMs) through the `LLM` class. It supports a variety of LLM providers, including OpenAI, Anthropic, and Google Gemini. You can choose the specific model and provider, set system and user prompts, and decide whether to stream the response.

We'll use a Harry Potter-themed example where we ask about the model's favorite room in Hogwarts, demonstrating how changing the system message affects the response.

<YouTube id="pdt-RXffEZg" />

In this guide, we'll walk through:

- Creating a model
- Giving it a prompt
- Testing the response
- Altering the system message
- Streaming the response

## Setup

Ensure you have the Mastra core package installed:

```bash copy
npm install @mastra/core
```

Set your API key for the LLM provider you intend to use. For OpenAI, set the `OPENAI_API_KEY` environment variable.

```bash filename=".env" copy
OPENAI_API_KEY=<your-openai-api-key>
```

<Steps>

## Create a Model

We'll start by creating a model configuration and initializing the Mastra instance.

```typescript
import { CoreMessage, Mastra, type ModelConfig } from "@mastra/core";

const mastra = new Mastra();

const modelConfig: ModelConfig = {
  provider: "OPEN_AI",
  name: "gpt-4",
};

const llm = mastra.LLM(modelConfig);
```

## Give It a Prompt

Next, we'll prepare our prompt. We'll ask:

```typescript
const prompt = "What is your favorite room in Hogwarts?";
```

## Test the Response

We'll use the `generate` method to get the response from the model.

```typescript
const response = await llm.generate(prompt);

console.log("Response:", response.text);
```

Run the script:

```bash copy
npx bun src/mastra/index.ts
```

Output:

```
Response: As an AI language model developed by OpenAI, I don't possess consciousness or experiences.
```

The model defaults to its own perspective. To get a more engaging response, we'll alter the system message.

## Alter the System Message

To change the perspective, we'll add a system message to specify the persona of the model. First, we'll have the model respond as Harry Potter.

**As Harry Potter**

```typescript
const messages = [
  {
    role: "system",
    content: "You are Harry Potter.",
  },
  {
    role: "user",
    content: "What is your favorite room in Hogwarts?",
  },
] as CoreMessage[];

const responseHarry = await llm.generate(messages);

console.log("Response as Harry Potter:", responseHarry.text);
```

Output:

```
Response as Harry Potter: My favorite room in Hogwarts is definitely the Gryffindor Common Room. It's where I feel most at home, surrounded by my friends, the warm fireplace, and the cozy chairs. It's a place filled with great memories.
```

---

**As Draco Malfoy**

Now, let's change the system message to have the model respond as Draco Malfoy.

```typescript
messages[0].content = "You are Draco Malfoy.";

const responseDraco = await llm.generate(messages);

console.log("Response as Draco Malfoy:", responseDraco.text);
```

Output:

```
Response as Draco Malfoy: My favorite room in Hogwarts is the Slytherin Common Room. It's located in the dungeons, adorned with green and silver decor, and has a magnificent view of the Black Lake's depths. It's exclusive and befitting of those with true ambition.
```

## Stream the Response

Finally, we'll demonstrate how to stream the response from the model. Streaming is useful for handling longer outputs or providing real-time feedback.

```typescript
const stream = await llm.stream(messages);

console.log('Streaming response as Draco Malfoy:');

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}

console.log('\n');
}

main();
```

Run the script again:

```bash copy
npx bun src/mastra/index.ts
```

**Output:**

```
Streaming response as Draco Malfoy: My favorite room in Hogwarts is the Slytherin Common Room. Situated in the dungeons, it's an elegant space with greenish lights and serpentine motifs...
```

</Steps>
## Summary

By following this guide, you've learned how to:

- Create and configure an LLM model in Mastra
- Provide prompts and receive responses
- Use system messages to change the model's perspective
- Stream responses from the model

Feel free to experiment with different system messages and prompts to explore the capabilities of Mastra's LLM support.


================================================================================
Source: src/pages/docs/guides/02-chef-michel.mdx
================================================================================

import { Steps } from 'nextra/components';

# Agents Guide: Building a Chef Assistant

In this guide, we'll walk through creating a "Chef Assistant" agent that helps users cook meals with available ingredients.

## Prerequisites

- Node.js installed
- Mastra installed: `npm install @mastra/core`

---

## Create the Agent

<Steps>
### Define the Agent

Create a new file `src/mastra/agents/chefAgent.ts` and define your agent:

```ts copy filename="src/mastra/agents/chefAgent.ts"
import { Agent } from '@mastra/core';

export const chefAgent = new Agent({
  name: 'chef-agent',
  instructions:
    'You are Michel, a practical and experienced home chef' +
    'You helps people cook with whatever ingredients they have available.',
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o',
    toolChoice: 'auto',
  },
});
```

---

## Set Up Environment Variables

Create a `.env` file in your project root and add your OpenAI API key:

```bash filename=".env" copy
OPENAI_API_KEY=your_openai_api_key
```

---

## Register the Agent with Mastra

In your main file, register the agent:

```ts filename="src/mastra/index.ts"
import { Mastra } from '@mastra/core';

import { chefAgent } from './agents/chefAgent';

export const mastra = new Mastra({
  agents: { chefAgent },
});
```

---

</Steps >

## Interacting with the Agent

<Steps>
### Generating Text Responses

```ts filename="src/mastra/index.ts"
async function main() {
  const query =
    'In my kitchen I have: pasta, canned tomatoes, garlic, olive oil, and some dried herbs (basil and oregano). What can I make?';
  console.log(`Query: ${query}`);

  const response = await chefAgent.generate([{ role: 'user', content: query }]);
  console.log('\nüë®‚Äçüç≥ Chef Michel:', response.text);
}

main();
```

**Output:**

```
Query: In my kitchen I have: pasta, canned tomatoes, garlic, olive oil, and some dried herbs (basil and oregano). What can I make?

üë®‚Äçüç≥ Chef Michel: You can make a delicious pasta al pomodoro! Here's how...
```

---

### Streaming Responses

```ts filename="src/mastra/index.ts"
async function main() {
  const query =
    "Now I'm over at my friend's house, and they have: chicken thighs, coconut milk, sweet potatoes, and some curry powder.";
  console.log(`Query: ${query}`);

  const stream = await chefAgent.stream([{ role: 'user', content: query }]);

  console.log('\n Chef Michel: ');

  for await (const chunk of stream.textStream) {
    process.stdout.write(chunk);
  }

  console.log('\n\n‚úÖ Recipe complete!');
}

main();
```

**Output:**

```
Query: Now I'm over at my friend's house, and they have: chicken thighs, coconut milk, sweet potatoes, and some curry powder.

üë®‚Äçüç≥ Chef Michel:
Great! You can make a comforting chicken curry...

‚úÖ Recipe complete!
```

---

### Generating a Recipe with Structured Data

```ts filename="src/mastra/index.ts"
import { z } from 'zod';

async function main() {
  const query = 'I want to make lasagna, can you generate a lasagna recipe for me?';
  console.log(`Query: ${query}`);

  // Define the Zod schema
  const schema = z.object({
    ingredients: z.array(
      z.object({
        name: z.string(),
        amount: z.string(),
      }),
    ),
    steps: z.array(z.string()),
  });

  const response = await chefAgent.generate([{ role: 'user', content: query }], { output: schema });
  console.log('\nüë®‚Äçüç≥ Chef Michel:', response.object);
}

main();
```

**Output:**

```
Query: I want to make lasagna, can you generate a lasagna recipe for me?

üë®‚Äçüç≥ Chef Michel: {
  ingredients: [
    { name: "Lasagna noodles", amount: "12 sheets" },
    { name: "Ground beef", amount: "1 pound" },
    // ...
  ],
  steps: [
    "Preheat oven to 375¬∞F (190¬∞C).",
    "Cook the lasagna noodles according to package instructions.",
    // ...
  ]
}
```

---

</Steps >

## Running the Agent Server

<Steps>

### Using `mastra dev`

You can run your agent as a service using the `mastra dev` command:

```bash
mastra dev
```

This will start a server exposing endpoints to interact with your registered agents.

### Accessing the Chef Assistant API

By default, `mastra dev` runs on `http://localhost:4111`. Your Chef Assistant agent will be available at:

```
POST http://localhost:4111/api/agents/chefAgent/generate
```

### Interacting with the Agent via `curl`

You can interact with the agent using `curl` from the command line:

```bash
curl -X POST http://localhost:4111/api/agents/chefAgent/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "I have eggs, flour, and milk. What can I make?"
      }
    ]
  }'
```

**Sample Response:**

```json
{
  "text": "You can make delicious pancakes! Here's a simple recipe..."
}
```

</Steps>


================================================================================
Source: src/pages/docs/guides/03-stock-agent.mdx
================================================================================

import { Steps } from "nextra/components";

# Stock Agent

We're going to create a simple agent that fetches the last day's closing stock price for a given symbol. This example will show you how to create a tool, add it to an agent, and use the agent to fetch stock prices.

## Project Structure

```
stock-price-agent/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ agents/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stockAgent.ts
‚îÇ   ‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stockPrices.ts
‚îÇ   ‚îî‚îÄ‚îÄ index.ts
‚îú‚îÄ‚îÄ package.json
‚îî‚îÄ‚îÄ .env
```

---

<Steps>
## Initialize the Project and Install Dependencies

First, create a new directory for your project and navigate into it:

```bash
mkdir stock-price-agent
cd stock-price-agent
```

Initialize a new Node.js project and install the required dependencies:

```bash
npm init -y
npm install @mastra/core zod
```

Set Up Environment Variables

Create a `.env` file at the root of your project to store your OpenAI API key.

```bash filename=".env" copy
OPENAI_API_KEY=your_openai_api_key
```

Create the necessary directories and files:

```bash
mkdir -p src/agents src/tools
touch src/agents/stockAgent.ts src/tools/stockPrices.ts src/index.ts
```

---

## Create the Stock Price Tool

Next, we'll create a tool that fetches the last day's closing stock price for a given symbol.

```ts filename="src/tools/stockPrices.ts"
import { createTool } from "@mastra/core";
import { z } from "zod";

const getStockPrice = async (symbol: string) => {
  const data = await fetch(
    `https://mastra-stock-data.vercel.app/api/stock-data?symbol=${symbol}`,
  ).then((r) => r.json());
  return data.prices["4. close"];
};

export const stockPrices = createTool({
  id: "Get Stock Price",
  inputSchema: z.object({
    symbol: z.string(),
  }),
  description: `Fetches the last day's closing stock price for a given symbol`,
  execute: async ({ context: { symbol } }) => {
    console.log("Using tool to fetch stock price for", symbol);
    return {
      symbol,
      currentPrice: await getStockPrice(symbol),
    };
  },
});
```

---

## Add the Tool to an Agent

We'll create an agent and add the `stockPrices` tool to it.

```ts filename="src/agents/stockAgent.ts"
import { Agent } from "@mastra/core";

import * as tools from "../tools/stockPrices";

export const stockAgent = new Agent<typeof tools>({
  name: "Stock Agent",
  instructions:
    "You are a helpful assistant that provides current stock prices. When asked about a stock, use the stock price tool to fetch the stock price.",
  model: {
    provider: "OPEN_AI",
    name: "gpt-4o",
  },
  tools: {
    stockPrices: tools.stockPrices,
  },
});
```

---

## Set Up the Mastra Instance

We need to initialize the Mastra instance with our agent and tool.

```ts filename="src/index.ts"
import { Mastra } from "@mastra/core";

import { stockAgent } from "./agents/stockAgent";

export const mastra = new Mastra({
  agents: { stockAgent },
});
```

## Serve the Application

Instead of running the application directly, we'll use the `mastra dev` command to start the server. This will expose your agent via REST API endpoints, allowing you to interact with it over HTTP.

In your terminal, start the Mastra server by running:

```bash
mastra dev --dir src
```

This command will start the server and make your agent available at:

```
http://localhost:4111/api/agents/stockAgent/generate
```

Make sure that your environment variables are set, especially your OpenAI API key. If you haven't set it yet, you can provide it inline:

```bash
OPENAI_API_KEY=your_openai_api_key mastra dev
```

---

## Test the Agent with cURL

Now that your server is running, you can test your agent's endpoint using `curl`:

```bash
curl -X POST http://localhost:4111/api/agents/stockAgent/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      { "role": "user", "content": "What is the current stock price of Apple (AAPL)?" }
    ]
  }'
```

**Expected Response:**

You should receive a JSON response similar to:

```json
{
  "text": "The current price of Apple (AAPL) is $174.55.",
  "agent": "Stock Agent"
}
```

This indicates that your agent successfully processed the request, used the `stockPrices` tool to fetch the stock price, and returned the result.

</Steps>


================================================================================
Source: src/pages/docs/guides/04-recruiter.mdx
================================================================================

# Introduction

In this guide, you‚Äôll learn how Mastra helps you build workflows with LLMs.

We‚Äôll walk through creating a workflow that gathers information from a candidate‚Äôs resume, then branches to either a technical or behavioral question based on the candidate‚Äôs profile. Along the way, you‚Äôll see how to structure workflow steps, handle branching, and integrate LLM calls.

Below is a concise version of the workflow. It starts by importing the necessary modules, sets up Mastra, defines steps to extract and classify candidate data, and then asks suitable follow-up questions. Each code block is followed by a short explanation of what it does and why it‚Äôs useful.

## 1. Imports and Setup

You need to import Mastra tools and Zod to handle workflow definitions and data validation.

```typescript filename="src/mastra/index.ts" copy
import { Step, Workflow, Mastra } from "@mastra/core";
import { z } from "zod";
```

Add your `OPENAI_API_KEY` to the `.env` file.

```bash filename=".env" copy
OPENAI_API_KEY=<your-openai-key>
```

## 2. Step One: Gather Candidate Info

You want to extract candidate details from the resume text and classify them as technical or non-technical. This step calls an LLM to parse the resume and return structured JSON, including the name, technical status, specialty, and the original resume text. The code reads resumeText from trigger data, prompts the LLM, and returns organized fields for use in subsequent steps.

```typescript filename="src/mastra/index.ts" copy
const gatherCandidateInfo = new Step({
  id: "gatherCandidateInfo",
  inputSchema: z.object({
    resumeText: z.string(),
  }),
  outputSchema: z.object({
    candidateName: z.string(),
    isTechnical: z.boolean(),
    specialty: z.string(),
    resumeText: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    if (!mastra?.llm) {
      throw new Error("Mastra instance is required to run this step");
    }
    const resumeText = context.machineContext?.getStepPayload<{
      resumeText: string;
    }>("trigger")?.resumeText;

    const llm = mastra.llm({ provider: "OPEN_AI", name: "gpt-4o" });

    const prompt = `
          You are given this resume text:
          "${resumeText}"
        `;
    const res = await llm.generate(prompt, {
      output: z.object({
        candidateName: z.string(),
        isTechnical: z.boolean(),
        specialty: z.string(),
        resumeText: z.string(),
      }),
    });

    return res.object;
  },
});
```

## 3. Technical Question Step

This step prompts a candidate who is identified as technical for more information about how they got into their specialty. It uses the entire resume text so the LLM can craft a relevant follow-up question. The code generates a question about the candidate‚Äôs specialty.

```typescript filename="src/mastra/index.ts" copy
interface CandidateInfo {
  candidateName: string;
  isTechnical: boolean;
  specialty: string;
  resumeText: string;
}

const askAboutSpecialty = new Step({
  id: "askAboutSpecialty",
  outputSchema: z.object({
    question: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    if (!mastra?.llm) {
      throw new Error("Mastra instance is required to run this step");
    }

    const candidateInfo = context.machineContext?.getStepPayload<CandidateInfo>(
      "gatherCandidateInfo",
    );

    const llm = mastra.llm({ provider: "OPEN_AI", name: "gpt-4o" });
    const prompt = `
          You are a recruiter. Given the resume below, craft a short question
          for ${candidateInfo?.candidateName} about how they got into "${candidateInfo?.specialty}".
          Resume: ${candidateInfo?.resumeText}
        `;
    const res = await llm.generate(prompt);
    return { question: res?.text?.trim() || "" };
  },
});
```

## 4. Behavioral Question Step

If the candidate is non-technical, you want a different follow-up question. This step asks what interests them most about the role, again referencing their complete resume text. The code solicits a role-focused query from the LLM.

```typescript filename="src/mastra/index.ts" copy
const askAboutRole = new Step({
  id: "askAboutRole",
  outputSchema: z.object({
    question: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    if (!mastra?.llm) {
      throw new Error("Mastra instance is required to run this step");
    }
    const candidateInfo = context.machineContext?.getStepPayload<CandidateInfo>(
      "gatherCandidateInfo",
    );

    const llm = mastra.llm({ provider: "OPEN_AI", name: "gpt-4o" });
    const prompt = `
          You are a recruiter. Given the resume below, craft a short question
          for ${candidateInfo?.candidateName} asking what interests them most about this role.
          Resume: ${candidateInfo?.resumeText}
        `;
    const res = await llm.generate(prompt);
    return { question: res?.text?.trim() || "" };
  },
});
```

## 5. Define the Workflow

You now combine the steps to implement branching logic based on the candidate‚Äôs technical status. The workflow first gathers candidate data, then either asks about their specialty or about their role, depending on isTechnical. The code chains gatherCandidateInfo with askAboutSpecialty and askAboutRole, and commits the workflow.

```typescript filename="src/mastra/index.ts" copy
const candidateWorkflow = new Workflow({
  name: "candidate-workflow",
  triggerSchema: z.object({
    resumeText: z.string(),
  }),
});

candidateWorkflow
  .step(gatherCandidateInfo)
  .then(askAboutSpecialty, {
    when: { "gatherCandidateInfo.isTechnical": true },
  })
  .after(gatherCandidateInfo)
  .step(askAboutRole, {
    when: { "gatherCandidateInfo.isTechnical": false },
  });

candidateWorkflow.commit();
```

## 6. Execute the Workflow

```typescript filename="src/mastra/index.ts" copy
const mastra = new Mastra({
  workflows: {
    candidateWorkflow,
  },
});

(async () => {
  const { runId, start } = mastra.getWorkflow("candidateWorkflow").createRun();

  console.log("Run", runId);

  const runResult = await start({
    triggerData: { resumeText: "Simulated resume content..." },
  });

  console.log("Final output:", runResult.results);
})();
```

You‚Äôve just built a workflow to parse a resume and decide which question to ask based on the candidate‚Äôs technical abilities. Congrats and happy hacking!


================================================================================
Source: src/pages/docs/index.mdx
================================================================================

# Introduction

Mastra is an opinionated Typescript framework that helps you build AI applications and features quickly. It gives you the set of primitives you need: workflows, agents, RAG, integrations and evals. You can run Mastra on your local machine, or deploy to a serverless cloud.

The main Mastra features are:

| Features                                       | Description                                                                                                                                                     |
| ---------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [LLM Models](/docs/llm-models/00-overview.mdx)    | Mastra supports a variety of LLM providers, including OpenAI, Anthropic, Google Gemini.                                                                         |
| [Agents](/docs/agents/00-overview.mdx)            | Agents are systems where the language model chooses a sequence of actions.                                                                                      |
| [Tools](/docs/agents/02-adding-tools.mdx)         | Tools are typed functions that can be executed by agents or workflows, with built-in integration access and parameter validation.                               |
| [Workflows](/docs/workflows/00-overview.mdx)      | Workflows are durable graph-based state machines with built-in tracing. They can execute complex sequences of LLM operations.                                   |
| [RAG](/docs/rag/overview.mdx)                     | Retrieval-augemented generation (RAG) lets you construct a knowledge base for your agents.                                                                      |
| [Integrations](/docs/local-dev/integrations.mdx)  | In Mastra, integrations are auto-generated, type-safe API clients for third-party services. |
| [Evals](/docs/08-running-evals)                   | Evals are automated tests that evaluate LLM outputs using model-graded, rule-based, and statistical methods.                                                    |


================================================================================
Source: src/pages/docs/llm-models/00-overview.mdx
================================================================================

import { Callout } from "nextra/components";

# LLM Class Overview

Mastra provides direct support for Large Language Models (LLMs) through the `LLM` class. The `LLM` class allows you to interact with various language models seamlessly, enabling you to generate text, handle conversations, and more. This guide covers:

- How to initialize the LLM class.
- Supported models and providers.
- Using the `generate` function.
- Message formats in `generate`.
- Output formats in `generate`.

---

## Initializing the LLM Class

To start using the `LLM` class, you need to initialize it with the desired model configuration. Here's how you can do it:

```typescript
import { Mastra } from "@mastra/core";

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: "OPEN_AI",
  name: "gpt-4o-mini",
});
```

This initialization allows telemetry to pass through to the LLM, providing insights into model usage and performance.

**Note:** You can find more details about the model configuration options in the [ModelConfig class reference](../reference/llm/model-config.mdx).

---

## Supported Models and Providers

Mastra supports major LLM providers out of the box, plus additional providers through AI SDK integrations. Custom providers can also be added via the Portkey service.

### Most Popular Providers and Models

| Provider          | Supported Models                                                                                                                                                         |
| ----------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **OpenAI**        | `gpt-4`, `gpt-4-turbo`, `gpt-3.5-turbo`, `gpt-4o`, `gpt-4o-mini`                                                                                                         |
| **Anthropic**     | `claude-3-5-sonnet-20241022`, `claude-3-5-sonnet-20240620`, `claude-3-5-haiku-20241022`, `claude-3-opus-20240229`, `claude-3-sonnet-20240229`, `claude-3-haiku-20240307` |
| **Google Gemini** | `gemini-1.5-pro-latest`, `gemini-1.5-pro`, `gemini-1.5-flash-latest`, `gemini-1.5-flash`                                                                                 |

A full list of supported models can be found [here](../reference/llm/providers-and-models.mdx).

<Callout>
  {" "}
  If you don't want to pay for an LLM provider, Google Gemini has a generous
  free tier for its API.
</Callout>

---

## The `generate` Function

The main function you'll use with the `LLM` class is `generate`. It allows you to send messages to the language model and receive responses. The `generate` function takes:

- **messages**: The first parameter, which can be a string, an array of strings, or an array of message objects.
- **options**: The second parameter, which includes additional configurations like streaming, schemas for structured output, etc.

This design covers all potential use cases and is extensible to multi-modal interactions in the future.

---

## Message Formats in `generate`

The `generate` function supports three types of message formats:

### 1. Simple String

You can pass a single string as the message:

```typescript
const response = await llm.generate("Tell me a joke.");
```

### 2. Array of Strings

You can provide an array of strings, which will be converted into user messages:

```typescript
const response = await llm.generate([
  "Hello!",
  "Can you explain quantum mechanics?",
]);
```

### 3. Detailed Message Objects

For finer control, you can pass an array of message objects, specifying the role and content:

```typescript
const response = await llm.generate([
  { role: "system", content: "You are a helpful assistant." },
  { role: "user", content: "What is the meaning of life?" },
]);
```

---

## Output Formats in `generate`

The `generate` function supports four types of output formats:

### 1. Simple Text Generation

Receive a basic text response from the model:

```typescript
const response = await llm.generate("What is AI?");

console.log(response.text);
```

### 2. Structured Output

Request a structured response by providing a schema. This is useful when you need the output in a specific format:

```typescript
import { z } from "zod";

const mySchema = z.object({
  definition: z.string(),
  examples: z.array(z.string()),
});

const response = await llm.generate(
  "Define machine learning and give examples.",
  {
    output: mySchema,
  },
);

console.log(response.object);
```

### 3. Streaming Text

Stream the response in real-time, which is useful for handling longer outputs or providing immediate feedback to users:

```typescript
const stream = await llm.stream("Tell me a story about a brave knight.");

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
```

### 4. Streaming Structured Output

Stream a structured response using a schema:

```typescript
const stream = await llm.stream("Provide live weather data.", {
  output: mySchema,
});

for await (const chunk of stream.textStream) {
  console.log(chunk);
}
```

---

## Additional Notes

- **Telemetry**: Initializing the `LLM` class through Mastra allows telemetry data to pass through, enabling better monitoring and debugging.
- **Extensibility**: The design of the `generate` function and message formats makes it future-proof and extensible for multi-modal interactions.


================================================================================
Source: src/pages/docs/local-dev/engine.mdx
================================================================================

# Mastra Engine

The Mastra Engine provides PostgreSQL-based storage for two core AI application needs:

1. Agent Memory - Persistent storage for conversation history and agent state
2. Vector Operations - Storage and similarity search for RAG applications

Mastra Engine is not required to run Mastra, but provides a "batteries-included" local development experience and a production-ready data layer.

## Setting up for local development

The Mastra CLI includes commands to manage your engine and database:

- `mastra engine add`: Sets up your dev environment, starts Docker containers for Postgres, and configures `.env`.
- `mastra engine up`: Starts your Docker containers as defined in your docker config file.
- `mastra engine migrate`: Runs database migrations.
- `mastra engine generate`: Generates TypeScript types from your database schema.
- `mastra engine down`: Stops your Docker containers.

The engine will run by default on port 5432 of `localhost`.

## Configuring for Agent Memory

Configure your agent to use PostgreSQL for memory storage:

```typescript
import { Mastra } from "@mastra/core";
import { PgMemory } from "@mastra/memory";

const pgMemory = new PgMemory({
  connectionString: process.env.POSTGRES_CONNECTION_STRING!,
});

export const mastra = new Mastra({
  memory: pgMemory,
  agents: { myAgent },
});
```

Now, agent memory operations like `saveMemory`, `getContextWindow`, and `getMemory` are available to your agent. See the [Memory](../agents/memory.mdx) reference for more details.

## Vector Operations (RAG)

For RAG applications, you can set up vector storage and reranking:

```typescript
import { PgVector } from "@mastra/rag";
import { Reranker } from "@mastra/rerank";

// Initialize vector store and reranker
const vectorStore = new PgVector({
  connectionString: process.env.POSTGRES_CONNECTION_STRING!
});
```

## Deploying to Production

To deploy to production, you'll need to set up a production Postgres database and configure your Mastra application to use it. 

You can run `DB_URL=your_prod_db_url mastra engine migrate` to create a schema in a production database.

## Mocking for Tests

For testing or development, you can use the `MockMastraEngine`:

```typescript
import { MockMastraEngine } from "@mastra/core";

const mockEngine = new MockMastraEngine({ url: "mock://localhost" });

// Create an entity
const entity = await mockEngine.createEntity({
  name: "Contacts",
  connectionId: "user_123",
});

// Insert records
await mockEngine.upsertRecords({
  entityId: entity.id,
  records: [
    { externalId: "c1", data: { name: "Alice" }, entityType: "Contacts" },
    { externalId: "c2", data: { name: "Bob" }, entityType: "Contacts" },
  ],
});

// Query the records
const records = await mockEngine.getRecordsByEntityId({ entityId: entity.id });
console.log(records);
```

This mock engine mimics the behavior of the real engine without requiring a real database connection, making it ideal for unit tests.

## Summary

The Mastra Engine is a PostgreSQL instance that stores agent memory and vector embeddings. You can spin it up locally with Docker for development and deploy it to a production instance. It's a helpful starting spot for handling agent memory and vector operations -- start with Postgres + pgvector, then adapt individual components based on your needs.


================================================================================
Source: src/pages/docs/local-dev/integrations.mdx
================================================================================

# Mastra Integrations

Integrations in Mastra are auto-generated, type-safe API clients for third-party services. They can be used as tools for agents or as steps in workflows.

## Installing an Integration

Mastra's default integrations are packaged as individually installable npm modules. You can add an integration to your project by installing it via npm and importing it into your Mastra configuration.

### Example: Adding the GitHub Integration

1. **Install the Integration Package**

To install the GitHub integration, run:

```bash
npm install @mastra/github
```

2. **Add the Integration to Your Project**

Create a new file for your integrations (e.g., `src/mastra/integrations/index.ts`) and import the integration:

```typescript filename="src/mastra/integrations/index.ts" showLineNumbers copy
import { GithubIntegration } from '@mastra/github';

export const github = new GithubIntegration({
  config: {
    PERSONAL_ACCESS_TOKEN: process.env.GITHUB_PAT!,
  },
});
```

Make sure to replace `process.env.GITHUB_PAT!` with your actual GitHub Personal Access Token or ensure that the environment variable is properly set.

3. **Use the Integration in Tools or Workflows**

You can now use the integration when defining tools for your agents or in workflows.

```typescript filename="src/mastra/tools/index.ts" showLineNumbers copy
import { createTool } from '@mastra/core';
import { z } from 'zod';
import { github } from '../integrations';

export const getMainBranchRef = createTool({
  id: 'getMainBranchRef',
  description: 'Fetch the main branch reference from a GitHub repository',
  inputSchema: z.object({
    owner: z.string(),
    repo: z.string(),
  }),
  outputSchema: z.object({
    ref: z.string().optional(),
  }),
  execute: async ({ context }) => {
    const client = await github.getApiClient();

    const mainRef = await client.gitGetRef({
      path: {
        owner: context.owner,
        repo: context.repo,
        ref: 'heads/main',
      },
    });

    return { ref: mainRef.data?.ref };
  },
});
```

In the example above:

- We import the `github` integration.
- We define a tool called `getMainBranchRef` that uses the GitHub API client to fetch the reference of the main branch of a repository.
- The tool accepts `owner` and `repo` as inputs and returns the reference string.

## Using Integrations in Agents

Once you've defined tools that utilize integrations, you can include these tools in your agents.

```typescript filename="src/mastra/agents/index.ts" showLineNumbers copy
import { Agent } from '@mastra/core';
import { getMainBranchRef } from '../tools';

export const codeReviewAgent = new Agent({
  name: 'Code Review Agent',
  instructions: 'An agent that reviews code repositories and provides feedback.',
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4',
  },
  tools: {
    getMainBranchRef,
    // other tools...
  },
});
```

In this setup:

- We create an agent named `Code Review Agent`.
- We include the `getMainBranchRef` tool in the agent's available tools.
- The agent can now use this tool to interact with GitHub repositories during conversations.

## Environment Configuration

Ensure that any required API keys or tokens for your integrations are properly set in your environment variables. For example, with the GitHub integration, you need to set your GitHub Personal Access Token:

```bash
GITHUB_PAT=your_personal_access_token
```

Consider using a `.env` file or another secure method to manage sensitive credentials.

## Available Integrations

Mastra provides several built-in integrations; primarily API-key based integrations that do not require OAuth. Some available integrations including Github, Stripe, Resend, Firecrawl, and more.

Check [Mastra's codebase](https://github.com/mastrajs/mastra/tree/main/packages/integrations) or [npm packages](https://www.npmjs.com/settings/mastra/packages) for a full list of available integrations.

## Conclusion

Integrations in Mastra enable your AI agents and workflows to interact with external services seamlessly. By installing and configuring integrations, you can extend the capabilities of your application to include operations such as fetching data from APIs, sending messages, or managing resources in third-party systems.

Remember to consult the documentation of each integration for specific usage details and to adhere to best practices for security and type safety.


================================================================================
Source: src/pages/docs/local-dev/mastra-dev.mdx
================================================================================

# `mastra dev`

The `mastra dev` command launches a development server that serves your Mastra application locally. 

## REST API Endpoints

`mastra dev` spins up REST API endpoints for your agents and workflows, such as:

- `POST /api/agents/:agentId/generate`
- `POST /api/agents/:agentId/stream`
- `POST /api/workflows/:workflowId/start`
- `POST /api/workflows/:workflowId/:instanceId/event`
- `GET /api/workflows/:workflowId/:instanceId/status`

By default, the server runs at http://localhost:4111, but you can change the port with the `--port` flag.

## UI Playground

`mastra dev` creates a UI with an agent chat interface, a workflow visualizer and a tool playground.

{/* TODO: Record a quick tour video here */}

## OpenAPI Specification

`mastra dev` provides an OpenAPI spec at:
- `GET /openapi.json`

## Summary

`mastra dev` makes it easy to develop, debug, and iterate on your AI logic in a self-contained environment before deploying to production.

- [Mastra Dev reference](../reference/cli/dev.mdx)


================================================================================
Source: src/pages/docs/local-dev/mastra-init.mdx
================================================================================

import { Tabs } from "nextra/components";

# Mastra Init

The `mastra init` command helps you add Mastra to an existing project. This command provides an interactive setup process to configure Mastra in your project.

## Using the CLI

### Install the CLI

First, install the mastra CLI.

<Tabs items={["npm", "yarn", "pnpm"]}>
  <Tabs.Tab>

```bash copy
npm i -g mastra
```

  </Tabs.Tab>
  <Tabs.Tab>
```bash copy 
yarn i -g mastra
```
</Tabs.Tab>
  <Tabs.Tab>
```bash copy 
pnpm i -g mastra
```
</Tabs.Tab>
</Tabs>

### Initialize Mastra

To initialize mastra in your project by following the interactive setup, run:

```bash copy
mastra init
```

### Set Up your API Key

Add the API key for your configured LLM provider in your `.env` file.

```env
OPENAI_API_KEY=<your-openai-key>
```

## Non-Interactive Mode

If you prefer to run the command with flags (non-interactive mode) and include the example code, you can use:

```bash copy
mastra init --dir src/mastra --components agents,tools --llm openai --example
```

This allows you to specify your preferences upfront without being prompted.

- [Mastra Init reference](../reference/cli/init.mdx)


================================================================================
Source: src/pages/docs/rag/chunking-and-embedding.mdx
================================================================================

## Chunking and Embedding Documents

Before processing, create a MDocument instance from your content. You can initialize it from various formats:

```ts showLineNumbers copy
const docFromText = MDocument.fromText("Your plain text content...");
const docFromHTML = MDocument.fromHTML("<html>Your HTML content...</html>");
const docFromMarkdown = MDocument.fromMarkdown("# Your Markdown content...");
const docFromJSON = MDocument.fromJSON(`{ "key": "value" }`);
```

## Step 1: Document Processing

Use `chunk` to split documents into manageable pieces. Mastra supports multiple chunking strategies optimized for different document types: 

- `recursive`: Smart splitting based on content structure
- `character`: Simple character-based splits
- `token`: Token-aware splitting
- `markdown`: Markdown-aware splitting
- `html`: HTML structure-aware splitting
- `json`: JSON structure-aware splitting
- `latex`: LaTeX structure-aware splitting

Here's an example of how to use the `recursive` strategy: 

```ts showLineNumbers copy 
const chunks = await doc.chunk({
  strategy: "recursive",
  size: 512,
  overlap: 50,
  separator: "\n",
  extract: {
    metadata: true, // Optionally extract metadata
  }
});
```
 
**Note:** Metadata extraction may use LLM calls, so ensure your API key is set.

We go deeper into chunking strategies in our [chunk documentation](/docs/reference/rag/chunk.mdx). 

## Step 2: Embedding Generation 

Transform chunks into embeddings using your preferred provider. Mastra supports both OpenAI and Cohere embeddings:

### Using OpenAI

```ts showLineNumbers copy
import { embed } from "@mastra/rag";

const embeddings = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3
});
```

### Using Cohere

```ts showLineNumbers copy
const embeddings = await embed(chunks, {
  provider: "COHERE",
  model: "embed-english-v3.0",
  maxRetries: 3
});
```

The embedding functions return vectors, arrays of numbers representing the semantic meaning of your text, ready for similarity searches in your vector database.

## Example: Complete Pipeline 

Here's an example showing document processing and embedding generation with both providers:

```ts showLineNumbers copy
import { MDocument, embed } from "@mastra/rag";

// Initialize document
const doc = MDocument.fromText(`
  Climate change poses significant challenges to global agriculture.
  Rising temperatures and changing precipitation patterns affect crop yields.
`);

// Create chunks
const chunks = await doc.chunk({
  strategy: "recursive",
  size: 256,
  overlap: 50
});

// Generate embeddings with OpenAI
const openAIEmbeddings = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002"
});

// Generate embeddings with Cohere
const cohereEmbeddings = await embed(chunks, {
  provider: "COHERE",
  model: "embed-english-v3.0"
});

// Store embeddings in your vector database
await vectorStore.upsert("embeddings", embeddings);
```

This example demonstrates how to process a document, split it into chunks, generate embeddings with both OpenAI and Cohere, and store the results in a vector database.

For more examples of different chunking strategies and embedding configurations, see:

- [Adjust Chunk Size](/docs/reference/rag/chunk.mdx#adjust-chunk-size)
- [Adjust Chunk Delimiters](/docs/reference/rag/chunk.mdx#adjust-chunk-delimiters)
- [Embed Text with Cohere](/docs/reference/rag/embed.mdx#using-cohere)


================================================================================
Source: src/pages/docs/rag/overview.mdx
================================================================================

# RAG (Retrieval-Augmented Generation) in Mastra

RAG in Mastra helps you enhance LLM outputs by incorporating relevant context from your own data sources, improving accuracy and grounding responses in real information.

Mastra's RAG system provides:
- Standardized APIs to process and embed documents
- Support for multiple vector stores
- Chunking and embedding strategies for optimal retrieval
- Observability for tracking embedding and retrieval performance

## Example 

To implement RAG, you process your documents into chunks, create embeddings, store them in a vector database, and then retrieve relevant context at query time.

```ts showLineNumbers copy
import { MDocument, embed, PgVector } from "@mastra/rag";
import { z } from "zod";

// 1. Initialize document   
const doc = MDocument.fromText(`Your document text here...`);

// 2. Create chunks
const chunks = await doc.chunk({
  strategy: "recursive",
  size: 512,
  overlap: 50
});

// 3. Generate embeddings
const { embeddings } = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002"
});

// 4. Store in vector database
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING);
await pgVector.store(embeddings);

// 5. Query similar chunks
const results = await pgVector.query("Your search query", {
  topK: 3
});

console.log("Similar chunks:", results);
``` 

This example shows the essentials: initialize a document, create chunks, generate embeddings, store them, and query for similar content.

## Document Processing

The basic building block of RAG is document processing. Documents can be chunked using various strategies (recursive, sliding window, etc.) and enriched with metadata. See the [chunking and embedding doc](./chunking-and-embedding.mdx).

## Vector Storage

Mastra supports multiple vector stores for embedding persistence and similarity search, including pgvector, Pinecone, and Qdrant. See the [vector database doc](./vector-databases.mdx).

## Observability and Debugging

Mastra's RAG system includes observability features to help you optimize your retrieval pipeline:

- Track embedding generation performance and costs
- Monitor chunk quality and retrieval relevance
- Analyze query patterns and cache hit rates
- Export metrics to your observability platform 

See the [OTel Configuration](../reference/observability/otel-config.mdx) page for more details. 

## More resources
- [Chain of Thought RAG Example](../examples/rag/cot-workflow-rag.mdx)
- [All RAG Examples](../../examples/rag) (including different chunking strategies, embedding models, and vector stores)





================================================================================
Source: src/pages/docs/rag/retrieval.mdx
================================================================================

## Retrieval in RAG Systems

After storing embeddings, you need to retrieve relevant chunks to answer user queries. 

Mastra provides flexible retrieval options with support for semantic search, filtering, and re-ranking.

## How Retrieval Works

1. The user's query is converted to an embedding using the same model used for document embeddings
2. This embedding is compared to stored embeddings using vector similarity
3. The most similar chunks are retrieved and can be optionally:
  - Filtered by metadata
  - Re-ranked for better relevance
  - Processed through a knowledge graph

## Basic Retrieval

The simplest approach is direct semantic search. This method uses vector similarity to find chunks that are semantically similar to the query:

```ts showLineNumbers copy
import { embed, EmbedResult } from "@mastra/rag";

// Convert query to embedding
const { embedding } = await embed(
  "What are the main points in the article?",
  {
    provider: "OPEN_AI",
    model: "text-embedding-ada-002",
    maxRetries: 3,
  }
);

// Query vector store
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING);
const results = await pgVector.query("embeddings", embedding, 10);
```

Results include both the text content and a similarity score:

```ts showLineNumbers copy
[
  {
    text: "Climate change poses significant challenges...",
    score: 0.89,
    metadata: { source: "article1.txt" }
  },
  {
    text: "Rising temperatures affect crop yields...",
    score: 0.82,
    metadata: { source: "article1.txt" }
  }
  // ... more results
]
```

## Advanced Retrieval options

### Metadata Filtering

Filter results based on metadata fields to narrow down the search space. This is useful when you have documents from different sources or time periods:

```ts showLineNumbers copy
const results = await pgVector.query("embeddings", embedding, {
  topK: 10,
  filter: {
    source: "article1.txt",
    date: { $gt: "2023-01-01" }
  }
});
``` 

### Re-ranking

Initial vector similarity search can sometimes miss nuanced relevance. Re-ranking is a more computationally expensive process, but more accurate algorithm that improves results by:

- Considering word order and exact matches
- Applying more sophisticated relevance scoring
- Using a method called cross-attention between query and documents

Here's how to set up re-ranking:

```ts showLineNumbers copy
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'embeddings',
  options: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002'
  },
  topK: 10,
  reranker: {
    type: 'cross-encoder',
    model: 'cross-encoder/ms-marco-MiniLM-L-6-v2'
  }
});
```

### Graph-based Retrieval

For documents with complex relationships, graph-based retrieval can follow connections between chunks. This helps when:

- Information is spread across multiple documents
- Documents reference each other
- You need to traverse relationships to find complete answers

Example setup:

```ts showLineNumbers copy
const graphQueryTool = createGraphQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'embeddings',
  graphOptions: {
    relationTypes: ['references', 'similar_to'],
    maxHops: 2
  }
});
```

## Example implementations

For complete examples of these retrieval methods in action, see:

- [Basic Retrieval](/docs/examples/rag/basic-retrieval.mdx)
- [Metadata Filtering](/docs/examples/rag/metadata-filtering.mdx)
- [Re-ranking Results](/docs/examples/rag/re-ranking.mdx)
- [Graph-based Retrieval](/docs/examples/rag/graph-retrieval.mdx)

================================================================================
Source: src/pages/docs/rag/vector-databases.mdx
================================================================================

## Vector Storage in Mastra

After generating embeddings, you need to store them in a database that supports vector similarity search. Mastra provides flexible options for vector storage, supporting both embedded and dedicated vector databases.

## Choosing a Vector Database

When selecting a vector database, consider:

- **Infrastructure:** Do you want to use your existing PostgreSQL database (with PgVector) or a dedicated vector database (Pinecone, Qdrant)?
- **Scale**: How many vectors will you store and query? Dedicated solutions like Pinecone often handle larger scales better.
- **Query Performance**: Need sub-second queries over millions of vectors? Consider specialized databases like Qdrant.
- **Management**: Are you comfortable managing another database, or prefer using your existing PostgreSQL setup? 

## Supported databases

### PostgreSQL with PgVector 

Best for teams already using PostgreSQL who want to minimize infrastructure complexity: 

``` ts showLineNumbers copy

import { PgVector } from '@mastra/rag';

const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING);

// Create an index (dimension = 1536 for text-embedding-ada-002)
await pgVector.createIndex("embeddings", 1536);

// Store embeddings with metadata
await pgVector.upsert(
  "embeddings",
  embeddings,
  chunks.map(chunk => ({ text: chunk.text }))
);
``` 

### Pinecone

Ideal for production deployments needing scalability: 


``` ts showLineNumbers copy

import { PineconeVector } from '@mastra/rag';

const pinecone = new PineconeVector(process.env.PINECONE_API_KEY);

// Create an index
await pinecone.createIndex("my-index", 1536);

// Store embeddings
await pinecone.upsert(
  "my-index",
  embeddings,
  chunks.map(chunk => ({ text: chunk.text }))
);
```

### Qdrant

Best for high-performance vector search: 

``` ts showLineNumbers copy

import { QdrantVector } from '@mastra/rag';

const qdrant = new QdrantVector({
  url: process.env.QDRANT_URL,
  apiKey: process.env.QDRANT_API_KEY
});

// Create collection
await qdrant.createCollection("my-collection", 1536);

// Store embeddings
await qdrant.upsert(
  "my-collection",
  embeddings,
  chunks.map(chunk => ({ text: chunk.text }))
);
```

## Adding Metadata

All vector stores support adding metadata to your vectors, which enables filtering during retrieval:

``` ts showLineNumbers copy
// Store embeddings with rich metadata
await vectorStore.upsert(
  "embeddings",
  embeddings,
  chunks.map(chunk => ({
    text: chunk.text,
    source: chunk.source,
    category: chunk.category,
    timestamp: new Date().toISOString()
  }))
);
```

## Performance Considerations

- **Indexing:** Create appropriate indexes before bulk insertions 
- **Batch Size:** Use bath oeprations for large insertions
- **Metadata:** Only store metadata you'll query against
- **Dimensions:** Match embedding dimensions to your model (eg., 1536 for text-embedding-ada-002)

## Examples
For complete examples of different vector store implementations, see:

- [Insert Embedding in PgVector](/docs/examples/rag/pgstore.mdx)
- [Insert Embedding in Pinecone](/docs/examples/rag/pinecone.mdx)
- [Basic RAG with Vector Storage](/docs/examples/rag/basic-rag.mdx)


================================================================================
Source: src/pages/docs/reference/agents/createTool.mdx
================================================================================

---
title: createTool
description: API Reference for createTool.
---

# `createTool()`

Tools are typed functions that can be executed by agents or workflows, with built-in integration access and parameter validation. Each tool has a schema that defines its inputs, an executor function that implements its logic, and access to configured integrations.

```ts filename="src/mastra/tools/index.ts" showLineNumbers copy
import { createTool } from "@mastra/core";
import { z } from "zod";

const getStockPrice = async (symbol: string) => {
  const data = await fetch(
    `https://mastra-stock-data.vercel.app/api/stock-data?symbol=${symbol}`,
  ).then((r) => r.json());
  return data.prices["4. close"];
};

export const stockPrices = createTool({
  id: "Get Stock Price",
  inputSchema: z.object({
    symbol: z.string(),
  }),
  description: `Fetches the last day's closing stock price for a given symbol`,
  execute: async ({ context }) => {
    console.log("Using tool to fetch stock price for", context.symbol);
    return {
      symbol: context.symbol,
      currentPrice: await getStockPrice(context.symbol),
    };
  },
});
```

## API Signature

### Parameters

<PropertiesTable
  content={[
    {
      name: "label",
      type: "string",
      required: true,
      description: 'Name of the tool (e.g., "Get Stock Prices")',
    },
    {
      name: "schema",
      type: "ZodSchema",
      required: true,
      description: "Zod schema for validating inputs",
    },
    {
      name: "description",
      type: "string",
      required: true,
      description: "Clear explanation of what market data the tool provides",
    },
    {
      name: "executor",
      type: "(params: ExecutorParams) => Promise<any>",
      required: true,
      description: "Async function that fetches the requested market data",
      properties: [
        {
          type: "ExecutorParams",
          parameters: [
            {
              name: "data",
              type: "object",
              description: "The validated input data (in this case, symbol)",
            },
            {
              name: "integrationsRegistry",
              type: "function",
              description: "Function to get connected integrations",
            },
            {
              name: "runId",
              type: "string",
              isOptional: true,
              description: "The runId of the current run",
            },
            {
              name: "agents",
              type: "Map<string, Agent<any>>",
              description: "Map of registered agents",
            },
            {
              name: "engine",
              isOptional: true,
              type: "MastraEngine",
              description: "Mastra engine instance",
            },
            {
              name: "llm",
              type: "LLM",
              description: "LLM instance",
            },
          ],
        },
      ],
    },
    {
      name: "outputSchema",
      type: "ZodSchema",
      isOptional: true,
      description: "Zod schema for validating outputs",
    },
  ]}
/>

### Returns

<PropertiesTable
  content={[
    {
      name: "ToolApi",
      type: "object",
      description:
        "The tool API object that includes the schema, label, description, and executor function.",
      properties: [
        {
          type: "ToolApi",
          parameters: [
            {
              name: "schema",
              type: "ZodSchema<IN>",
              description: "Zod schema for validating inputs.",
            },
            {
              name: "label",
              type: "string",
              description: "Name of the tool.",
            },
            {
              name: "description",
              type: "string",
              description: "Description of the tool's functionality.",
            },
            {
              name: "outputSchema",
              type: "ZodSchema<OUT>",
              isOptional: true,
              description: "Zod schema for validating outputs.",
            },
            {
              name: "executor",
              type: "(params: IntegrationApiExcutorParams<IN>) => Promise<OUT>",
              description: "Async function that executes the tool's logic.",
            },
          ],
        },
      ],
    },
  ]}
/>


================================================================================
Source: src/pages/docs/reference/agents/generate.mdx
================================================================================


# generate()

The `generate()` method is used to interact with an agent to produce text or structured responses. This method accepts `messages` and an optional `options` object as parameters.

## Parameters

### `messages`

The `messages` parameter can be:

- A single string
- An array of strings
- An array of message objects with `role` and `content` properties

The message object structure:

```typescript
interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}
```

### `options` (Optional)

An optional object that can include:

- `structuredOutput` (or `schema`): An object defining the expected structure of the output. Can be a JSON Schema or a Zod schema.
- Additional options like `onStepFinish`, `maxSteps`, `threadId`, `resourceId`, etc.

<PropertiesTable
  content={[
    {
      name: "messages",
      type: "string | Array<string> | Array<Message>",
      description: "The messages to be processed by the agent. Can be a single string, an array of strings, or an array of message objects with `role` and `content`.",
    },
    {
      name: "options",
      type: "object",
      isOptional: true,
      description: "Additional options for the `generate` method.",
      properties: [
        {
          name: "structuredOutput",
          type: "object | Zod schema",
          isOptional: true,
          description: "Defines the expected structure of the output. Can be a JSON Schema object or a Zod schema.",
        },
        {
          name: "onStepFinish",
          type: "(step: string) => void",
          isOptional: true,
          description: "Callback function called after each execution step. Receives step details as a JSON string.",
        },
        {
          name: "maxSteps",
          type: "number",
          isOptional: true,
          default: 5,
          description: "Maximum number of execution steps allowed.",
        },
        {
          name: "threadId",
          type: "string",
          isOptional: true,
          description: "Identifier for the conversation thread. Allows for maintaining context across multiple interactions.",
        },
        {
          name: "resourceId",
          type: "string",
          isOptional: true,
          description: "Identifier for the user or resource interacting with the agent.",
        },
        {
          name: "context",
          type: "Array<Message>",
          isOptional: true,
          description: "Additional context messages to provide to the agent.",
        },
      ],
    },
  ]}
/>

## Returns

The return value of the `generate()` method depends on the options provided, specifically the `structuredOutput` option.

### PropertiesTable for Return Values

<PropertiesTable
  content={[
    {
      name: "text",
      type: "string",
      isOptional: true,
      description: "The generated text response. Present if `structuredOutput` is not provided.",
    },
    {
      name: "object",
      type: "object",
      isOptional: true,
      description: "The generated structured response based on the provided schema. Present if `structuredOutput` is provided.",
    },
    {
      name: "toolCalls",
      type: "Array<ToolCall>",
      isOptional: true,
      description: "The tool calls made during the generation process.",
    },
    {
      name: "error",
      type: "string",
      isOptional: true,
      description: "Error message if the generation fails.",
    },
  ]}
/>

#### ToolCall Structure

<PropertiesTable
  content={[
    {
      name: "toolName",
      type: "string",
      description: "The name of the tool invoked.",
    },
    {
      name: "args",
      type: "any",
      description: "The arguments passed to the tool.",
    },
  ]}
/>

## Related Methods

For real-time streaming responses, see the [`stream()`](./stream.mdx) method documentation.


================================================================================
Source: src/pages/docs/reference/agents/getAgent.mdx
================================================================================

---
title: getAgent
description: API Reference for getAgent.
---

# `getAgent()`

Retrieve an agent based on the provided configuration

```ts showLineNumbers copy
async function getAgent({
  connectionId,
  agent,
  apis,
  logger,
}: {
  connectionId: string;
  agent: Record<string, any>;
  apis: Record<string, IntegrationApi>;
  logger: any;
}): Promise<(props: { prompt: string }) => Promise<any>> {
  return async (props: { prompt: string }) => {
    return { message: "Hello, world!" };
  };
}
```

## API Signature

### Parameters

<PropertiesTable
  content={[
    {
      name: "connectionId",
      type: "string",
      description: "The connection ID to use for the agent's API calls.",
    },
    {
      name: "agent",
      type: "Record<string, any>",
      description: "The agent configuration object.",
    },
    {
      name: "apis",
      type: "Record<string, IntegrationAPI>",
      description: "A map of API names to their respective API objects.",
    },
  ]}
/>

### Returns

<PropertiesTable content={[]} />


================================================================================
Source: src/pages/docs/reference/agents/stream.mdx
================================================================================

# `stream()`

The `stream()` method enables real-time streaming of responses from an agent. This method accepts `messages` and an optional `options` object as parameters, similar to `generate()`.

## Parameters

### `messages`

The `messages` parameter can be:

- A single string
- An array of strings
- An array of message objects with `role` and `content` properties

#### Message Object Structure

```typescript
interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}
```

### `options` (Optional)

An optional object that can include:

<PropertiesTable
  content={[
    {
      name: 'output',
      type: 'string | JSONSchema7 | ZodSchema',
      isOptional: true,
      default: "'text'",
      description: 'Defines the output format. Can be "text" or a schema for structured output.',
    },
    {
      name: 'context',
      type: 'CoreMessage[]',
      isOptional: true,
      description: 'Additional context messages to provide to the agent.',
    },
    {
      name: 'threadId',
      type: 'string',
      isOptional: true,
      description: 'Identifier for the conversation thread. Allows for maintaining context across multiple interactions.',
    },
    {
      name: 'resourceId',
      type: 'string',
      isOptional: true,
      description: 'Identifier for the user or resource interacting with the agent.',
    },
    {
      name: 'onFinish',
      type: '(result: string) => Promise<void> | void',
      isOptional: true,
      description: 'Callback function called when streaming is complete.',
    },
    {
      name: 'onStepFinish',
      type: '(step: string) => void',
      isOptional: true,
      description: 'Callback function called after each step during streaming.',
    },
    {
      name: 'maxSteps',
      type: 'number',
      isOptional: true,
      default: '5',
      description: 'Maximum number of steps allowed during streaming.',
    },
    {
      name: 'toolsets',
      type: 'ToolsetsInput',
      isOptional: true,
      description: 'Additional toolsets to make available to the agent during this stream.',
    }
  ]}
/>

## Returns

The method returns a promise that resolves to an object containing one or more of the following properties:

<PropertiesTable
  content={[
    {
      name: 'textStream',
      type: 'AsyncIterable<string>',
      isOptional: true,
      description: 'An async iterable stream of text chunks. Present when output is "text".',
    },
    {
      name: 'objectStream',
      type: 'AsyncIterable<object>',
      isOptional: true,
      description: 'An async iterable stream of structured data. Present when a schema is provided.',
    },
    {
      name: 'object',
      type: 'Promise<object>',
      isOptional: true,
      description: 'A promise that resolves to the final structured output when using a schema.',
    }
  ]}
/>

## Examples

### Basic Text Streaming

```typescript
const stream = await myAgent.stream([
  { role: "user", content: "Tell me a story." }
]);

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
```

### Structured Output Streaming with Thread Context

```typescript
const schema = {
  type: 'object',
  properties: {
    summary: { type: 'string' },
    nextSteps: { type: 'array', items: { type: 'string' } }
  },
  required: ['summary', 'nextSteps']
};

const response = await myAgent.stream(
  "What should we do next?",
  {
    output: schema,
    threadId: "project-123",
    onFinish: text => console.log("Finished:", text)
  }
);

for await (const chunk of response.textStream) {
  console.log(chunk);
}

const result = await response.object;
console.log("Final structured result:", result);
```

The key difference between Agent's `stream()` and LLM's `stream()` is that Agents maintain conversation context through `threadId`, can access tools, and integrate with the agent's memory system.


================================================================================
Source: src/pages/docs/reference/cli/deploy.mdx
================================================================================

# Deploy

## `mastra deploy vercel`

Deploy your Mastra project to Vercel.

## `mastra deploy cloudflare`

Deploy your Mastra project to Cloudflare.

## `mastra deploy netlify`

Deploy your Mastra project to Netlify.

### Flags

- `-d, --dir <dir>`: Path to your mastra folder


================================================================================
Source: src/pages/docs/reference/cli/dev.mdx
================================================================================

# `mastra dev` Reference

The `mastra dev` command starts a development server that exposes REST endpoints for your agents, tools, and workflows,

## Parameters

<PropertiesTable
  content={[
    {
      name: "--dir",
      type: "string",
      description:
        "Specifies the path to your Mastra folder (containing agents, tools, and workflows). Defaults to the current working directory.",
      isOptional: true,
    },
    {
      name: "--env",
      type: "string",
      description:
        "Specifies which environment file to load. Defaults to `.env.development`, falling back to `.env` if not found.",
      isOptional: true,
    },
    {
      name: "--tools",
      type: "string",
      description:
        "Comma-separated paths to additional tool directories that should be registered. For example: 'src/tools/dbTools,src/tools/scraperTools'.",
      isOptional: true,
    },
    {
      name: "--port",
      type: "number",
      description:
        "Specifies the port number for the development server. Defaults to 4111.",
      isOptional: true,
    },
  ]}
/>

## Routes

Starting the server with `mastra dev` exposes a set of REST endpoints by default:

### Agent Routes

Agents are expected to be exported from `src/mastra/agents`.

‚Ä¢ `GET /api/agents`
  - Lists the registered agents found in your Mastra folder.
‚Ä¢ `POST /api/agents/:agentId/generate`
  - Sends a text-based prompt to the specified agent, returning the agent‚Äôs response.

### Tool Routes

Tools are expected to be exported from `src/mastra/tools` (or the configured tools directory).

‚Ä¢ `POST /api/tools/:toolName`
  - Invokes a specific tool by name, passing input data in the request body.

### Workflow Routes

Workflows are expected to be exported from `src/mastra/workflows` (or the configured workflows directory).

‚Ä¢ `POST /api/workflows/:workflowName/start`
  - Starts the specified workflow.
‚Ä¢ `POST /api/workflows/:workflowName/:instanceId/event`
  - Sends an event or trigger signal to an existing workflow instance.
‚Ä¢ `GET /api/workflows/:workflowName/:instanceId/status`
  - Returns status info for a running workflow instance.

### OpenAPI Specification

‚Ä¢ `GET /openapi.json`
  - Returns an auto-generated OpenAPI specification for your project‚Äôs endpoints.

## Additional Notes

The port defaults to 4111.

Make sure you have your environment variables set up in your `.env.development` or `.env` file for any providers you use (e.g., `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`, etc.).

### Example request

To test an agent after running `mastra dev`:

```bash
curl -X POST http://localhost:4111/api/agents/myAgent/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      { "role": "user", "content": "Hello, how can you assist me today?" }
    ]
  }'
```

## Related Docs

- [REST Endpoints Overview](../../local-dev/mastra-dev.mdx) ‚Äì More detailed usage of the dev server and agent endpoints.
- [mastra deploy](../../deployment/deployment.mdx) ‚Äì Deploy your project to Vercel or Cloudflare.


================================================================================
Source: src/pages/docs/reference/cli/engine.mdx
================================================================================

# Engine

## `mastra engine add`

Installs the `@mastra/engine` dependency to your project. The Mastra engine enables:

- **Data Persistence**: Store conversation history, agent states, and vector embeddings
- **Background Processing**: Run long-running tasks and data synchronization jobs
- **RAG Capabilities**: Build and search knowledge bases with vector embeddings
- **Type Safety**: Generate TypeScript types from your database schema

While not required for basic agent interactions, the engine becomes essential when your application needs persistence, background tasks, or vector search capabilities.

This command sets up your development environment by:

1. Creating or updating your environment file with the correct database URL
2. Configuring the necessary environment variables
3. Running `docker-compose up` to start required Docker containers

## `mastra engine generate`

Generates the Drizzle database client and TypeScript types based on your database schema. Requires a valid database connection.

## `mastra engine up`

Runs `docker-compose up` to start required Docker containers.

- Accepts a `-f` or `--file` option for a custom Docker config file path.

## `mastra engine down`

Runs `docker-compose down` to stop required Docker containers.

- Accepts a `-f` or `--file` option for a custom Docker config file path.

## `mastra engine migrate`

Runs database migrations to keep your schema up to date.

- Requires a valid `DB_URL` in your environment file
- If `DB_URL` is missing, you'll be prompted to run `mastra engine up` first
- Automatically applies any pending migrations


================================================================================
Source: src/pages/docs/reference/cli/init.mdx
================================================================================

# Init

## `mastra init`

This creates a new Mastra project. You can run it in three different ways:

1. **Interactive Mode (Recommended)**
   Run without flags to use the interactive prompt, which will guide you through:

   - Choosing a directory for Mastra files
   - Selecting components to install (Agents, Tools, Workflows)
   - Choosing a default LLM provider (OpenAI, Anthropic, or Groq)
   - Deciding whether to include example code

2. **Quick Start with Defaults**

   ```bash
   mastra init --default
   ```

   This sets up a project with:

   - Source directory: `src/`
   - All components: agents, tools, workflows
   - OpenAI as the default provider
   - No example code

3. **Custom Setup**
   ```bash
   mastra init --dir src/mastra --components agents,tools --llm openai --example
   ```
   Options:
   - `-d, --dir`: Directory for Mastra files (defaults to src/mastra)
   - `-c, --components`: Comma-separated list of components (agents, tools, workflows)
   - `-l, --llm`: Default model provider (openai, anthropic, or groq)
   - `-k, --llm-api-key`: API key for the selected LLM provider (will be added to .env file)
   - `-e, --example`: Include example code
   - `-ne, --no-example`: Skip example code


================================================================================
Source: src/pages/docs/reference/core/mastra-class.mdx
================================================================================

# The Mastra Class

The Mastra class is the core entry point for your application. It manages agents, workflows, and server endpoints.

## Constructor Options

<PropertiesTable
  content={[
    {
      name: "agents",
      type: "Agent[]",
      description: "Array of Agent instances to register",
      isOptional: true,
      defaultValue: "[]",
    },
    {
      name: "tools",
      type: "Record<string, ToolApi>",
      description:
        "Custom tools to register. Structured as a key-value pair, with keys being the tool name and values being the tool function.",
      isOptional: true,
      defaultValue: "{}",
    },
    {
      name: "integrations",
      type: "Integration[]",
      description:
        "Array of Mastra integrations to register. Will be used by agents, workflows, and tools.",
      isOptional: true,
      defaultValue: "[]",
    },
    {
      name: "engine",
      type: "MastraEngine",
      description: "Database engine instance",
      isOptional: true,
    },
    {
      name: "vectors",
      type: "Record<string, MastraVector>",
      description:
        "Vector store instance, used for semantic search and vector-based tools (eg Pinecone, PgVector or Qdrant)",
      isOptional: true,
    },
    {
      name: "logger",
      type: "Logger",
      description: "Logger instance created with createLogger()",
      isOptional: true,
      defaultValue: "Console logger with INFO level",
    },
    {
      name: "workflows",
      type: "Workflow[]",
      description: "Array of Workflow instances to register",
      isOptional: true,
      defaultValue: "[]",
    },
  ]}
/>

## Initialization

The Mastra class is typically initialized in your `src/mastra/index.ts` file:

```typescript copy filename=src/mastra/index.ts
import { Mastra, createLogger } from "@mastra/core";

// Basic initialization
export const mastra = new Mastra({});

// Full initialization with all options
export const mastra = new Mastra({
  agents: {},
  workflows: [],
  integrations: [],
  logger: createLogger({
    name: "Mastra",
    level: "info",
  }),
  engine: {},
  tools: {},
  vectors: {},
});
```

You can think of the `Mastra` class as a top-level registry. When you register tools with Mastra, your registered agents and workflows can use them. When you register integrations with Mastra, agents, workflows, and tools can use them.

## Methods

<PropertiesTable
  content={[
    {
      name: "getAgent(name)",
      type: "Agent",
      description:
        "Returns an agent instance by id. Throws if agent not found.",
      example: 'const agent = mastra.getAgent("agentOne");',
    },
    {
      name: "sync<K>(key, params)",
      type: "Promise<void>",
      description:
        "Executes a sync operation. Requires engine to be configured. Throws if sync not found.",
      example: 'await mastra.sync("googleDocs", { folder: "123" });',
    },
    {
      name: "setLogger({ key, logger })",
      type: "void",
      description:
        "Sets a logger for a specific component (AGENT | WORKFLOW). Advanced use case.",
      example: 'mastra.setLogger({ key: "AGENT", logger });',
    },
    {
      name: "getLogger(key)",
      type: "Logger | undefined",
      description:
        "Gets the logger for a specific component. Advanced use case.",
      example: 'const logger = mastra.getLogger("AGENT");',
    },
  ]}
/>

## Error Handling

The Mastra class methods throw typed errors that can be caught:

```typescript copy
try {
  const tool = mastra.getTool("nonexistentTool");
} catch (error) {
  if (error instanceof Error) {
    console.log(error.message); // "Tool with name nonexistentTool not found"
  }
}
```


================================================================================
Source: src/pages/docs/reference/llm/generate.mdx
================================================================================

# `generate()`

The `generate()` method is used to interact with the language model to produce text or structured responses. This method accepts `messages` and an optional `options` object as parameters.

## Parameters

### `messages`

The `messages` parameter can be:

- A single string
- An array of strings
- An array of message objects with `role` and `content` properties

#### Message Object Structure

```typescript
interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}
```

### `options` (Optional)

<PropertiesTable
  content={[
    {
      name: 'messages',
      type: 'string | string[] | Message[]',
      description: 'The messages to be processed by the LLM. Can be a single string, an array of strings, or an array of message objects with `role` and `content`.',
    },
    {
      name: 'options',
      type: 'object',
      isOptional: true,
      description: 'Additional options for the `generate` method.',
      properties: [
        {
          name: 'output',
          type: 'string | JSONSchema7 | ZodSchema',
          isOptional: true,
          default: "'text'",
          description: 'Defines the output format. Can be "text" or a schema for structured output.',
        },
        {
          name: 'onFinish',
          type: '(result: string) => Promise<void> | void',
          isOptional: true,
          description: 'Callback function called when generation is complete.',
        },
        {
          name: 'onStepFinish',
          type: '(step: string) => void',
          isOptional: true,
          description: 'Callback function called after each step during generation.',
        },
        {
          name: 'maxSteps',
          type: 'number',
          isOptional: true,
          default: '5',
          description: 'Maximum number of steps allowed during generation.',
        },
        {
          name: 'tools',
          type: 'ToolsInput',
          isOptional: true,
          description: 'Tools available for the LLM to use during generation.',
        },
        {
          name: 'runId',
          type: 'string',
          isOptional: true,
          description: 'Unique identifier for the generation run, useful for tracing and logging.',
        }
      ],
    },
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: 'text',
      type: 'string',
      isOptional: true,
      description: 'The generated text response. Present when output is "text".',
    },
    {
      name: 'object',
      type: 'object',
      isOptional: true,
      description: 'The generated structured response based on the provided schema. Present when a schema is provided.',
    },
    {
      name: 'error',
      type: 'string',
      isOptional: true,
      description: 'Error message if the generation fails.',
    },
  ]}
/>

## Examples

### Basic Text Generation

```typescript
const response = await llm.generate("What is AI?");
console.log(response.text);
```

### Structured Output

```typescript
import { z } from "zod";

const mySchema = z.object({
  definition: z.string(),
  examples: z.array(z.string()),
});

const response = await llm.generate(
  "Define machine learning and give examples.",
  {
    output: mySchema,
  },
);

console.log(response.object);
```

## Related Methods

For real-time streaming responses, see the [`stream()`](./stream.mdx) method documentation.

================================================================================
Source: src/pages/docs/reference/llm/providers-and-models.mdx
================================================================================

# Providers and Models

Mastra supports a variety of language models from different providers. There are four types of providers we support:

- **Most popular providers.** OpenAI, Anthropic, Google Gemini. These are the most popular models and are highly recommended for most use cases. We will reference them and use them in docs and examples.
- **Other natively supported providers.** Mastra is built on AI SDK and supports a number of AI SDK supported models out of the box. We will always try to use these models in docs and examples.
- **Community supported providers.** A number of other providers have built AI SDK integrations (via creating an AI SDK provider).
- **Custom providers through Portkey.** If a provider does not have an AI SDK integration, you can use them through Portkey (an open-source AI gateway).

## Most popular providers

| Provider      | Supported Models                                                                                                                                                         |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| OpenAI        | `gpt-4`, `gpt-4-turbo`, `gpt-3.5-turbo`, `gpt-4o`, `gpt-4o-mini`                                                                                                         |
| Anthropic     | `claude-3-5-sonnet-20241022`, `claude-3-5-sonnet-20240620`, `claude-3-5-haiku-20241022`, `claude-3-opus-20240229`, `claude-3-sonnet-20240229`, `claude-3-haiku-20240307` |
| Google Gemini | `gemini-1.5-pro-latest`, `gemini-1.5-pro`, `gemini-1.5-flash-latest`, `gemini-1.5-flash`, `gemini-2.0-flash-exp-latest`, `gemini-2.0-flash-thinking-exp-1219`, `gemini-exp-1206` |

## Other natively supported providers

| Provider         | Supported Models                                                                                                                                |
| ---------------- | ----------------------------------------------------------------------------------------------------------------------------------------------- |
| Groq             | `llama3-groq-70b-8192-tool-use-preview`, `llama3-groq-8b-8192-tool-use-preview`, `gemma2-9b-it`, `gemma-7b-it`                                  |
| Perplexity       | `llama-3.1-sonar-small-128k-online`, `llama-3.1-sonar-large-128k-online`, `llama-3.1-sonar-huge-128k-online`, `llama-3.1-sonar-small-128k-chat` |
| TogetherAI       | `codellama/CodeLlama-34b-Instruct-hf`, `upstage/SOLAR-10.7B-Instruct-v1.0`, `mistralai/Mixtral-8x7B-v0.1`, `WhereIsAI/UAE-Large-V1`             |
| LM Studio        | `qwen2-7b-instruct-4bit`, `qwen2-math-1.5b`, `qwen2-0.5b`, `aya-23-8b`, `mistral-7b-v0.3`                                                       |
| Baseten          | `llama-3.1-70b-instruct`, `qwen2.5-7b-math-instruct`, `qwen2.5-14b-instruct`, `qwen2.5-32b-coder-instruct`                                      |
| Fireworks        | `llama-3.1-405b-instruct`, `llama-3.1-70b-instruct`, `llama-3.1-8b-instruct`, `llama-3.2-3b-instruct`                                           |
| Mistral          | `pixtral-large-latest`, `mistral-large-latest`, `mistral-small-latest`, `ministral-3b-latest`                                                   |
| X Grok           | `grok-beta`, `grok-vision-beta`                                                                                                                 |
| Cohere           | `command-r-plus`                                                                                                                                |
| Azure            | `gpt-35-turbo-instruct`                                                                                                                         |
| Amazon           | `amazon-titan-tg1-large`, `amazon-titan-text-express-v1`, `anthropic-claude-3-5-sonnet-20241022-v2:0`                                           |
| Anthropic Vertex | `claude-3-5-sonnet@20240620`, `claude-3-opus@20240229`, `claude-3-sonnet@20240229`, `claude-3-haiku@20240307`                                   |

## Community supported providers

You can see a list of Vercel's community supported providers [here](https://sdk.vercel.ai/providers/community-providers). You can also write your own provider if desired.

##### Example: Custom Provider - Ollama

Here is an example of using a custom provider, Ollama, to create a model instance.

```bash npm2yarn copy
npm install ollama-ai-provider
```

Import and configure the Ollama model by using `createOllama` from the `ollama-ai-provider` package.

```typescript copy showLineNumbers
import { createOllama } from "ollama-ai-provider";

const ollama = createOllama({
  // optional settings, e.g.
  baseURL: "https://api.ollama.com",
});
```

After creating the instance, you can use it like any other model in Mastra.

```typescript copy showLineNumbers lines={9-14, 27} filename="src/mastra/index.ts"
import { Mastra, type ModelConfig } from "@mastra/core";
import { createOllama } from "ollama-ai-provider";

const ollama = createOllama({
  // optional settings, e.g.
  baseURL: "https://api.ollama.com",
});

const modelConfig: ModelConfig = {
  model: ollama.chat("gemma"), // The model instance created by the Ollama provider
  apiKey: process.env.OLLAMA_API_KEY,
  provider: "Ollama",
  toolChoice: "auto", // Controls how the model handles tool/function calling
};

const mastra = new Mastra({});

const llm = mastra.llm;

const response = await llm.generate(
  [
    {
      role: "user",
      content: "What is machine learning?",
    },
  ],
  { model: modelConfig },
);
```

### Portkey supported providers

[Portkey](https://portkey.ai/) is an open-source AI gateway with support for 200+ providers, so if the provider you want isn't available through AI SDK, it probably is through Portkey.

You can refer to the [Portkey documentation](https://docs.portkey.ai/docs/custom-models) for more details on how to implement custom models.

================================================================================
Source: src/pages/docs/reference/llm/stream.mdx
================================================================================

# `stream()`

The `stream()` method enables real-time streaming of responses from the language model. This method accepts `messages` and an optional `options` object as parameters, similar to `generate()`.

## Parameters

### `messages`

The `messages` parameter can be:

- A single string
- An array of strings  
- An array of message objects with `role` and `content` properties

#### Message Object Structure

```typescript
interface Message {
  role: 'system' | 'user' | 'assistant';
  content: string;
}
```

### `options` (Optional)

An optional object that can include:

<PropertiesTable
  content={[
    {
      name: 'output',
      type: 'string | JSONSchema7 | ZodSchema',
      isOptional: true,
      default: "'text'",
      description: 'Defines the output format. Can be "text" or a schema for structured output.',
    },
    {
      name: 'onFinish',
      type: '(result: string) => Promise<void> | void',
      isOptional: true,
      description: 'Callback function called when streaming is complete.',
    },
    {
      name: 'onStepFinish',
      type: '(step: string) => void',
      isOptional: true,
      description: 'Callback function called after each step during streaming.',
    },
    {
      name: 'maxSteps',
      type: 'number',
      isOptional: true,
      default: '5',
      description: 'Maximum number of steps allowed during streaming.',
    },
    {
      name: 'tools',
      type: 'ToolsInput',
      isOptional: true,
      description: 'Tools available for the LLM to use during streaming.',
    },
    {
      name: 'runId',
      type: 'string',
      isOptional: true,
      description: 'Unique identifier for the streaming run, useful for tracing and logging.',
    }
  ]}
/>

## Returns

The method returns a promise that resolves to an object containing one or more of the following properties:

<PropertiesTable
  content={[
    {
      name: 'textStream',
      type: 'AsyncIterable<string>',
      isOptional: true,
      description: 'An async iterable stream of text chunks. Present when output is "text".',
    },
    {
      name: 'objectStream',
      type: 'AsyncIterable<object>',
      isOptional: true,
      description: 'An async iterable stream of structured data. Present when a schema is provided.',
    },
    {
      name: 'object',
      type: 'Promise<object>',
      isOptional: true,
      description: 'A promise that resolves to the final structured output when using a schema.',
    }
  ]}
/>

## Examples

### Basic Text Streaming

```typescript
const stream = await llm.stream("Tell me a story about a brave knight.");

for await (const chunk of stream.textStream) {
  process.stdout.write(chunk);
}
```

### Structured Output Streaming

```typescript
const schema = {
  type: 'object',
  properties: {
    answer: { type: 'number' },
    explanation: { type: 'string' }
  },
  required: ['answer', 'explanation']
};

const response = await llm.stream("What is 2+2?", {
  output: schema,
  onFinish: text => console.log("Finished:", text)
});

for await (const chunk of response.textStream) {
  console.log(chunk);
}

const result = await response.object;
console.log("Final structured result:", result);
```

================================================================================
Source: src/pages/docs/reference/mcp/client.mdx
================================================================================

---
title: MastraMCPClient
description: API Reference for MastraMCPClient - A client implementation for the Model Context Protocol.
---

# MastraMCPClient

The `MastraMCPClient` class provides a client implementation for interacting with Model Context Protocol (MCP) servers. It handles connection management, resource discovery, and tool execution through the MCP protocol.

## Constructor

Creates a new instance of the MastraMCPClient.

```typescript
constructor({
    name,
    version = '1.0.0',
    server,
    capabilities = {},
}: {
    name: string;
    server: StdioServerParameters;
    capabilities?: ClientCapabilities;
    version?: string;
})
```

### Parameters

<PropertiesTable
  content={[
    {
      name: 'name',
      type: 'string',
      description: 'The name identifier for this client instance.',
    },
    {
      name: 'version',
      type: 'string',
      isOptional: true,
      defaultValue: '1.0.0',
      description: 'The version of the client.',
    },
    {
      name: 'server',
      type: 'StdioServerParameters',
      description: 'Configuration parameters for the stdio server connection.',
    },
    {
      name: 'capabilities',
      type: 'ClientCapabilities',
      isOptional: true,
      defaultValue: '{}',
      description: 'Optional capabilities configuration for the client.',
    },
  ]}
/>

## Methods

### connect()

Establishes a connection with the MCP server.

```typescript
async connect(): Promise<void>
```

### disconnect()

Closes the connection with the MCP server.

```typescript
async disconnect(): Promise<void>
```

### resources()

Retrieves the list of available resources from the server.

```typescript
async resources(): Promise<ListResourcesResult>
```

### tools()

Fetches and initializes available tools from the server, converting them into Mastra-compatible tool formats.

```typescript
async tools(): Promise<Record<string, Tool>>
```

Returns an object mapping tool names to their corresponding Mastra tool implementations.

## Examples

### Using with Mastra Agent

```typescript
import { Agent } from '@mastra/core';
import { MastraMCPClient } from '@mastra/mcp-client';

// Initialize the EverArt MCP client
const everArtClient = new MastraMCPClient({
    name: 'everart',
    server: {
        command: '/usr/local/bin/docker',
        args: ['run', '-i', '--rm', '--network=host', '-e', 'EVERART_API_KEY', 'mcp/everart'],
        env: {
            EVERART_API_KEY: process.env.EVERART_API_KEY!,
        },
    },
});

// Create a Mastra Agent
const agent = new Agent({
    name: 'everart',
    instructions: 'You are my artist. Include the url in your response.',
    model: {
        provider: 'ANTHROPIC',
        name: 'claude-3-5-sonnet-20241022',
        toolChoice: 'auto',
    },
});

// Example usage in an async function
async function main() {
    try {
        // Connect to the MCP server
        await everArtClient.connect();
        
        // Get available tools
        const tools = await everArtClient.tools();
        
        // Use the agent with the MCP tools
        const response = await agent.generate('Can you make me a picture of a dog?', {
            toolsets: {
                everart: tools,
            },
        });
        
        console.log(response.text);
        
    } catch (error) {
        console.error('Error:', error);
    } finally {
        // Always disconnect when done
        await everArtClient.disconnect();
    }
}
```

## Related Information

- For more details about the Model Context Protocol, see the [@modelcontextprotocol/sdk documentation](https://github.com/modelcontextprotocol/sdk).

================================================================================
Source: src/pages/docs/reference/observability/create-logger.mdx
================================================================================

# createLogger()

The `createLogger()` function is used to instantiate a logger based on a given configuration. You can create console-based, file-based, or Upstash Redis-based loggers by specifying the type and any additional parameters relevant to that type.

### Usage

#### Console Logger (Development)

```typescript showLineNumbers copy
const consoleLogger = createLogger({ type: 'CONSOLE', level: 'DEBUG' });
consoleLogger.info('App started'); 
```

#### File Logger (Structured Logs)

```typescript showLineNumbers copy
const fileLogger = createLogger({ type: 'FILE', dirPath: 'my-logs', level: 'WARN' });
fileLogger.warn({ message: 'Low disk space', destinationPath: 'system', type: 'WORKFLOW' });
```

#### Upstash Logger (Remote Log Drain)

```typescript showLineNumbers copy
const upstashLogger = createLogger({
  type: 'UPSTASH',
  url: process.env.UPSTASH_URL!,
  token: process.env.UPSTASH_TOKEN!,
  level: 'info',
  key: 'production-logs',
});
await upstashLogger.info({ message: 'User signed in', destinationPath: 'auth', type: 'AGENT', runId: 'run_123' });
```

### Parameters

<PropertiesTable content={[ { name: 'type', type: 'CONSOLE' | 'FILE' | 'UPSTASH', description: 'Specifies the logger implementation to create.', }, { name: 'level', type: 'LogLevel', isOptional: true, default: 'INFO', description: 'Minimum severity level of logs to record. One of DEBUG, INFO, WARN, or ERROR.', }, { name: 'dirPath', type: 'string', isOptional: true, description: 'For FILE type only. Directory path where log files are stored (default: "logs").', }, { name: 'url', type: 'string', isOptional: true, description: 'For UPSTASH type only. Upstash Redis endpoint URL used for storing logs.', }, { name: 'token', type: 'string', isOptional: true, description: 'For UPSTASH type only. Upstash Redis access token.', }, { name: 'key', type: 'string', isOptional: true, default: 'logs', description: 'For UPSTASH type only. Redis list key under which logs are stored.', }, ]} />


================================================================================
Source: src/pages/docs/reference/observability/logger.mdx
================================================================================

# Logger Instance

A Logger instance is created by `createLogger()` and provides methods to record events at various severity levels. Depending on the logger type, messages may be written to the console, file, or an external service.

## Example

```typescript showLineNumbers copy
// Using a console logger
const logger = createLogger({ type: 'CONSOLE', level: 'INFO' });

logger.debug('Debug message'); // Won't be logged because level is INFO
logger.info({ message: 'User action occurred', destinationPath: 'user-actions', type: 'AGENT' }); // Logged
logger.error('An error occurred'); // Logged as ERROR
```

## Methods

<PropertiesTable
  content={[
    {
      name: 'debug',
      type: '(message: BaseLogMessage | string, ...args: any[]) => void | Promise<void>',
      description: 'Write a DEBUG-level log. Only recorded if level ‚â§ DEBUG.',
    },
    {
      name: 'info',
      type: '(message: BaseLogMessage | string, ...args: any[]) => void | Promise<void>',
      description: 'Write an INFO-level log. Only recorded if level ‚â§ INFO.',
    },
    {
      name: 'warn',
      type: '(message: BaseLogMessage | string, ...args: any[]) => void | Promise<void>',
      description: 'Write a WARN-level log. Only recorded if level ‚â§ WARN.',
    },
    {
      name: 'error',
      type: '(message: BaseLogMessage | string, ...args: any[]) => void | Promise<void>',
      description: 'Write an ERROR-level log. Only recorded if level ‚â§ ERROR.',
    },
    {
      name: 'cleanup',
      type: '() => Promise<void>',
      isOptional: true,
      description:
        'Cleanup resources held by the logger (e.g., network connections for Upstash). Not all loggers implement this.',
    },
  ]}
/>

**Note:** Some loggers require a `BaseLogMessage` object (with `message`, `destinationPath`, `type` fields). For instance, the `File` and `Upstash` loggers need structured messages.


================================================================================
Source: src/pages/docs/reference/observability/otel-config.mdx
================================================================================

# `OtelConfig`

The `OtelConfig` object is used to configure OpenTelemetry instrumentation, tracing, and exporting behavior within your application. By adjusting its properties, you can control how telemetry data (such as traces) is collected, sampled, and exported. 

To use the `OtelConfig` within Mastra, pass it as the value of the `telemetry` key when initializing Mastra. This will configure Mastra to use your custom OpenTelemetry settings for tracing and instrumentation.

```typescript showLineNumbers copy
import { Mastra } from 'mastra';

const otelConfig: OtelConfig = {
  serviceName: 'my-awesome-service',
  enabled: true,
  sampling: {
    type: 'ratio',
    probability: 0.5,
  },
  export: {
    type: 'otlp',
    endpoint: 'https://otel-collector.example.com/v1/traces',
    headers: {
      Authorization: 'Bearer YOUR_TOKEN_HERE',
    },
  },
};
```

### Properties

<PropertiesTable
  content={[
    {
      name: 'serviceName',
      type: 'string',
      isOptional: true,
      default: 'default-service',
      description:
        'Human-readable name used to identify your service in telemetry backends.',
    },
    {
      name: 'enabled',
      type: 'boolean',
      isOptional: true,
      default: 'true',
      description:
        'Whether telemetry collection and export are enabled.',
    },
    {
      name: 'sampling',
      type: 'SamplingStrategy',
      isOptional: true,
      description:
        'Defines the sampling strategy for traces, controlling how much data is collected.',
      properties: [
        {
          name: 'type',
          type: `'ratio' | 'always_on' | 'always_off' | 'parent_based'`,
          description:
            'Specifies the sampling strategy type.',
        },
        {
          name: 'probability',
          type: 'number (0.0 to 1.0)',
          isOptional: true,
          description:
            'For `ratio` or `parent_based` strategies, defines the sampling probability.',
        },
        {
          name: 'root',
          type: 'object',
          isOptional: true,
          description:
            'For `parent_based` strategy, configures root-level probability sampling.',
          properties: [
            {
              name: 'probability',
              type: 'number (0.0 to 1.0)',
              isOptional: true,
              description:
                'Sampling probability for root traces in `parent_based` strategy.',
            },
          ],
        },
      ],
    },
    {
      name: 'export',
      type: 'object',
      isOptional: true,
      description:
        'Configuration for exporting collected telemetry data.',
      properties: [
        {
          name: 'type',
          type: `'otlp' | 'console'`,
          description:
            'Specifies the exporter type. Use `otlp` for external exporters or `console` for development.',
        },
        {
          name: 'endpoint',
          type: 'string',
          isOptional: true,
          description:
            'For `otlp` type, the OTLP endpoint URL to send traces to.',
        },
        {
          name: 'headers',
          type: 'Record<string, string>',
          isOptional: true,
          description:
            'Additional headers to send with OTLP requests, useful for authentication or routing.',
        },
      ],
    },
  ]}
/>

================================================================================
Source: src/pages/docs/reference/observability/providers/braintrust.mdx
================================================================================

# Braintrust

Braintrust is an evaluation and monitoring platform for LLM applications.

## Configuration

To use Braintrust with Mastra, configure these environment variables:

```env
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.braintrust.dev/otel
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer <Your API Key>, x-bt-parent=project_id:<Your Project ID>"
```

## Implementation

Here's how to configure Mastra to use Braintrust:

```typescript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard

Access your Braintrust dashboard at [braintrust.dev](https://www.braintrust.dev/)


================================================================================
Source: src/pages/docs/reference/observability/providers/index.mdx
================================================================================

# Observability Providers

Observability providers include:
- [SigNoz](./signoz.mdx)
- [Braintrust](./braintrust.mdx)
- [Langfuse](./langfuse.mdx)
- [Langsmith](./langsmith.mdx)
- [New Relic](./new-relic.mdx)
- [Traceloop](./traceloop.mdx)
- [Laminar](./laminar.mdx)

================================================================================
Source: src/pages/docs/reference/observability/providers/laminar.mdx
================================================================================

# Laminar

Laminar is a specialized observability platform for LLM applications.

## Configuration

To use Laminar with Mastra, configure these environment variables:

```env
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.laminar.dev/v1/traces
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer your_api_key, x-laminar-team-id=your_team_id"
```

## Implementation

Here's how to configure Mastra to use Laminar:

```typescript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard

Access your Laminar dashboard at [https://lmnr.ai/](https://lmnr.ai/)


================================================================================
Source: src/pages/docs/reference/observability/providers/langfuse.mdx
================================================================================

# Langfuse

Langfuse is an open-source observability platform designed specifically for LLM applications.

## Configuration

To use Langfuse with Mastra, you'll need to configure the following environment variables:

```env
LANGFUSE_PUBLIC_KEY=your_public_key
LANGFUSE_SECRET_KEY=your_secret_key
LANGFUSE_BASEURL=https://cloud.langfuse.com  # Optional - defaults to cloud.langfuse.com
```

## Implementation

Here's how to configure Mastra to use Langfuse:

```typescript
import { Mastra } from "@mastra/core";
import { LangfuseExporter } from "langfuse-vercel";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "custom",
      exporter: new LangfuseExporter({
        publicKey: process.env.LANGFUSE_PUBLIC_KEY,
        secretKey: process.env.LANGFUSE_SECRET_KEY,
        baseUrl: process.env.LANGFUSE_BASEURL,
      }),
    },
  },
});
```

## Dashboard

Once configured, you can view your traces and analytics in the Langfuse dashboard at [cloud.langfuse.com](https://cloud.langfuse.com)


================================================================================
Source: src/pages/docs/reference/observability/providers/langsmith.mdx
================================================================================

# LangSmith

LangSmith is LangChain's platform for debugging, testing, evaluating, and monitoring LLM applications.

## Configuration

To use LangSmith with Mastra, you'll need to configure the following environment variables:

```env
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.smith.langchain.com/v1/traces
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer your_api_key, x-langsmith-project-id=your_project_id"
```

## Implementation

Here's how to configure Mastra to use LangSmith:

```typescript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard

Access your traces and analytics in the LangSmith dashboard at [smith.langchain.com](https://smith.langchain.com)


================================================================================
Source: src/pages/docs/reference/observability/providers/langwatch.mdx
================================================================================

# LangWatch

LangWatch is a specialized observability platform for LLM applications.

## Configuration

To use LangWatch with Mastra, configure these environment variables:

```env
LANGWATCH_API_KEY=your_api_key
LANGWATCH_PROJECT_ID=your_project_id
```

## Implementation

Here's how to configure Mastra to use LangWatch:

```typescript
import { Mastra } from "@mastra/core";
import { LangWatchExporter } from "langwatch";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "custom",
      exporter: new LangWatchExporter({
        apiKey: process.env.LANGWATCH_API_KEY,
        projectId: process.env.LANGWATCH_PROJECT_ID,
      }),
    },
  },
});
```

## Dashboard

Access your LangWatch dashboard at [app.langwatch.ai](https://app.langwatch.ai)


================================================================================
Source: src/pages/docs/reference/observability/providers/new-relic.mdx
================================================================================

# New Relic

New Relic is a comprehensive observability platform that supports OpenTelemetry (OTLP) for full-stack monitoring.

## Configuration

To use New Relic with Mastra via OTLP, configure these environment variables:

```env
OTEL_EXPORTER_OTLP_ENDPOINT=https://otlp.nr-data.net:4317
OTEL_EXPORTER_OTLP_HEADERS="api-key=your_license_key"
```

## Implementation

Here's how to configure Mastra to use New Relic:

```typescript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard

View your telemetry data in the New Relic One dashboard at [one.newrelic.com](https://one.newrelic.com)


================================================================================
Source: src/pages/docs/reference/observability/providers/signoz.mdx
================================================================================

# SigNoz

SigNoz is an open-source APM and observability platform that provides full-stack monitoring capabilities through OpenTelemetry.

## Configuration

To use SigNoz with Mastra, configure these environment variables:

```env
OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.{region}.signoz.cloud:443
OTEL_EXPORTER_OTLP_HEADERS=signoz-ingestion-key=your_signoz_token
```

## Implementation

Here's how to configure Mastra to use SigNoz:

```typescript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard

Access your SigNoz dashboard at [cloud.signoz.io](https://cloud.signoz.io)


================================================================================
Source: src/pages/docs/reference/observability/providers/traceloop.mdx
================================================================================

# Traceloop

Traceloop is an OpenTelemetry-native observability platform specifically designed for LLM applications.

## Configuration

To use Traceloop with Mastra, configure these environment variables:

```env
OTEL_EXPORTER_OTLP_ENDPOINT=https://api.traceloop.com/v1/traces
OTEL_EXPORTER_OTLP_HEADERS="Authorization=Bearer your_api_key, x-traceloop-destination-id=your_destination_id"
```

## Implementation

Here's how to configure Mastra to use Traceloop:

```typescript
import { Mastra } from "@mastra/core";

export const mastra = new Mastra({
  // ... other config
  telemetry: {
    serviceName: "your-service-name",
    enabled: true,
    export: {
      type: "otlp",
    },
  },
});
```

## Dashboard

Access your traces and analytics in the Traceloop dashboard at [app.traceloop.com](https://app.traceloop.com)


================================================================================
Source: src/pages/docs/reference/rag/chunk.mdx
================================================================================

# Chunk Function Reference

The `chunk` function splits documents into smaller segments using various strategies and options.

## Parameters

<PropertiesTable
  content={[
    {
      name: "strategy",
      type: "'recursive' | 'character' | 'token' | 'markdown' | 'html' | 'json' | 'latex'",
      isOptional: true,
      description:
        "The chunking strategy to use. If not specified, defaults based on document type.",
    },
     {
      name: "size",
      type: "number",
      isOptional: true,
      defaultValue: "512",
      description: "Maximum size of each chunk",
    },
    {
      name: "overlap",
      type: "number",
      isOptional: true,
      defaultValue: "50",
      description: "Number of characters/tokens that overlap between chunks",
    },
    {
      name: "separator",
      type: "string",
      isOptional: true,
      description: "Character(s) to split on",
    },
    {
      name: "isSeparatorRegex",
      type: "boolean",
      isOptional: true,
      defaultValue: "false",
      description: "Whether the separator is a regex pattern",
    },
    {
      name: "keepSeparator",
      type: "'start' | 'end'",
      isOptional: true,
      description:
        "Whether to keep the separator at the start or end of chunks",
    },
    {
      name: "extract",
      type: "ExtractParams",
      isOptional: true,
      description: "Metadata extraction options (requires OpenAI API key)",
    },
  ]}
/>

## Strategy-Specific Options

### HTML

<PropertiesTable
  content={[
    {
      name: "headers",
      type: "Array<[string, string]>",
      description:
        "Array of [selector, metadata key] pairs for header-based splitting",
    },
    {
      name: "sections",
      type: "Array<[string, string]>",
      description:
        "Array of [selector, metadata key] pairs for section-based splitting",
    },
    {
      name: "returnEachLine",
      type: "boolean",
      isOptional: true,
      description: "Whether to return each line as a separate chunk",
    },
  ]}
/>

### Markdown

<PropertiesTable
  content={[
    {
      name: "headers",
      type: "Array<[string, string]>",
      description: "Array of [header level, metadata key] pairs",
    },
    {
      name: "stripHeaders",
      type: "boolean",
      isOptional: true,
      description: "Whether to remove headers from the output",
    },
    {
      name: "returnEachLine",
      type: "boolean",
      isOptional: true,
      description: "Whether to return each line as a separate chunk",
    },
  ]}
/>

### Token

<PropertiesTable
  content={[
    {
      name: "encodingName",
      type: "string",
      isOptional: true,
      description: "Name of the token encoding to use",
    },
    {
      name: "modelName",
      type: "string",
      isOptional: true,
      description: "Name of the model for tokenization",
    },
  ]}
/>

### JSON

<PropertiesTable
  content={[
    {
      name: "maxSize",
      type: "number",
      description: "Maximum size of each chunk",
    },
    {
      name: "minSize",
      type: "number",
      isOptional: true,
      description: "Minimum size of each chunk",
    },
    {
      name: "ensureAscii",
      type: "boolean",
      isOptional: true,
      description: "Whether to ensure ASCII encoding",
    },
    {
      name: "convertLists",
      type: "boolean",
      isOptional: true,
      description: "Whether to convert lists in the JSON",
    },
  ]}
/>

## Return Value

Returns a `MDocument` instance containing the chunked documents. Each chunk includes:

```typescript
interface DocumentNode {
  text: string;
  metadata: Record<string, any>;
  embedding?: number[];
}
```


================================================================================
Source: src/pages/docs/reference/rag/document.mdx
================================================================================

# Document Processing

The `MDocument` class handles document chunking and metadata extraction.

## Constructor

<PropertiesTable
  content={[
    {
      name: "text",
      type: "string",
      description: "Document text content",
    },
    {
      name: "metadata",
      type: "Record<string, any>",
      isOptional: true,
      description: "Optional metadata about the document",
    },
  ]}
/>

## Methods

### chunk()

Splits document into chunks and optionally extracts metadata.

<PropertiesTable
  content={[
    {
      name: "strategy",
      type: "'sentence' | 'paragraph' | 'fixed'",
      description: "Chunking strategy to use",
    },
    {
      name: "parseMarkdown",
      type: "boolean",
      isOptional: true,
      defaultValue: "false",
      description: "Whether to parse markdown syntax",
    },
    {
      name: "metadataExtraction",
      type: "object",
      isOptional: true,
      description: "Metadata extraction options (requires OpenAI)",
      properties: [
        {
          name: "title",
          type: "boolean | TitleExtractorsArgs",
          description: "Extract document title",
        },
        {
          name: "summary",
          type: "boolean | SummaryExtractArgs",
          description: "Generate document summary",
        },
        {
          name: "questionsAnswered",
          type: "boolean | QuestionAnswerExtractArgs",
          description: "Extract potential questions document answers",
        },
        {
          name: "keyword",
          type: "boolean | KeywordExtractArgs",
          description: "Extract key terms and concepts",
        },
      ],
    },
  ]}
/>

## Response Types

The chunk method returns an array of document nodes:

```typescript copy
interface DocumentNode {
  text: string;
  metadata: Record<string, any>;
  embedding?: number[];
}
```

## Error Handling

```typescript copy
try {
  const chunks = await doc.chunk({
    strategy: "sentence",
  });
} catch (error) {
  if (error instanceof DocumentProcessingError) {
    console.log(error.code); // 'invalid_strategy' | 'extraction_failed' etc
    console.log(error.details); // Additional error context
  }
}
```


================================================================================
Source: src/pages/docs/reference/rag/embeddings.mdx
================================================================================

# Embed

The `embed` function generates vector embeddings for text inputs, enabling similarity search and RAG workflows.

## Parameters

<PropertiesTable
  content={[
    {
      name: "input",
      type: "string | string[] | { text: string; metadata?: Record<string, any> }[]",
      description: "Content to embed. Can be a single string, an array of strings, or an array of objects with `text` and optional `metadata`."
    },
    {
      name: "options",
      type: "object",
      description: "Configuration for the embedding call.",
      properties: [
        {
          name: "provider",
          type: "'openai' | 'anthropic' | 'google' | 'custom'",
          description: "Embedding provider."
        },
        {
          name: "model",
          type: "string",
          description: "Name of the embedding model."
        }
      ]
    }
  ]}
/>

## Return Value

<PropertiesTable
  content={[
    {
      name: "embedding",
      type: "number[] | number[][]",
      description: "The embedding vector(s). If `input` is a single string, returns one vector; if multiple, returns an array of vectors."
    }
  ]}
/>

================================================================================
Source: src/pages/docs/reference/rag/graph-rag.mdx
================================================================================

# GraphRAG

The `GraphRAG` class implements a graph-based approach to retrieval augmented generation. It creates a knowledge graph from document chunks where nodes represent documents and edges represent semantic relationships, enabling both direct similarity matching and discovery of related content through graph traversal.

## Basic Usage

```typescript
import { GraphRAG } from "@mastra/rag";

const graphRag = new GraphRAG({
  dimension: 1536,
  threshold: 0.7
});

// Create the graph from chunks and embeddings
graphRag.createGraph(documentChunks, embeddings);

// Query the graph with embedding
const results = await graphRag.query({
  query: queryEmbedding,
  topK: 5,
  randomWalkSteps: 100,
  restartProb: 0.15
});
```

## Constructor Parameters

<PropertiesTable
  content={[
    {
      name: "dimension",
      type: "number",
      description: "Dimension of the embedding vectors",
      isOptional: true,
      defaultValue: "1536",
    },
    {
      name: "threshold",
      type: "number",
      description: "Similarity threshold for creating edges between nodes (0-1)",
      isOptional: true,
      defaultValue: "0.7",
    }
  ]}
/>

## Methods

### createGraph

Creates a knowledge graph from document chunks and their embeddings.

```typescript
createGraph(chunks: GraphChunk[], embeddings: GraphEmbedding[]): void
```

#### Parameters
<PropertiesTable
  content={[
    {
      name: "chunks",
      type: "GraphChunk[]",
      description: "Array of document chunks with text and metadata",
      isOptional: false,
    },
    {
      name: "embeddings",
      type: "GraphEmbedding[]",
      description: "Array of embeddings corresponding to chunks",
      isOptional: false,
    }
  ]}
/>

### query

Performs a graph-based search combining vector similarity and graph traversal.

```typescript
query(query: number[], topK?: number, randomWalkSteps?: number, restartProb?: number): RankedNode[]
```

#### Parameters
<PropertiesTable
  content={[
    {
      name: "query",
      type: "number[]",
      description: "Query embedding vector",
      isOptional: false,
    },
    {
      name: "topK",
      type: "number",
      description: "Number of results to return",
      isOptional: true,
      defaultValue: "3",
    },
    {
      name: "randomWalkSteps",
      type: "number",
      description: "Number of steps in random walk",
      isOptional: true,
      defaultValue: "100",
    },
    {
      name: "restartProb",
      type: "number",
      description: "Probability of restarting walk from query node",
      isOptional: true,
      defaultValue: "0.15",
    }
  ]}
/>

#### Returns
Returns an array of `RankedNode` objects, where each node contains:

<PropertiesTable
  content={[
    {
      name: "id",
      type: "string",
      description: "Unique identifier for the node",
    },
    {
      name: "content",
      type: "string",
      description: "Text content of the document chunk",
    },
    {
      name: "metadata",
      type: "Record<string, any>",
      description: "Additional metadata associated with the chunk",
    },
    {
      name: "score",
      type: "number",
      description: "Combined relevance score from graph traversal",
    }
  ]}
/>

## Advanced Example

```typescript
const graphRag = new GraphRAG({
  dimension: 1536,
  threshold: 0.8  // Stricter similarity threshold
});

// Create graph from chunks and embeddings
graphRag.createGraph(documentChunks, embeddings);

// Query with custom parameters
const results = await graphRag.query({
  query: queryEmbedding,
  topK: 5,
  randomWalkSteps: 200,
  restartProb: 0.2
});
```

## Related

- [createGraphRAGTool](../tools/graph-rag-tool)


================================================================================
Source: src/pages/docs/reference/rag/pgstore.mdx
================================================================================

# PgStore

The PgStore class provides vector search using PostgreSQL with pgvector extension.

## Constructor Options

<PropertiesTable
  content={[
    {
      name: "connectionString",
      type: "string",
      description: "PostgreSQL connection URL",
    },
    {
      name: "tableName",
      type: "string",
      isOptional: true,
      defaultValue: "embeddings",
      description: "Table name for vector storage",
    },
    {
      name: "dimension",
      type: "number",
      isOptional: true,
      defaultValue: "1536",
      description: "Vector dimension (must match your embedding model)",
    },
  ]}
/>

## Methods

### createIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to create",
    },
    {
      name: "dimension",
      type: "number",
      description: "Vector dimension size",
    },
    {
      name: "metric",
      type: "'cosine' | 'euclidean' | 'dotproduct'",
      isOptional: true,
      defaultValue: "cosine",
      description: "Distance metric for similarity search",
    },
  ]}
/>

### upsert()

<PropertiesTable
  content={[
    {
      name: "vectors",
      type: "number[][]",
      description: "Array of embedding vectors",
    },
    {
      name: "metadata",
      type: "Record<string, any>[]",
      isOptional: true,
      description: "Metadata for each vector",
    },
    {
      name: "ids",
      type: "string[]",
      isOptional: true,
      description: "Optional vector IDs (auto-generated if not provided)",
    },
  ]}
/>

### query()

<PropertiesTable
  content={[
    {
      name: "vector",
      type: "number[]",
      description: "Query vector",
    },
    {
      name: "topK",
      type: "number",
      isOptional: true,
      defaultValue: "10",
      description: "Number of results to return",
    },
    {
      name: "filter",
      type: "Record<string, any>",
      isOptional: true,
      description: "Metadata filters",
    },
    {
      name: "minScore",
      type: "number",
      isOptional: true,
      defaultValue: "0",
      description: "Minimum similarity score threshold",
    },
  ]}
/>

### describeIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to describe",
    },
  ]}
/>

Returns:

```typescript copy
interface IndexStats {
  dimension: number;
  count: number;
  metric: "cosine" | "euclidean" | "dotproduct";
}
```

### deleteIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to delete",
    },
  ]}
/>

### disconnect()

Closes the database connection pool. Should be called when done using the store.

## Response Types

Query results are returned in this format:

```typescript copy
interface QueryResult {
  id: string;
  score: number;
  metadata: Record<string, any>;
}
```

## Error Handling

The store throws typed errors that can be caught:

```typescript copy
try {
  await store.query(queryVector);
} catch (error) {
  if (error instanceof VectorStoreError) {
    console.log(error.code); // 'connection_failed' | 'invalid_dimension' | etc
    console.log(error.details); // Additional error context
  }
}
```


================================================================================
Source: src/pages/docs/reference/rag/pinecone.mdx
================================================================================

# Pinecone

The PineconeStore class provides an interface to Pinecone's vector database.

## Constructor Options

<PropertiesTable
  content={[
    {
      name: "apiKey",
      type: "string",
      description: "Pinecone API key",
    },
    {
      name: "environment",
      type: "string",
      description: 'Pinecone environment (e.g., "us-west1-gcp")',
    },
    {
      name: "indexName",
      type: "string",
      description: "Name of your Pinecone index",
    },
  ]}
/>

## Methods

### createIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to create",
    },
    {
      name: "dimension",
      type: "number",
      description: "Vector dimension size",
    },
    {
      name: "metric",
      type: "'cosine' | 'euclidean' | 'dotproduct'",
      isOptional: true,
      defaultValue: "cosine",
      description: "Distance metric for similarity search",
    },
  ]}
/>

### upsert()

<PropertiesTable
  content={[
    {
      name: "vectors",
      type: "number[][]",
      description: "Array of embedding vectors",
    },
    {
      name: "metadata",
      type: "Record<string, any>[]",
      isOptional: true,
      description: "Metadata for each vector",
    },
    {
      name: "namespace",
      type: "string",
      isOptional: true,
      description: "Optional namespace for organization",
    },
  ]}
/>

### query()

<PropertiesTable
  content={[
    {
      name: "vector",
      type: "number[]",
      description: "Query vector to find similar vectors",
    },
    {
      name: "topK",
      type: "number",
      isOptional: true,
      defaultValue: "10",
      description: "Number of results to return",
    },
    {
      name: "filter",
      type: "Record<string, any>",
      isOptional: true,
      description: "Metadata filters for the query",
    },
  ]}
/>

### listIndexes()

Returns an array of index names as strings.

### describeIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to describe",
    },
  ]}
/>

Returns:

```typescript copy
interface IndexStats {
  dimension: number;
  count: number;
  metric: "cosine" | "euclidean" | "dotproduct";
}
```

### deleteIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to delete",
    },
  ]}
/>

## Response Types

Query results are returned in this format:

```typescript copy
interface QueryResult {
  id: string;
  score: number;
  metadata: Record<string, any>;
}
```

## Error Handling

The store throws typed errors that can be caught:

```typescript copy
try {
  await store.query(queryVector);
} catch (error) {
  if (error instanceof VectorStoreError) {
    console.log(error.code); // 'connection_failed' | 'invalid_dimension' | etc
    console.log(error.details); // Additional error context
  }
}
```

### Environment Variables

Required environment variables:

- `PINECONE_API_KEY`: Your Pinecone API key
- `PINECONE_ENVIRONMENT`: Pinecone environment (e.g., 'us-west1-gcp')


================================================================================
Source: src/pages/docs/reference/rag/qdrant.mdx
================================================================================

# Qdrant

[Qdrant](http://qdrant.tech/) is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage vectors with additional payload and extended filtering support.

## Constructor Options

<PropertiesTable
  content={[
    {
      name: "url",
      type: "string",
      description:
        "REST URL of the Qdrant instance. Eg. https://xyz-example.eu-central.aws.cloud.qdrant.io:6333",
    },
    {
      name: "apiKey",
      type: "string",
      description: "Optional Qdrant API key",
    },
    {
      name: "https",
      type: "boolean",
      description:
        "Whether to use TLS when setting up the connection. Recommended.",
    },
  ]}
/>

## Methods

### createIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to create",
    },
    {
      name: "dimension",
      type: "number",
      description: "Vector dimension size",
    },
    {
      name: "metric",
      type: "'cosine' | 'euclidean' | 'dotproduct'",
      isOptional: true,
      defaultValue: "cosine",
      description: "Distance metric for similarity search",
    },
  ]}
/>

### upsert()

<PropertiesTable
  content={[
    {
      name: "vectors",
      type: "number[][]",
      description: "Array of embedding vectors",
    },
    {
      name: "metadata",
      type: "Record<string, any>[]",
      isOptional: true,
      description: "Metadata for each vector",
    },
    {
      name: "namespace",
      type: "string",
      isOptional: true,
      description: "Optional namespace for organization",
    },
  ]}
/>

### query()

<PropertiesTable
  content={[
    {
      name: "vector",
      type: "number[]",
      description: "Query vector to find similar vectors",
    },
    {
      name: "topK",
      type: "number",
      isOptional: true,
      defaultValue: "10",
      description: "Number of results to return",
    },
    {
      name: "filter",
      type: "Record<string, any>",
      isOptional: true,
      description: "Metadata filters for the query",
    },
  ]}
/>

### listIndexes()

Returns an array of index names as strings.

### describeIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to describe",
    },
  ]}
/>

Returns:

```typescript copy
interface IndexStats {
  dimension: number;
  count: number;
  metric: "cosine" | "euclidean" | "dotproduct";
}
```

### deleteIndex()

<PropertiesTable
  content={[
    {
      name: "indexName",
      type: "string",
      description: "Name of the index to delete",
    },
  ]}
/>

## Response Types

Query results are returned in this format:

```typescript copy
interface QueryResult {
  id: string;
  score: number;
  metadata: Record<string, any>;
}
```

## Error Handling

The store throws typed errors that can be caught:

```typescript copy
try {
  await store.query(queryVector);
} catch (error) {
  if (error instanceof VectorStoreError) {
    console.log(error.code); // 'connection_failed' | 'invalid_dimension' | etc
    console.log(error.details); // Additional error context
  }
}
```


================================================================================
Source: src/pages/docs/reference/rag/reranker.mdx
================================================================================

# Reranker

The `Reranker` class provides advanced reranking capabilities for vector search results by combining semantic relevance, vector similarity, and position-based scoring. 

You initialize it with a model and a set of weights to adjust the importance of each of the three scoring components).

It has one method, `rerank()`, which takes a query and a set of results, and returns a set of reranked results.

If you want to have an agent running your RAG pipeline, the Reranker class can be used through the `createVectorQueryTool`.

## Basic Usage

```typescript
import { Reranker } from "@mastra/rag";

const reranker = new Reranker({
  semanticProvider: "agent",
  agentProvider: {
    provider: "openai",
    name: "gpt-4"
  },
  weights: {
    semantic: 0.5,
    vector: 0.3,
    position: 0.2
  }
});

const results = await reranker.rerank({
  query: "How do I deploy to production?",
  vectorStoreResults: searchResults,
  topK: 3
});
```

## Reranker Initialization config

<PropertiesTable
  content={[
    {
      name: "semanticProvider",
      type: "'cohere' | 'agent'",
      description: "Provider to use for semantic scoring",
      isOptional: false,
    },
    {
      name: "weights",
      type: "WeightConfig",
      description: "Weights for different scoring components",
      isOptional: true,
      defaultValue: "{ semantic: 0.4, vector: 0.4, position: 0.2 }",
    },
    {
      name: "cohereApiKey",
      type: "string",
      description: "Required when using Cohere provider",
      isOptional: true,
    },
    {
      name: "cohereModel",
      type: "string",
      description: "Specific Cohere model to use",
      isOptional: true,
    },
    {
      name: "agentProvider",
      type: "{ provider: string; name: string }",
      description: "Required when using Agent provider",
      isOptional: true,
    }
  ]}
/>

## rerank() Params

<PropertiesTable
  content={[
    {
      name: "query",
      type: "string",
      description: "The search query text",
      isOptional: false,
    },
    {
      name: "vectorStoreResults",
      type: "QueryResult[]",
      description: "Results from vector store to rerank",
      isOptional: false,
    },
    {
      name: "queryEmbedding",
      type: "number[]",
      description: "Original query embedding for advanced scoring",
      isOptional: true,
    },
    {
      name: "topK",
      type: "number",
      description: "Number of top results to return",
      isOptional: true,
      defaultValue: "3",
    }
  ]}
/>

## rerank() Returns

The rerank method returns an array of `RerankResult` objects. 

Here's the type definition of a `RerankResult`:

<PropertiesTable
  content={[
    {
      name: "result",
      type: "QueryResult",
      description: "The original query result",
    },
    {
      name: "score",
      type: "number",
      description: "Combined reranking score (0-1)",
    },
    {
      name: "details",
      type: "RerankDetails",
      description: "Detailed scoring information",
    }
  ]}
/>

## Related

- [createVectorQueryTool](../tools/vector-query-tool)


================================================================================
Source: src/pages/docs/reference/tools/document-chunker-tool.mdx
================================================================================

# createDocumentChunkerTool()

The `createDocumentChunkerTool()` function creates a tool for splitting documents into smaller chunks for efficient processing and retrieval. It supports different chunking strategies and configurable parameters.

## Basic Usage

```typescript
import { createDocumentChunkerTool, MDocument } from "@mastra/rag";

const document = new MDocument({
  text: "Your document content here...",
  metadata: { source: "user-manual" }
});

const chunker = createDocumentChunkerTool({
  doc: document,
  params: {
    strategy: "recursive",
    size: 512,
    overlap: 50,
    separator: "\n"
  }
});

const { chunks } = await chunker.execute();
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "doc",
      type: "MDocument",
      description: "The document to be chunked",
      isOptional: false,
    },
    {
      name: "params",
      type: "ChunkParams",
      description: "Configuration parameters for chunking",
      isOptional: true,
      defaultValue: "Default chunking parameters",
    }
  ]}
/>

### ChunkParams

<PropertiesTable
  content={[
    {
      name: "strategy",
      type: "'recursive'",
      description: "The chunking strategy to use",
      isOptional: true,
      defaultValue: "'recursive'",
    },
    {
      name: "size",
      type: "number",
      description: "Target size of each chunk in tokens/characters",
      isOptional: true,
      defaultValue: "512",
    },
    {
      name: "overlap",
      type: "number",
      description: "Number of overlapping tokens/characters between chunks",
      isOptional: true,
      defaultValue: "50",
    },
    {
      name: "separator",
      type: "string",
      description: "Character(s) to use as chunk separator",
      isOptional: true,
      defaultValue: "'\\n'",
    }
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: "chunks",
      type: "DocumentChunk[]",
      description: "Array of document chunks with their content and metadata",
    }
  ]}
/>

## Example with Custom Parameters

```typescript
const technicalDoc = new MDocument({
  text: longDocumentContent,
  metadata: {
    type: "technical",
    version: "1.0"
  }
});

const chunker = createDocumentChunkerTool({
  doc: technicalDoc,
  params: {
    strategy: "recursive",
    size: 1024,      // Larger chunks
    overlap: 100,    // More overlap
    separator: "\n\n" // Split on double newlines
  }
});

const { chunks } = await chunker.execute();

// Process the chunks
chunks.forEach((chunk, index) => {
  console.log(`Chunk ${index + 1} length: ${chunk.content.length}`);
});
```

## Tool Details

The chunker is created as a Mastra tool with the following properties:

- **Tool ID**: `Document Chunker {strategy} {size}`
- **Description**: `Chunks document using {strategy} strategy with size {size} and {overlap} overlap`
- **Input Schema**: Empty object (no additional inputs required)
- **Output Schema**: Object containing the chunks array

## Related

- [MDocument](../rag/mdocument)
- [createVectorQueryTool](./vector-query-tool) 

================================================================================
Source: src/pages/docs/reference/tools/graph-rag-tool.mdx
================================================================================

# createGraphRAGTool()

The `createGraphRAGTool()` creates a tool that enhances RAG by building a graph of semantic relationships between documents. It uses the `GraphRAG` system under the hood to provide graph-based retrieval, finding relevant content through both direct similarity and connected relationships.

## Usage Example

```typescript
import { createGraphRAGTool } from "@mastra/rag";

const graphTool = createGraphRAGTool({
  vectorStoreName: "pinecone",
  indexName: "docs",
  options: {
    provider: "OPEN_AI",
    model: "text-embedding-ada-002",
    maxRetries: 3
  },
  topK: 5,
  graphOptions: {
    dimension: 1536,
    threshold: 0.7,
    randomWalkSteps: 100,
    restartProb: 0.15
  }
});
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "vectorStoreName",
      type: "string",
      description: "Name of the vector store to query",
      isOptional: false,
    },
    {
      name: "indexName",
      type: "string",
      description: "Name of the index within the vector store",
      isOptional: false,
    },
    {
      name: "options",
      type: "EmbeddingOptions",
      description: "Configuration for embedding generation",
      isOptional: false,
    },
    {
      name: "topK",
      type: "number",
      description: "Maximum number of results to retrieve",
      isOptional: true,
      defaultValue: "10",
    },
    {
      name: "graphOptions",
      type: "GraphOptions",
      description: "Configuration for the graph-based retrieval",
      isOptional: true,
      defaultValue: "Default graph options",
    }
  ]}
/>

### GraphOptions

<PropertiesTable
  content={[
    {
      name: "dimension",
      type: "number",
      description: "Dimension of the embedding vectors",
      isOptional: true,
      defaultValue: "1536",
    },
    {
      name: "threshold",
      type: "number",
      description: "Similarity threshold for creating edges between nodes (0-1)",
      isOptional: true,
      defaultValue: "0.7",
    },
    {
      name: "randomWalkSteps",
      type: "number",
      description: "Number of steps in random walk for graph traversal",
      isOptional: true,
      defaultValue: "100",
    },
    {
      name: "restartProb",
      type: "number",
      description: "Probability of restarting random walk from query node",
      isOptional: true,
      defaultValue: "0.15",
    }
  ]}
/>

## Returns
The tool returns an object with:

<PropertiesTable
  content={[
    {
      name: "relevantContext",
      type: "string",
      description: "Combined text from the most relevant document chunks, retrieved using graph-based ranking",
    }
  ]}
/>

## Advanced Example

```typescript
const graphTool = createGraphRAGTool({
  vectorStoreName: "pinecone",
  indexName: "docs",
  options: {
    provider: "OPEN_AI",
    model: "text-embedding-ada-002",
    maxRetries: 3
  },
  topK: 5,
  graphOptions: {
    dimension: 1536,
    threshold: 0.8,        // Higher similarity threshold
    randomWalkSteps: 200,  // More exploration steps
    restartProb: 0.2      // Higher restart probability
  }
});
```

## Related

- [createVectorQueryTool](./vector-query-tool)
- [GraphRAG](../rag/graph-rag) 

================================================================================
Source: src/pages/docs/reference/tools/vector-query-tool.mdx
================================================================================

# createVectorQueryTool()

The `createVectorQueryTool()` function creates a tool for semantic search over vector stores. It supports filtering, reranking, and integrates with various vector store backends.

## Basic Usage

```typescript
import { createVectorQueryTool } from "@mastra/rag";

const queryTool = createVectorQueryTool({
  vectorStoreName: "pinecone",
  indexName: "docs",
  options: {
    provider: "OPEN_AI",
    model: "text-embedding-ada-002",
    maxRetries: 3
  }
});
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "vectorStoreName",
      type: "string",
      description: "Name of the vector store to query (must be configured in Mastra)",
      isOptional: false,
    },
    {
      name: "indexName",
      type: "string",
      description: "Name of the index within the vector store",
      isOptional: false,
    },
    {
      name: "options",
      type: "EmbeddingOptions",
      description: "Configuration for embedding generation",
      isOptional: false,
    },
    {
      name: "topK",
      type: "number",
      description: "Maximum number of results to retrieve",
      isOptional: true,
      defaultValue: "10",
    },
    {
      name: "vectorFilterType",
      type: "'pg' | 'astra' | 'qdrant' | 'upstash' | 'pinecone' | 'chroma' | ''",
      description: "Type of vector store for filter formatting",
      isOptional: true,
      defaultValue: "''",
    },
    {
      name: "rerankOptions",
      type: "RerankerOptions",
      description: "Options for reranking results",
      isOptional: true,
    }
  ]}
/>

## Returns

The tool returns an object with:

<PropertiesTable
  content={[
    {
      name: "relevantContext",
      type: "string",
      description: "Combined text from the most relevant document chunks",
    }
  ]}
/>

## Example with Filters

```typescript
// Pinecone/PG/Astra
const queryTool = createVectorQueryTool({
  vectorStoreName: "pinecone",
  indexName: "docs",
  options: {
    provider: "OPEN_AI",
    model: "text-embedding-ada-002",
    maxRetries: 3
  },
  vectorFilterType: "pinecone",
  topK: 5
});
```

Filter Formats:
- Pinecone/PG/Astra: `{ category: { eq: "technical" } }`

## Example with Reranking

```typescript
const queryTool = createVectorQueryTool({
  vectorStoreName: "milvus",
  indexName: "documentation",
  options: {
    provider: "OPEN_AI",
    model: "text-embedding-ada-002",
    maxRetries: 3
  },
  topK: 5,
  rerankOptions: {
    model: "cross-encoder",
    threshold: 0.7
  }
});
```

## Tool Details

The tool is created with:
- **ID**: `VectorQuery {vectorStoreName} {indexName} Tool`
- **Description**: `Fetches and combines the top {topK} relevant chunks from the {vectorStoreName} vector store using the {indexName} index`
- **Input Schema**: Requires queryText and filter objects
- **Output Schema**: Returns relevantContext string

## Related

- [RAG Reranker](../rag/reranker) 
- [createGraphRAGTool](./graph-rag-tool) 

================================================================================
Source: src/pages/docs/reference/tts/generate.mdx
================================================================================

# `generate()`

The `generate()` method is used to interact with the TTS model to produce an audio response. This method accepts `text` and `voice` as parameters.

## Parameters


<PropertiesTable
  content={[
    {
      name: 'text',
      type: 'string',
      description: 'The messages to be processed by TTS.',
    },
    {
      name: 'voice',
      type: 'string',
      description: 'Voice ID to be used with generation.',
    },
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: 'audioResult',
      type: 'Readable',
      isOptional: true,
      description: 'The generated audio stream',
    },
  ]}
/>

## Examples

### Basic Audio Generation (ElevenLabs)

```typescript
import { ElevenLabsTTS } from '@mastra/tts'

 const tts = new ElevenLabsTTS({
    model: {
      name: 'eleven_multilingual_v2',
      apiKey: process.env.ELEVENLABS_API_KEY!,
    },
  });

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id!;

const { audioResult } = await tts.generate({ text: "What is AI?", voice: voiceId });

await writeFile(path.join(process.cwd(), '/test-outputs/generate-output.mp3'), audioBuffer);
```

### Basic Audio Generation (OpenAI)

```typescript
import { OpenAITTS } from '@mastra/tts'

 const tts = new OpenAITTS({
    model: {
      name: 'tts-1',
      apiKey: process.env.OPENAI_API_KEY!,
    },
  });

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id!;

const { audioResult } = await tts.generate({ text: "What is AI?", voice: voiceId });

const outputPath = path.join(process.cwd(), 'test-outputs/open-aigenerate-test.mp3');
writeFileSync(outputPath, audioResult);
```

## Related Methods

For streaming audio responses, see the [`stream()`](./stream.mdx) method documentation.

================================================================================
Source: src/pages/docs/reference/tts/providers-and-models.mdx
================================================================================

# Providers and Models

## Most popular providers

| Provider      | Supported Models                                                                                                                                                         |
| ------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| ElevenLabs    | `eleven_multilingual_v2`, `eleven_flash_v2_5`, `eleven_flash_v2`, `eleven_multilingual_sts_v2`, `eleven_english_sts_v2`                                                  |
| OpenAI        | `tts-1`, `tts-1-hd`                                                                                                                                                      |                                                                                                  

================================================================================
Source: src/pages/docs/reference/tts/stream.mdx
================================================================================

# `stream()`

The `stream()` method is used to interact with the TTS model to produce an audio response stream. This method accepts `text` and `voice` as parameters.

## Parameters


<PropertiesTable
  content={[
    {
      name: 'text',
      type: 'string',
      description: 'The messages to be processed by TTS.',
    },
    {
      name: 'voice',
      type: 'string',
      description: 'Voice ID to be used with generation.',
    },
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: 'audioResult',
      type: 'Readable',
      isOptional: true,
      description: 'The generated audio stream',
    },
  ]}
/>

## Examples

### Basic Audio Stream (ElevenLabs)

```typescript
import { ElevenLabsTTS } from '@mastra/tts'

 const tts = new ElevenLabsTTS({
    model: {
      name: 'eleven_multilingual_v2',
      apiKey: process.env.ELEVENLABS_API_KEY!,
    },
  });

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id!;

const { audioResult } = await tts.stream({ text: "What is AI?", voice: voiceId });

// Create a write stream to simulate real-time playback
const outputPath = path.join(process.cwd(), '/test-outputs/streaming-output.mp3');
const writeStream = createWriteStream(outputPath);

let firstChunkTime: number | null = null;
let lastChunkTime: number | null = null;
let totalChunks = 0;

// Process chunks as they arrive
for await (const chunk of audioResult) {
    if (!firstChunkTime) {
    firstChunkTime = Date.now();
    }
    lastChunkTime = Date.now();
    totalChunks++;

    // Write chunk immediately as it arrives
    writeStream.write(chunk);

    // Log timing of chunk arrival
    console.log(`Received chunk ${totalChunks} at ${lastChunkTime - firstChunkTime!}ms`);
}

writeStream.end()
```

### Basic Audio Stream (OpenAI)

```typescript
import { OpenAITTS } from '@mastra/tts'

 const tts = new OpenAITTS({
    model: {
      name: 'tts-1',
      apiKey: process.env.OPENAI_API_KEY!,
    },
  });

const voices = await tts.voices();
const voiceId = voices?.[0]?.voice_id!;

const { audioResult } = await tts.stream({ text: "What is AI?", voice: voiceId });

// Create a write stream to simulate real-time playback
const outputPath = path.join(process.cwd(), '/test-outputs/streaming-output.mp3');
const writeStream = createWriteStream(outputPath);

let firstChunkTime: number | null = null;
let lastChunkTime: number | null = null;
let totalChunks = 0;

// Process chunks as they arrive
for await (const chunk of audioResult) {
    if (!firstChunkTime) {
    firstChunkTime = Date.now();
    }
    lastChunkTime = Date.now();
    totalChunks++;

    // Write chunk immediately as it arrives
    writeStream.write(chunk);

    // Log timing of chunk arrival
    console.log(`Received chunk ${totalChunks} at ${lastChunkTime - firstChunkTime!}ms`);
}

writeStream.end()
```

================================================================================
Source: src/pages/docs/reference/workflows/after.mdx
================================================================================

# after

The `after` method defines explicit dependencies between workflow steps, enabling branching and merging paths in your workflow execution.

## Usage

```typescript
workflow
  .step(stepA)
    .then(stepB)
  .after(stepA)  // Create new branch after stepA completes
    .step(stepC);
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "stepId",
      type: "string | string[]",
      description: "ID(s) of step(s) that must complete before continuing",
      isOptional: false
    }
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: "workflow",
      type: "Workflow",
      description: "The workflow instance for method chaining"
    }
  ]}
/>

## Related

- [Branching Paths example](../../examples/workflows/branching-paths.mdx)
- [Workflow Class Reference](./workflow.mdx)
- [Step Reference](./step.mdx) 
- [Control Flow Guide](../../workflows/control-flow.mdx)


================================================================================
Source: src/pages/docs/reference/workflows/commit.mdx
================================================================================

# commit()

The commit method finalizes a workflow definition, validating its structure and making it ready for execution. When calling `.commit()`, the workflow validates:

- No circular dependencies between steps
- All paths must have an end point
- No unreachable steps
- No duplicate step IDs
- Variable references to non-existent steps

## Usage

```typescript
workflow
  .step(stepA)
  .then(stepB)
  .commit();
```

## Returns

<PropertiesTable
  content={[
    {
      name: "workflow",
      type: "Workflow",
      description: "The validated workflow instance"
    }
  ]}
/>

## Error Handling

```typescript
try {
  workflow
    .step(stepA)
    .after(['stepB', 'stepC'])
    .step(stepD)
    .commit();
} catch (error) {
  if (error instanceof ValidationError) {
    console.log(error.type); // 'circular_dependency' | 'no_terminal_path' | 'unreachable_step'
    console.log(error.details);
  }
}
```

### Validation Error Types

<PropertiesTable
  content={[
    {
      name: "circular_dependency",
      type: "string",
      description: "Steps form a circular reference"
    },
    {
      name: "no_terminal_path", 
      type: "string",
      description: "Path has no end point"
    },
    {
      name: "unreachable_step",
      type: "string", 
      description: "Step cannot be reached from workflow start"
    },
    {
      name: "duplicate_step_id",
      type: "string",
      description: "Multiple steps share the same ID"
    }
  ]}
/>

## Related

- [Branching Paths example](../../examples/workflows/branching-paths.mdx)
- [Workflow Class Reference](./workflow.mdx)
- [Step Reference](./step.mdx)
- [Control Flow Guide](../../workflows/control-flow.mdx)
```


================================================================================
Source: src/pages/docs/reference/workflows/createRun.mdx
================================================================================

# createRun

The createRun method initializes a new workflow run instance. It generates a unique run ID for tracking and returns a start function that begins workflow execution when called.

## Usage

```typescript
const { runId, start } = workflow.createRun({ 
  triggerData: { inputValue: 42 },
  metadata: {
    requestId: "abc-123" 
  }
});

const result = await start();
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "config",
      type: "object",
      description: "Configuration options for the workflow run",
      isOptional: true
    }
  ]}
/>

### config

<PropertiesTable
  content={[
    {
      name: "triggerData",
      type: "Record<string, any>",
      description: "Initial data passed to trigger the workflow execution",
      isOptional: true
    },
    {
      name: "metadata", 
      type: "Record<string, any>",
      description: "Additional metadata to associate with this run",
      isOptional: true
    }
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: "runId",
      type: "string",
      description: "Unique identifier for tracking this workflow run"
    },
    {
      name: "start",
      type: "() => Promise<WorkflowResult>",
      description: "Function that begins workflow execution when called"
    }
  ]}
/>

## Error Handling

The start function may throw validation errors if the workflow configuration is invalid:

```typescript
try {
  const { runId, start } = workflow.createRun();
  await start({ triggerData: data });
} catch (error) {
  if (error instanceof ValidationError) {
    // Handle validation errors
    console.log(error.type); // 'circular_dependency' | 'no_terminal_path' | 'unreachable_step'
    console.log(error.details); 
  }
}
```

## Related

- [Workflow Class Reference](./workflow.mdx)
- [Step Class Reference](./step.mdx)
- See the [Creating a Workflow](../../examples/workflows/creating-a-workflow.mdx) example for complete usage
```

================================================================================
Source: src/pages/docs/reference/workflows/resume.mdx
================================================================================

# resume

The resume function continues execution of a suspended workflow step, optionally providing new context data that will be merged with existing step results.

## Usage

```typescript copy showLineNumbers
await workflow.resume({
  runId: "abc-123",
  stepId: "stepTwo", 
  context: {
    secondValue: 100
  }
});
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "config",
      type: "object",
      description: "Configuration for resuming the workflow",
      isOptional: false
    }
  ]}
/>

### config

<PropertiesTable
  content={[
    {
      name: "runId",
      type: "string", 
      description: "Unique identifier of the workflow run to resume",
      isOptional: false
    },
    {
      name: "stepId",
      type: "string",
      description: "ID of the suspended step to resume",
      isOptional: false
    },
    {
      name: "context",
      type: "Record<string, any>",
      description: "New context data to merge with existing step results",
      isOptional: true
    }
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: "Promise<WorkflowResult>",
      type: "object",
      description: "Result of the resumed workflow execution"
    }
  ]}
/>

## Error Handling

The resume function may throw several types of errors:

```typescript
try {
  await workflow.resume({
    runId,
    stepId: "stepTwo",
    context: newData
  });
} catch (error) {
  if (error.message === "No snapshot found for workflow run") {
    // Handle missing workflow state
  }
  if (error.message === "Failed to parse workflow snapshot") {
    // Handle corrupted workflow state
  }
}
```

## Related

- [Suspend and Resume](../examples/workflows/suspend-and-resume.mdx)
- [suspend Reference](./suspend.mdx)
- [watch Reference](./watch.mdx)
- [Workflow Class Reference](./workflow.mdx)
```


================================================================================
Source: src/pages/docs/reference/workflows/start.mdx
================================================================================

# start

The start function begins execution of a workflow run. It processes all steps in the defined workflow order, handling parallel execution, branching logic, and step dependencies.

## Usage

```typescript copy showLineNumbers
const { runId, start } = workflow.createRun();
const result = await start({ 
  triggerData: { inputValue: 42 } 
});
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "config",
      type: "object",
      description: "Configuration for starting the workflow run",
      isOptional: true
    }
  ]}
/>

### config

<PropertiesTable
  content={[
    {
      name: "triggerData",
      type: "Record<string, any>",
      description: "Initial data that matches the workflow's triggerSchema",
      isOptional: false
    }
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: "results",
      type: "Record<string, any>",
      description: "Combined output from all completed workflow steps"
    },
    {
      name: "status",
      type: "'completed' | 'error' | 'suspended'",
      description: "Final status of the workflow run"
    }
  ]}
/>

## Error Handling

The start function may throw several types of validation errors:

```typescript copy showLineNumbers
try {
  const result = await start({ triggerData: data });
} catch (error) {
  if (error instanceof ValidationError) {
    console.log(error.type); // 'circular_dependency' | 'no_terminal_path' | 'unreachable_step'
    console.log(error.details);
  }
}
```

## Related

- [Example: Creating a Workflow](../../examples/workflows/creating-a-workflow.mdx)
- [Example: Suspend and Resume](../../examples/workflows/suspend-and-resume.mdx)
- [createRun Reference](./createRun.mdx)
- [Workflow Class Reference](./workflow.mdx)
- [Step Class Reference](./step.mdx)
```


================================================================================
Source: src/pages/docs/reference/workflows/step-class.mdx
================================================================================

---
title: Step
---
# Step

The Step class defines individual units of work within a workflow, encapsulating execution logic, data validation, and input/output handling.

## Usage

```typescript
const processOrder = new Step({
  id: "processOrder",
  inputSchema: z.object({
    orderId: z.string(),
    userId: z.string()
  }),
  outputSchema: z.object({
    status: z.string(),
    orderId: z.string()
  }),
  execute: async ({ context, runId }) => {
    return {
      status: "processed",
      orderId: context.orderId
    };
  }
});
```

## Constructor Parameters

<PropertiesTable
  content={[
    {
      name: "id",
      type: "string",
      description: "Unique identifier for the step",
      required: true
    },
    {
      name: "inputSchema",
      type: "z.ZodSchema",
      description: "Zod schema to validate input data before execution",
      required: false
    },
    {
      name: "outputSchema", 
      type: "z.ZodSchema",
      description: "Zod schema to validate step output data",
      required: false
    },
    {
      name: "payload",
      type: "Record<string, any>",
      description: "Static data to be merged with variables",
      required: false
    },
    {
      name: "execute",
      type: "(params: ExecuteParams) => Promise<any>",
      description: "Async function containing step logic",
      required: true
    }
  ]}
/>

### ExecuteParams

<PropertiesTable
  content={[
    {
      name: "context",
      type: "StepContext",
      description: "Access to workflow context and step results"
    },
    {
      name: "runId",
      type: "string",
      description: "Unique identifier for current workflow run"
    },
    {
      name: "suspend",
      type: "() => Promise<void>",
      description: "Function to suspend step execution"
    }
  ]}
/>

## Related

- [Workflow Reference](./workflow.mdx)
- [Step Configuration Guide](../../workflows/steps.mdx)
- [Control Flow Guide](../../workflows/control-flow.mdx)
```


================================================================================
Source: src/pages/docs/reference/workflows/step-condition.mdx
================================================================================

# StepCondition

Conditions determine whether a step should execute based on the output of previous steps or trigger data.

## Usage

There are three ways to specify conditions: function, query object, and simple path comparison. 

### 1. Function Condition
```typescript copy showLineNumbers
workflow.step(processOrder, {
  when: async ({ context }) => {
    const auth = context?.getStepPayload<{status: string}>("auth");
    return auth?.status === "authenticated";
  }
});
```

### 2. Query Object
```typescript copy showLineNumbers
workflow.step(processOrder, {
  when: {
    ref: { step: 'auth', path: 'status' },
    query: { $eq: 'authenticated' }
  }
});
```

### 3. Simple Path Comparison
```typescript copy showLineNumbers
workflow.step(processOrder, {
  when: {
    "auth.status": "authenticated"
  }
});
```

Based on the type of condition, the workflow runner will try to match the condition to one of these types.

1. Simple Path Condition (when there's a dot in the key)
2. Base/Query Condition (when there's a 'ref' property)
3. Function Condition (when it's an async function)

## StepCondition

<PropertiesTable
  content={[
    {
      name: "ref",
      type: "{ stepId: string | 'trigger'; path: string }",
      description: "Reference to step output value. stepId can be a step ID or 'trigger' for initial data. path specifies location of value in step result",
      isOptional: false
    },
    {
      name: "query",
      type: "Query<any>",
      description: "MongoDB-style query using sift operators ($eq, $gt, etc)",
      isOptional: false
    }
  ]}
/>

## Query

The Query object provides MongoDB-style query operators for comparing values from previous steps or trigger data. It supports basic comparison operators like `$eq`, `$gt`, `$lt` as well as array operators like `$in` and `$nin`, and can be combined with and/or operators for complex conditions. 

This query syntax allows for readable conditional logic for determining whether a step should execute.

<PropertiesTable
  content={[
    {
      name: "$eq",
      type: "any",
      description: "Equal to value"
    },
    {
      name: "$ne", 
      type: "any",
      description: "Not equal to value"
    },
    {
      name: "$gt",
      type: "number",
      description: "Greater than value"
    },
    {
      name: "$gte",
      type: "number", 
      description: "Greater than or equal to value"
    },
    {
      name: "$lt",
      type: "number",
      description: "Less than value"
    },
    {
      name: "$lte",
      type: "number",
      description: "Less than or equal to value"
    },
    {
      name: "$in",
      type: "any[]",
      description: "Value exists in array"
    },
    {
      name: "$nin",
      type: "any[]", 
      description: "Value does not exist in array"
    },
    {
      name: "and",
      type: "StepCondition[]",
      description: "Array of conditions that must all be true"
    },
    {
      name: "or",
      type: "StepCondition[]",
      description: "Array of conditions where at least one must be true"
    }
  ]}
/>

## Related

- [Step Options Reference](./step-options.mdx)
- [Step Function Reference](./step-function.mdx)
- [Control Flow Guide](../../workflows/control-flow.mdx)
```


================================================================================
Source: src/pages/docs/reference/workflows/step-function.mdx
================================================================================

# step()

The `.step()` method adds a new step to the workflow, optionally configuring its variables and execution conditions.

## Usage

```typescript
workflow.step({
  id: "stepTwo",
  outputSchema: z.object({
    result: z.number()
  }),
  execute: async ({ context }) => {
    return { result: 42 };
  }
});
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "stepConfig",
      type: "Step | StepDefinition | string",
      description: "Step instance, configuration object, or step ID to add to workflow",
      isOptional: false
    },
    {
      name: "options",
      type: "StepOptions",
      description: "Optional configuration for step execution",
      isOptional: true
    }
  ]}
/>

### StepDefinition

<PropertiesTable
  content={[
    {
      name: "id",
      type: "string",
      description: "Unique identifier for the step",
      isOptional: false
    },
    {
      name: "outputSchema",
      type: "z.ZodSchema",
      description: "Schema for validating step output",
      isOptional: true
    },
    {
      name: "execute",
      type: "(params: ExecuteParams) => Promise<any>",
      description: "Function containing step logic",
      isOptional: false
    }
  ]}
/>

### StepOptions

<PropertiesTable
  content={[
    {
      name: "variables",
      type: "Record<string, VariableRef>",
      description: "Map of variable names to their source references",
      isOptional: true
    },
    {
      name: "when",
      type: "StepCondition",
      description: "Condition that must be met for step to execute",
      isOptional: true
    }
  ]}
/>

## Related
- [Basic Usage with Step Instance](../../workflows/steps.mdx)
- [Step Class Reference](./step.mdx)
- [Workflow Class Reference](./workflow.mdx)
- [Control Flow Guide](../../workflows/control-flow.mdx)
```


================================================================================
Source: src/pages/docs/reference/workflows/step-options.mdx
================================================================================



```markdown
# StepOptions

Configuration options for workflow steps that control variable mapping, execution conditions, and other runtime behavior.

## Usage

```typescript
workflow.step(processOrder, {
  variables: {
    orderId: { step: 'trigger', path: 'id' },
    userId: { step: 'auth', path: 'user.id' }
  },
  when: {
    ref: { step: 'auth', path: 'status' },
    query: { $eq: 'authenticated' }
  }
});
```

## Properties

<PropertiesTable
  content={[
    {
      name: "variables",
      type: "Record<string, VariableRef>",
      description: "Maps step input variables to values from other steps",
      isOptional: true
    },
    {
      name: "when",
      type: "StepCondition",
      description: "Condition that must be met for step execution",
      isOptional: true
    }
  ]}
/>

### VariableRef

<PropertiesTable
  content={[
    {
      name: "step",
      type: "string | Step | { id: string }",
      description: "Source step for the variable value",
      isOptional: false
    },
    {
      name: "path",
      type: "string",
      description: "Path to the value in the step's output",
      isOptional: false
    }
  ]}
/>

## Related
- [Path Comparison](../../workflows/control-flow.mdx#path-comparison)
- [Step Function Reference](./step-function.mdx)
- [Step Class Reference](./step-class.mdx)
- [Workflow Class Reference](./workflow.mdx)
- [Control Flow Guide](../../workflows/control-flow.mdx)
```

================================================================================
Source: src/pages/docs/reference/workflows/then.mdx
================================================================================

# then()

The `then` method creates a sequential dependency between workflow steps, ensuring steps execute in a specific order.

## Usage

```typescript
workflow
  .step(stepOne)
  .then(stepTwo)
  .then(stepThree);
```

## Parameters

<PropertiesTable
  content={[
    {
      name: "step",
      type: "Step | string",
      description: "The step instance or step ID that should execute after the previous step completes",
      isOptional: false
    }
  ]}
/>

## Returns

<PropertiesTable
  content={[
    {
      name: "workflow",
      type: "Workflow",
      description: "The workflow instance for method chaining"
    }
  ]}
/>

## Validation

When using `then`:
- The previous step must exist in the workflow
- Steps cannot form circular dependencies
- Each step can only appear once in a sequential chain

## Error Handling

```typescript
try {
  workflow
    .step(stepA)
    .then(stepB)
    .then(stepA) // Will throw error - circular dependency
    .commit();
} catch (error) {
  if (error instanceof ValidationError) {
    console.log(error.type); // 'circular_dependency'
    console.log(error.details);
  }
}
```

## Related

- [step Reference](./step.mdx)
- [after Reference](./after.mdx)
- [Sequential Steps Example](../../examples/workflows/sequential-steps.mdx)
- [Control Flow Guide](../../workflows/control-flow.mdx)
```


================================================================================
Source: src/pages/docs/reference/workflows/workflow.mdx
================================================================================

---
title: Workflow
description: API Reference for Workflow class
---

# Workflow

The Workflow class enables you to create state machines for complex sequences of operations with conditional branching and data validation.

```ts copy
import { Workflow } from "@mastra/core";

const workflow = new Workflow({ name: "my-workflow" });
```

## API Reference

### Constructor

<PropertiesTable
  content={[
    {
      name: "name",
      type: "string",
      description: "Identifier for the workflow",
    },
    {
      name: "logger",
      type: "Logger<WorkflowLogMessage>",
      isOptional: true,
      description: "Optional logger instance for workflow execution details",
    },
    {
      name: "steps",
      type: "Step[]",
      description: "Array of steps to include in the workflow",
    },
    {
      name: "triggerSchema",
      type: "z.Schema",
      description: "Optional schema for validating workflow trigger data",
    },
  ]}
/>

### Core Methods

#### `step()`

Adds a [Step](./step.mdx) to the workflow, including transitions to other steps. Returns the workflow instance for chaining. [Learn more about steps](./step.mdx).

#### `commit()`

Validates and finalizes the workflow configuration. Must be called after adding all steps.

#### `execute()`

Executes the workflow with optional trigger data. Typed based on the [trigger schema](./workflow.mdx#trigger-schemas).

## Trigger Schemas

Trigger schemas validate the initial data passed to a workflow using Zod.

```ts showLineNumbers copy
const workflow = new Workflow({
  name: "order-process",
  triggerSchema: z.object({
    orderId: z.string(),
    customer: z.object({
      id: z.string(),
      email: z.string().email(),
    }),
  }),
});
```

The schema:

- Validates data passed to `execute()`
- Provides TypeScript types for your workflow input

## Variables & Data Flow

Variables allow steps to access data from:

- Previous steps' outputs
- Trigger data

Variables payloads are typesafe with fields defined in the [Step](./step.mdx) `inputSchema`.

```ts showLineNumbers copy
workflow
  .step("createOrder", {
    // Access trigger data
    variables: {
      orderId: { stepId: "trigger", path: "orderId" },
    },
  })
  .step("processPayment", {
    variables: {
      // Access previous step's data
      orderStatus: { stepId: "createOrder", path: "status" },
      amount: { stepId: "createOrder", path: "total" },
    },
  });
```

#### Variable Resolution

- Variables are resolved in order of step execution
- Each step can access outputs of all previous steps
- Paths use dot notation for nested data
- Missing or invalid paths throw errors during execution

### Example

```ts showLineNumbers copy
const workflow = new Workflow({
  name: "process-data",
  triggerSchema: z.object({
    items: z.array(
      z.object({
        id: z.number(),
        value: z.number(),
      }),
    ),
  }),
})
  .step("filter", {
    variables: {
      items: { stepId: "trigger", path: "." },
    },
  })
  .step("process", {
    variables: {
      items: { stepId: "filter", path: "filtered.user.name" },
    },
  })
  .commit();
```

## Validation

Workflow validation happens at two key times:

### 1. At Commit Time

When you call `.commit()`, the workflow validates:

```ts showLineNumbers copy
workflow
  .step('step1', {...})
  .step('step2', {...})
  .commit(); // Validates workflow structure
```

- Circular dependencies between steps
- Terminal paths (every path must end)
- Unreachable steps
- Variable references to non-existent steps
- Duplicate step IDs

### 2. During Execution

When you call `start()`, it validates:

```ts showLineNumbers copy
const { runId, start } = workflow.createRun();

// Validates trigger data against schema
await start({
  triggerData: {
    orderId: "123",
    customer: {
      id: "cust_123",
      email: "invalid-email", // Will fail validation
    },
  },
});
```

- Trigger data against trigger schema
- Each step's input data against its inputSchema
- Variable paths exist in referenced step outputs
- Required variables are present

### Error Handling

```ts showLineNumbers copy
try {
  const { runId, start } = workflow.createRun();
  await start({ triggerData: data });
} catch (error) {
  if (error instanceof ValidationError) {
    // Handle validation errors
    console.log(error.type); // 'circular_dependency' | 'no_terminal_path' | 'unreachable_step'
    console.log(error.details); // { stepId?: string, path?: string[] }
  }
}
```

## Related Documentation

- [Step](./step.mdx)
- [Transition](./transition.mdx)


================================================================================
Source: src/pages/docs/workflows/00-overview.mdx
================================================================================

---
title: Workflows in Mastra
---

# Workflows in Mastra

Workflows in Mastra help you orchestrate complex sequences of operations with features like branching, parallel execution, resource suspension, and more. 

## When to use workflows

Most AI applications need more than a single call to a language model. You may want to run multiple steps, conditionally skip certain paths, or even pause execution altogether until you receive user input. Sometimes your agent tool calling is not accurate enough.

Mastra‚Äôs workflow system provides:

- A standardized way to define steps and link them together.
- Support for both simple (linear) and advanced (branching, parallel) paths.
- Debugging and observability features to track each workflow run.

## Example

To create a workflow, you define one or more steps, link them, and then commit the workflow before starting it.

Typically, you would call an LLM or other service in some way in some step. But for this example, we'll just double the input value in the first step and increment it in the second.

```typescript copy showLineNumbers
import { Workflow, Step } from "@mastra/core";
import { z } from "zod";

// 1. Define the workflow
const myWorkflow = new Workflow({
  name: "my-workflow",
  triggerSchema: z.object({
    inputValue: z.number(),
  }),
});

// 2. Create steps
const stepOne = new Step({
  id: "stepOne",
  execute: async ({ context: { machineContext } }) => ({
    doubledValue: machineContext.triggerData.inputValue * 2
  })
});

const stepTwo = new Step({
  id: "stepTwo",
  execute: async ({ context: { machineContext } }) => ({
    incrementedValue: machineContext.stepResults.stepOne.payload.doubledValue + 1
  })
});

// 3. Link and commit steps
myWorkflow.step(stepOne).then(stepTwo).commit();

// 4. Run the workflow
const { runId, start } = myWorkflow.createRun({ triggerData: { inputValue: 3 } });

const result = await start();
console.log("Workflow result:", result.results);
```

This example shows the essentials: define your workflow, add steps, commit the workflow, then execute it. 

## Defining Steps

The basic building block of a workflow [is a step](./steps.mdx). Steps are defined using schemas for inputs and outputs, and can fetch prior step results.

## Control Flow

Workflows let you define a [control flow](./control-flow.mdx) to chain steps together in with parallel steps, branching paths, and more.

## Suspend and Resume

When you need to pause execution for external data, user input, or asynchronous events, Mastra [supports suspension at any step](./suspend-and-resume.mdx), persisting the state of the workflow so you can resume it later.

## Observability and Debugging

Mastra workflows automatically [log the input and output of each step within a workflow run](../reference/observability/otel-config.mdx), allowing you to send this data to your preferred logging, telemetry, or observability tools. 

You can:

- Track the status of each step (e.g., `success`, `error`, or `suspended`).
- Store run-specific metadata for analysis.
- Integrate with third-party observability platforms like Datadog or New Relic by forwarding logs.

## More Resources

- The [Workflow Guide](../guides/04-recruiter.mdx) in the Guides section is a tutorial that covers the main concepts.
- [Sequential Steps workflow example]("../examples/workflows/sequential-steps.mdx")
- [Parallel Steps workflow example]("../examples/workflows/parallel-steps.mdx")
- [Branching Paths workflow example]("../examples/workflows/branching-paths.mdx")
- [Cyclical Dependencies workflow example]("../examples/workflows/cyclical-dependencies.mdx")
- [Suspend and Resume workflow example]("../examples/workflows/suspend-and-resume.mdx")



================================================================================
Source: src/pages/docs/workflows/control-flow.mdx
================================================================================

# Control Flow in Workflows

When you create a multi-step process, you may need to run steps in parallel, chain them sequentially, or follow different paths based on outcomes. This page describes how you can manage branching, merging, and conditions to construct workflows that meet your logic requirements. The code snippets show the key patterns for structuring complex control flow.

## Parallel Execution

You can run multiple steps at the same time if they don‚Äôt depend on each other. This approach can speed up your workflow when steps perform independent tasks. The code below shows how to add two steps in parallel:

```typescript
myWorkflow.step(fetchUserData).step(fetchOrderData);
```

See the [Parallel Steps](../examples/workflows/parallel-steps.mdx) example for more details.

## Sequential Execution

Sometimes you need to run steps in strict order to ensure outputs from one step become inputs for the next. Use .then() to link dependent operations. The code below shows how to chain steps sequentially:

```typescript
myWorkflow.step(fetchOrderData).then(validateData).then(processOrder);
```

See the [Sequential Steps](../examples/workflows/sequential-steps.mdx) example for more details.

## Branching and Merging Paths

When different outcomes require different paths, branching is helpful. You can also merge paths later once they complete. The code below shows how to branch after stepA and later converge on stepF:

```typescript
myWorkflow
  .step(stepA)
    .then(stepB)
    .then(stepD)
  .after(stepA)
    .step(stepC)
    .then(stepE)
  .after([stepD, stepE])
    .step(stepF);
```

In this example:

- stepA leads to stepB, then to stepD.  
- Separately, stepA also triggers stepC, which in turn leads to stepE.  
- The workflow waits for both stepD and stepE to finish before proceeding to stepF.

See the [Branching Paths](../examples/workflows/branching-paths.mdx) example for more details.

## Cyclical Dependencies

You can loop back to earlier steps based on conditions, allowing you to repeat tasks until certain results are achieved. The code below shows a workflow that repeats fetchData when a status is ‚Äúretry‚Äù:

```typescript
myWorkflow
  .step(fetchData)
  .then(processData)
  .after(processData)
  .step(finalizeData, {
    when: { "processData.status": "success" },
  })
  .step(fetchData, {
    when: { "processData.status": "retry" },
  });
```

If processData returns ‚Äúsuccess,‚Äù finalizeData runs. If it returns ‚Äúretry,‚Äù the workflow loops back to fetchData.

See the [Cyclical Dependencies](../examples/workflows/cyclical-dependencies.mdx) example for more details.

## Conditions

Use the when property to control whether a step runs based on data from previous steps. Below are three ways to specify conditions.

### Option 1: Function

```typescript
myWorkflow.step(
  new Step({
    id: "processData",
    execute: async ({ context }) => {
      // Action logic
    },
  }),
  {
    when: async ({ context }) => {
      const fetchData = context?.getStepPayload<{ status: string }>("fetchData");
      return fetchData?.status === "success";
    },
  },
);
```

### Option 2: Query Object

```typescript
myWorkflow.step(
  new Step({
    id: "processData",
    execute: async ({ context }) => {
      // Action logic
    },
  }),
  {
    when: {
      ref: {
        step: {
          id: "fetchData",
        },
        path: "status",
      },
      query: { $eq: "success" },
    },
  },
);
```

### Option 3: Simple Path Comparison

```typescript
myWorkflow.step(
  new Step({
    id: "processData",
    execute: async ({ context }) => {
      // Action logic
    },
  }),
  {
    when: {
      "fetchData.status": "success",
    },
  },
);
```

## Renaming Variables

Variables let you pass outputs from one step into another step‚Äôs inputs.

### Passing Trigger Data

```typescript
myWorkflow.step(stepOne).then(stepTwo, {
  variables: {
    valueToIncrement: {
      step: "trigger",
      path: "inputValue",
    },
  },
});
```

### Passing Output from a Previous Step

```typescript
myWorkflow.step(stepOne).then(stepTwo, {
  variables: {
    valueToIncrement: {
      step: stepOne,
      path: "doubledValue",
    },
  },
});
```

### Passing Output Using a Step ID

```typescript
myWorkflow.step(stepOne).then(stepTwo, {
  variables: {
    valueToIncrement: {
      step: {
        id: "stepOne",
      },
      path: "doubledValue",
    },
  },
});
```

In all these examples, you pick the specific data you want to pass forward. This approach helps decouple steps and keep your workflow logic clear.


================================================================================
Source: src/pages/docs/workflows/steps.mdx
================================================================================

# Defining Steps in a Workflow

When you build a workflow, you typically break down operations into smaller tasks that can be linked and reused. Steps provide a structured way to manage these tasks by defining inputs, outputs, and execution logic. 

The code below shows how to define these steps inline or separately.

## Inline Step Creation

You can create steps directly within your workflow using `.step()` and `.then()`. This code shows how to define, link, and execute two steps in sequence.

```typescript showLineNumbers filename="src/mastra/workflows/index.ts" copy
import { Step, Workflow } from "@mastra/core";
import { z } from "zod";

export const myWorkflow = new Workflow({
  name: "my-workflow",
  triggerSchema: z.object({
    inputValue: z.number(),
  }),
});

myWorkflow
  .step(
    new Step({
      id: "stepOne",
      outputSchema: z.object({
        doubledValue: z.number(),
      }),
      execute: async ({ context: { machineContext } }) => ({
        doubledValue: machineContext.triggerData.inputValue * 2,
      }),
    }),
  )
  .then(
    new Step({
      id: "stepTwo",
      outputSchema: z.object({
        incrementedValue: z.number(),
      }),
      execute: async ({ context: { machineContext } }) => ({
        incrementedValue: machineContext.stepResults.stepOne.payload.doubledValue + 1,
      }),
    }),
  );
```

## Creating Steps Separately

If you prefer to manage your step logic in separate entities, you can define steps outside and then add them to your workflow. This code shows how to define steps independently and link them afterward.

```typescript showLineNumbers filename="src/mastra/workflows/index.ts" copy
import { Step, Workflow } from "@mastra/core";
import { z } from "zod";

// Define steps separately
const stepOne = new Step({
  id: "stepOne",
  outputSchema: z.object({
    doubledValue: z.number(),
  }),
  execute: async ({ context: { machineContext } }) => ({
    doubledValue: machineContext.triggerData.inputValue * 2,
  }),
});

const stepTwo = new Step({
  id: "stepTwo",
  outputSchema: z.object({
    incrementedValue: z.number(),
  }),
  execute: async ({ context: { machineContext } }) => ({
    incrementedValue: machineContext.stepResults.stepOne.payload.doubledValue + 1,
  }),
});

// Build the workflow
const myWorkflow = new Workflow({
  name: "my-workflow",
  triggerSchema: z.object({
    inputValue: z.number(),
  }),
});

myWorkflow.step(stepOne).then(stepTwo);
myWorkflow.commit();
```


================================================================================
Source: src/pages/docs/workflows/suspend-and-resume.mdx
================================================================================

# Suspend and Resume in Workflows

Complex workflows often need to pause execution while waiting for external input or resources. 

Mastra's suspend and resume features let you pause workflow execution at any step, persist the workflow state, and continue when ready.

## When to Use Suspend/Resume

Common scenarios for suspending workflows include:
- Waiting for human approval or input
- Pausing until external API resources become available  
- Collecting additional data needed for later steps
- Rate limiting or throttling expensive operations

## Basic Suspend Example

Here's a simple workflow that suspends when a value is too low and resumes when given a higher value:

```typescript
const stepTwo = new Step({
  id: "stepTwo",
  outputSchema: z.object({
    incrementedValue: z.number(), 
  }),
  execute: async ({ context, suspend }) => {
    const currentValue = context.stepResults.stepOne.payload.doubledValue;
    
    if (currentValue < 100) {
      await suspend();
      return { incrementedValue: 0 };
    }
    return { incrementedValue: currentValue + 1 };
  },
});
```

## Watching and Resuming

To handle suspended workflows, use the `watch` method to monitor workflow status and `resume` to continue execution:

```typescript
// Create and start the workflow
const { runId, start } = myWorkflow.createRun({ 
  triggerData: { inputValue: 45 } 
});

// Start watching the workflow before executing it
myWorkflow.watch(runId, async ({ context }) => {
  // Check each step's status
  const stepStatus = context.stepResults?.stepTwo?.status;
  
  if (stepStatus === 'suspended') {
    console.log('Workflow suspended, resuming with new value');
    
    // Resume the workflow with new context
    await myWorkflow.resume({
      runId,
      stepId: 'stepTwo',
      context: { 
        secondValue: 60  // This value will be added to the original value
      },
    });
  }
});

// Start the workflow execution
const result = await start();
```

## Related Resources

- See the [Suspend and Resume Example](../../examples/workflows/suspend-and-resume.mdx) for a complete working example
- Check the [Step Class Reference](../reference/workflows/step.mdx) for suspend/resume API details
- Review [Workflow Observability](../reference/observability/otel-config.mdx) for monitoring suspended workflows

================================================================================
Source: src/pages/examples/agents/agentic-workflows.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Agentic Workflows

When building AI applications, you often need to coordinate multiple steps that depend on each other's outputs. This example shows how to create an AI workflow that fetches weather data and uses it to suggest activities, demonstrating how to integrate external APIs with LLM-powered planning.

```ts showLineNumbers copy
import { Mastra, Step, Workflow } from "@mastra/core";
import { z } from "zod";

const fetchWeather = new Step({
  id: "fetch-weather",
  description: "Fetches weather forecast for a given city",
  inputSchema: z.object({
    city: z.string().describe("The city to get the weather for"),
  }),
  execute: async ({ context }) => {
    const triggerData = context.machineContext?.getStepPayload<{
      city: string;
    }>("trigger");

    if (!triggerData) {
      throw new Error("Trigger data not found");
    }

    const geocodingUrl = `https://geocoding-api.open-meteo.com/v1/search?name=${encodeURIComponent(triggerData.city)}&count=1`;
    const geocodingResponse = await fetch(geocodingUrl);
    const geocodingData = await geocodingResponse.json();

    if (!geocodingData.results?.[0]) {
      throw new Error(`Location '${triggerData.city}' not found`);
    }

    const { latitude, longitude, name } = geocodingData.results[0];

    const weatherUrl = `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&daily=temperature_2m_max,temperature_2m_min,precipitation_probability_mean,weathercode&timezone=auto`;
    const response = await fetch(weatherUrl);
    const data = await response.json();

    const forecast = data.daily.time.map((date: string, index: number) => ({
      date,
      maxTemp: data.daily.temperature_2m_max[index],
      minTemp: data.daily.temperature_2m_min[index],
      precipitationChance: data.daily.precipitation_probability_mean[index],
      condition: getWeatherCondition(data.daily.weathercode[index]),
      location: name,
    }));

    return forecast;
  },
});

const forecastSchema = z.array(
  z.object({
    date: z.string(),
    maxTemp: z.number(),
    minTemp: z.number(),
    precipitationChance: z.number(),
    condition: z.string(),
    location: z.string(),
  }),
);

const planActivities = new Step({
  id: "plan-activities",
  description: "Suggests activities based on weather conditions",
  inputSchema: forecastSchema,
  execute: async ({ context, mastra }) => {
    const forecast =
      context.machineContext?.getStepPayload<z.infer<typeof forecastSchema>>(
        "fetch-weather",
      );

    if (!forecast) {
      throw new Error("Forecast data not found");
    }

    const prompt = `Based on the following weather forecast for ${forecast[0].location}, suggest appropriate activities:
      ${JSON.stringify(forecast, null, 2)}
      `;

    if (!mastra?.llm) {
      throw new Error("Mastra not found");
    }

    const llm = mastra.llm({
      provider: "OPEN_AI",
      name: "gpt-4o",
    });

    const response = await llm.stream([
      {
        role: "system",
        content: `You are a local activities and travel expert who excels at weather-based planning. Analyze the weather data and provide practical activity recommendations.
        For each day in the forecast, structure your response exactly as follows:
        üìÖ [Day, Month Date, Year]
        ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        üå°Ô∏è WEATHER SUMMARY
        ‚Ä¢ Conditions: [brief description]
        ‚Ä¢ Temperature: [X¬∞C/Y¬∞F to A¬∞C/B¬∞F]
        ‚Ä¢ Precipitation: [X% chance]
        üåÖ MORNING ACTIVITIES
        Outdoor:
        ‚Ä¢ [Activity Name] - [Brief description including specific location/route]
          Best timing: [specific time range]
          Note: [relevant weather consideration]
        üåû AFTERNOON ACTIVITIES
        Outdoor:
        ‚Ä¢ [Activity Name] - [Brief description including specific location/route]
          Best timing: [specific time range]
          Note: [relevant weather consideration]
        üè† INDOOR ALTERNATIVES
        ‚Ä¢ [Activity Name] - [Brief description including specific venue]
          Ideal for: [weather condition that would trigger this alternative]
        ‚ö†Ô∏è SPECIAL CONSIDERATIONS
        ‚Ä¢ [Any relevant weather warnings, UV index, wind conditions, etc.]
        Guidelines:
        - Suggest 2-3 time-specific outdoor activities per day
        - Include 1-2 indoor backup options
        - For precipitation >50%, lead with indoor activities
        - All activities must be specific to the location
        - Include specific venues, trails, or locations
        - Consider activity intensity based on temperature
        - Keep descriptions concise but informative
        Maintain this exact formatting for consistency, using the emoji and section headers as shown.`,
      },
      {
        role: "user",
        content: prompt,
      },
    ]);

    for await (const chunk of response.textStream) {
      process.stdout.write(chunk);
    }

    return {
      activities: response.text,
    };
  },
});

function getWeatherCondition(code: number): string {
  const conditions: Record<number, string> = {
    0: "Clear sky",
    1: "Mainly clear",
    2: "Partly cloudy",
    3: "Overcast",
    45: "Foggy",
    48: "Depositing rime fog",
    51: "Light drizzle",
    53: "Moderate drizzle",
    55: "Dense drizzle",
    61: "Slight rain",
    63: "Moderate rain",
    65: "Heavy rain",
    71: "Slight snow fall",
    73: "Moderate snow fall",
    75: "Heavy snow fall",
    95: "Thunderstorm",
  };
  return conditions[code] || "Unknown";
}

const weatherWorkflow = new Workflow({
  name: "weather-workflow",
  triggerSchema: z.object({
    city: z.string().describe("The city to get the weather for"),
  }),
})
  .step(fetchWeather)
  .then(planActivities);

weatherWorkflow.commit();

const mastra = new Mastra({
  workflows: {
    weatherWorkflow,
  },
});

async function main() {
  const { start } = mastra.getWorkflow("weatherWorkflow").createRun();

  const result = await start({
    triggerData: {
      city: "London",
    },
  });

  console.log("\n \n");
  console.log(result);
}

main();
```

<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/agentic-workflows"
  }
/>


================================================================================
Source: src/pages/examples/agents/bird-checker.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Making a Bird Checker

We will get a random image from [Unsplash](https://unsplash.com/) that matches a selected query and uses a [Mastra AI Agent](https://mastra.ai/docs/guide/creating-agents/00-overview) to determine if it is a bird or not

```ts showLineNumbers copy
import { Agent } from "@mastra/core";
import { z } from "zod";

export type Image = {
  alt_description: string;
  urls: {
    regular: string;
    raw: string;
  };
  user: {
    first_name: string;
    links: {
      html: string;
    };
  };
};

export type ImageResponse<T, K> =
  | {
      ok: true;
      data: T;
    }
  | {
      ok: false;
      error: K;
    };

const getRandomImage = async ({
  query,
}: {
  query: string;
}): Promise<ImageResponse<Image, string>> => {
  const page = Math.floor(Math.random() * 20);
  const order_by = Math.random() < 0.5 ? "relevant" : "latest";
  try {
    const res = await fetch(
      `https://api.unsplash.com/search/photos?query=${query}&page=${page}&order_by=${order_by}`,
      {
        method: "GET",
        headers: {
          Authorization: `Client-ID ${process.env.UNSPLASH_ACCESS_KEY}`,
          "Accept-Version": "v1",
        },
        cache: "no-store",
      },
    );

    if (!res.ok) {
      return {
        ok: false,
        error: "Failed to fetch image",
      };
    }

    const data = (await res.json()) as {
      results: Array<Image>;
    };
    const randomNo = Math.floor(Math.random() * data.results.length);

    return {
      ok: true,
      data: data.results[randomNo] as Image,
    };
  } catch (err) {
    return {
      ok: false,
      error: "Error fetching image",
    };
  }
};

const instructions = `
  You can view an image and figure out if it is a bird or not. 
  You can also figure out the species of the bird and where the picture was taken.
`;

export const birdCheckerAgent = new Agent({
  name: "Bird checker",
  instructions,
  model: {
    provider: "ANTHROPIC",
    name: "claude-3-haiku-20240307",
    toolChoice: "auto",
  },
});

const queries: string[] = ["wildlife", "feathers", "flying", "birds"];
const randomQuery = queries[Math.floor(Math.random() * queries.length)];

// Get the image url from Unsplash with random type
const imageResponse = await getRandomImage({ query: randomQuery });

if (!imageResponse.ok) {
  console.log("Error fetching image", imageResponse.error);
  process.exit(1);
}

console.log("Image URL: ", imageResponse.data.urls.regular);
const response = await birdCheckerAgent.generate(
  [
    {
      role: "user",
      content: [
        {
          type: "image",
          image: new URL(imageResponse.data.urls.regular),
        },
        {
          type: "text",
          text: "view this image and let me know if it's a bird or not, and the scientific name of the bird without any explanation. Also summarize the location for this picture in one or two short sentences understandable by a high school student",
        },
      ],
    },
  ],
  {
    output: z.object({
      bird: z.boolean(),
      species: z.string(),
      location: z.string(),
    }),
  },
);

console.log(response.object);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />

<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/bird-checker"
  }
/>


================================================================================
Source: src/pages/examples/agents/hierarchical-multi-agent.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Hierarchical Multi-Agent System

This example demonstrates how to create a hierarchical multi-agent system where agents interact through tool functions, with one agent coordinating the work of others.

The system consists of three agents:

1. A Publisher agent (supervisor) that orchestrates the process
2. A Copywriter agent that writes the initial content
3. An Editor agent that refines the content

First, define the Copywriter agent and its tool:

```ts showLineNumbers copy
const copywriterAgent = new Agent({
  name: "Copywriter",
  instructions: "You are a copywriter agent that writes blog post copy.",
  model: {
    provider: "ANTHROPIC",
    name: "claude-3-5-sonnet-20241022",
    toolChoice: "required",
  },
});

const copywriterTool = createTool({
  id: "copywriter-agent",
  description: "Calls the copywriter agent to write blog post copy.",
  inputSchema: z.object({
    topic: z.string().describe("Blog post topic"),
  }),
  outputSchema: z.object({
    copy: z.string().describe("Blog post copy"),
  }),
  execute: async ({ context }) => {
    const result = await copywriterAgent.generate(
      `Create a blog post about ${context.topic}`,
    );
    return { copy: result.text };
  },
});
```

Next, define the Editor agent and its tool:

```ts showLineNumbers copy
const editorAgent = new Agent({
  name: "Editor",
  instructions: "You are an editor agent that edits blog post copy.",
  model: {
    provider: "OPEN_AI",
    name: "gpt-4o",
  },
});

const editorTool = createTool({
  id: "editor-agent",
  description: "Calls the editor agent to edit blog post copy.",
  inputSchema: z.object({
    copy: z.string().describe("Blog post copy"),
  }),
  outputSchema: z.object({
    copy: z.string().describe("Edited blog post copy"),
  }),
  execute: async ({ context }) => {
    const result = await editorAgent.generate(
      `Edit the following blog post only returning the edited copy: ${context.copy}`,
    );
    return { copy: result.text };
  },
});
```

Finally, create the Publisher agent that coordinates the others:

```ts showLineNumbers copy
const publisherAgent = new Agent({
  name: "publisherAgent",
  instructions:
    "You are a publisher agent that first calls the copywriter agent to write blog post copy about a specific topic and then calls the editor agent to edit the copy. Just return the final edited copy.",
  model: {
    provider: "ANTHROPIC",
    name: "claude-3-5-sonnet-20241022",
  },
  tools: { copywriterTool, editorTool },
});

const mastra = new Mastra({
  agents: { publisherAgent },
});
```
To use the entire system: 

```ts showLineNumbers copy
async function main() {
  const agent = mastra.getAgent("publisherAgent");
  const result = await agent.generate(
    "Write a blog post about React JavaScript frameworks. Only return the final edited copy.",
  );
  console.log(result.text);
}

main();
``` 


<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />

<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/hierarchical-multi-agent"
  }
/>


================================================================================
Source: src/pages/examples/agents/multi-agent-workflow.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Multi-Agent Workflow

This example demonstrates how to create an agentic workflow with work product being passed between multiple agents with a worker agent and a supervisor agent.

In this example, we create a sequential workflow that calls two agents in order:

1. A Copywriter agent that writes the initial blog post
2. An Editor agent that refines the content

First, import the required dependencies:

```typescript
import { Agent, Step, Workflow } from "@mastra/core";
import { z } from "zod";
```

Create the copywriter agent that will generate the initial blog post:

```typescript
const copywriterAgent = new Agent({
  name: "Copywriter",
  instructions: "You are a copywriter agent that writes blog post copy.",
  model: {
    provider: "ANTHROPIC",
    name: "claude-3-5-sonnet-20241022",
    toolChoice: "required",
  },
});
```

Define the copywriter step that executes the agent and handles the response:

```typescript
const copywriterStep = new Step({
  id: "copywriterStep",
  execute: async ({ context: { machineContext } }) => {
    if (!machineContext?.triggerData?.topic) {
      throw new Error("Topic not found in trigger data");
    }
    const result = await copywriterAgent.generate(
      `Create a blog post about ${machineContext.triggerData.topic}`,
    );
    console.log("copywriter result", result.text);
    return {
      copy: result.text,
    };
  },
});
```

Set up the editor agent to refine the copywriter's content:

```typescript
const editorAgent = new Agent({
  name: "Editor",
  instructions: "You are an editor agent that edits blog post copy.",
  model: {
    provider: "OPEN_AI",
    name: "gpt-4o",
  },
});
```

Create the editor step that processes the copywriter's output:

```typescript
const editorStep = new Step({
  id: "editorStep",
  execute: async ({ context }) => {
    const copy = context?.machineContext?.getStepPayload<{ copy: number }>(
      "copywriterStep",
    )?.copy;

    const result = await editorAgent.generate(
      `Edit the following blog post only returning the edited copy: ${copy}`,
    );
    console.log("editor result", result.text);
    return {
      copy: result.text,
    };
  },
});
```

Configure the workflow and execute the steps:

```typescript
const myWorkflow = new Workflow({
  name: "my-workflow",
  triggerSchema: z.object({
    topic: z.string(),
  }),
});

// Run steps sequentially.
myWorkflow.step(copywriterStep).then(editorStep).commit();

const { runId, start } = myWorkflow.createRun();

const res = await start({
  triggerData: { topic: "React JavaScript frameworks" },
});
console.log("Results: ", res.results);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />

<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/multi-agent-workflow"
  }
/>


================================================================================
Source: src/pages/examples/agents/system-prompt.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Giving an Agent a System Prompt

When building AI agents, you often need to give them specific instructions and capabilities to handle specialized tasks effectively. System prompts allow you to define an agent's personality, knowledge domain, and behavioral guidelines. This example shows how to create an AI agent with custom instructions and integrate it with a dedicated tool for retrieving verified information.

```ts showLineNumbers copy
import { Agent, createTool } from "@mastra/core";

import { z } from "zod";

const instructions = `You are a helpful cat expert assistant. When discussing cats, you should always include an interesting cat fact.

  Your main responsibilities:
  1. Answer questions about cats
  2. Use the catFact tool to provide verified cat facts
  3. Incorporate the cat facts naturally into your responses

  Always use the catFact tool at least once in your responses to ensure accuracy.`;

const getCatFact = async () => {
  const { fact } = (await fetch("https://catfact.ninja/fact").then((res) =>
    res.json(),
  )) as {
    fact: string;
  };

  return fact;
};

const catFact = createTool({
  id: "Get cat facts",
  inputSchema: z.object({}),
  description: "Fetches cat facts",
  execute: async () => {
    console.log("using tool to fetch cat fact");
    return {
      catFact: await getCatFact(),
    };
  },
});

const catOne = new Agent({
  name: "cat-one",
  instructions: instructions,
  model: {
    provider: "OPEN_AI",
    name: "gpt-4o",
    toolChoice: "required",
  },
  tools: {
    catFact,
  },
});

const result = await catOne.generate("Tell me a cat fact");

console.log(result.text);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />

<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/system-prompt"
  }
/>


================================================================================
Source: src/pages/examples/agents/using-a-tool.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Using a Tool

When building AI agents, you often need to integrate external data sources or functionality to enhance their capabilities. This example shows how to create an AI agent that uses a dedicated weather tool to provide accurate weather information for specific locations.

```ts showLineNumbers copy
import { createTool, Agent, Mastra } from "@mastra/core";
import { z } from "zod";

interface WeatherResponse {
  current: {
    time: string;
    temperature_2m: number;
    apparent_temperature: number;
    relative_humidity_2m: number;
    wind_speed_10m: number;
    wind_gusts_10m: number;
    weather_code: number;
  };
}

const weatherTool = createTool({
  id: "get-weather",
  description: "Get current weather for a location",
  inputSchema: z.object({
    location: z.string().describe("City name"),
  }),
  outputSchema: z.object({
    temperature: z.number(),
    feelsLike: z.number(),
    humidity: z.number(),
    windSpeed: z.number(),
    windGust: z.number(),
    conditions: z.string(),
    location: z.string(),
  }),
  execute: async ({ context }) => {
    return await getWeather(context.location);
  },
});

const getWeather = async (location: string) => {
  const geocodingUrl = `https://geocoding-api.open-meteo.com/v1/search?name=${encodeURIComponent(location)}&count=1`;
  const geocodingResponse = await fetch(geocodingUrl);
  const geocodingData = await geocodingResponse.json();

  if (!geocodingData.results?.[0]) {
    throw new Error(`Location '${location}' not found`);
  }

  const { latitude, longitude, name } = geocodingData.results[0];

  const weatherUrl = `https://api.open-meteo.com/v1/forecast?latitude=${latitude}&longitude=${longitude}&current=temperature_2m,apparent_temperature,relative_humidity_2m,wind_speed_10m,wind_gusts_10m,weather_code`;

  const response = await fetch(weatherUrl);
  const data: WeatherResponse = await response.json();

  return {
    temperature: data.current.temperature_2m,
    feelsLike: data.current.apparent_temperature,
    humidity: data.current.relative_humidity_2m,
    windSpeed: data.current.wind_speed_10m,
    windGust: data.current.wind_gusts_10m,
    conditions: getWeatherCondition(data.current.weather_code),
    location: name,
  };
};

function getWeatherCondition(code: number): string {
  const conditions: Record<number, string> = {
    0: "Clear sky",
    1: "Mainly clear",
    2: "Partly cloudy",
    3: "Overcast",
    45: "Foggy",
    48: "Depositing rime fog",
    51: "Light drizzle",
    53: "Moderate drizzle",
    55: "Dense drizzle",
    56: "Light freezing drizzle",
    57: "Dense freezing drizzle",
    61: "Slight rain",
    63: "Moderate rain",
    65: "Heavy rain",
    66: "Light freezing rain",
    67: "Heavy freezing rain",
    71: "Slight snow fall",
    73: "Moderate snow fall",
    75: "Heavy snow fall",
    77: "Snow grains",
    80: "Slight rain showers",
    81: "Moderate rain showers",
    82: "Violent rain showers",
    85: "Slight snow showers",
    86: "Heavy snow showers",
    95: "Thunderstorm",
    96: "Thunderstorm with slight hail",
    99: "Thunderstorm with heavy hail",
  };
  return conditions[code] || "Unknown";
}

const weatherAgent = new Agent({
  name: "Weather Agent",
  instructions: `You are a helpful weather assistant that provides accurate weather information.
Your primary function is to help users get weather details for specific locations. When responding:
- Always ask for a location if none is provided
- Include relevant details like humidity, wind conditions, and precipitation
- Keep responses concise but informative
Use the weatherTool to fetch current weather data.`,
  model: {
    provider: "OPEN_AI",
    name: "gpt-4o",
  },
  tools: { weatherTool },
});

const mastra = new Mastra({
  agents: { weatherAgent },
});

async function main() {
  const agent = await mastra.getAgent("weatherAgent");
  const result = await agent.generate("What is the weather in London?");
  console.log(result.text);
}

main();
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />

<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/agents/using-a-tool"
  }
/>


================================================================================
Source: src/pages/examples/index.mdx
================================================================================

---
title: "Examples Overview"
description: "Explore practical examples of AI development with Mastra, including text generation, RAG implementations, structured outputs, and multi-modal interactions. Learn how to build AI applications using OpenAI, Anthropic, and Google Gemini."
---

import { CardItems, CardItem } from "../../components/example-cards";

# Overview

The Examples section is a short list of example projects demonstrating basic AI engineering with Mastra, including text generation, structured output, streaming responses, and retrieval‚Äêaugmented generation (RAG).

<CardItems>
  <CardItem
    title="LLM"
    links={[
      { title: "Generate Text", href: "examples/llms/generate-text" },
      {
        title: "Use a System Prompt",
        href: "/examples/llms/use-a-system-prompt",
      },
      { title: "Describe an Image", href: "/examples/llms/describe-an-image" },
      { title: "Call Claude", href: "/examples/llms/call-claude" },
      {
        title: "Call Google Gemini",
        href: "/examples/llms/call-google-gemini",
      },
      {
        title: "Generate Objects with Structured Output",
        href: "/examples/llms/generate-object-with-structured-output",
      },
      { title: "Stream Text", href: "/examples/llms/stream-text" },
      {
        title: "Stream Objects with Structured Output",
        href: "/examples/llms/stream-object-with-structured-output",
      },
    ]}
  />
  <CardItem
    title="Agent"
    links={[
      {
        title: "Agent with System Prompt",
        href: "/examples/agents/system-prompt",
      },
      {
        title: "Agentic Workflows",
        href: "/examples/agents/agentic-workflows",
      },
      {
        title: "Using a Tool",
        href: "/examples/agents/using-a-tool",
      },
      {
        title: "Hierarchical Multi-Agent System",
        href: "/examples/agents/hierarchical-multi-agent",
      },
      {
        title: "Multi-Agent Workflow",
        href: "/examples/agents/multi-agent-workflow",
      },
      {
        title: "Bird Checker",
        href: "/examples/agents/bird-checker",
      },
    ]}
  />
  <CardItem
    title="Workflow"
    links={[
      {
        title: "Creating a Workflow",
        href: "/examples/workflows/creating-a-workflow",
      },
      {
        title: "Using a Tool as a Step",
        href: "/examples/workflows/using-a-tool-as-a-step",
      },
      { title: "Parallel Steps", href: "/examples/workflows/parallel-steps" },
      {
        title: "Sequential Steps",
        href: "/examples/workflows/sequential-steps",
      },
      { title: "Branching Paths", href: "/examples/workflows/branching-paths" },
      {
        title: "Cyclical Dependencies",
        href: "/examples/workflows/cyclical-dependencies",
      },
      {
        title: "Suspend and Resume",
        href: "/examples/workflows/suspend-and-resume",
      },
      { title: "Calling an LLM", href: "/examples/workflows/calling-llm" },
      { title: "Calling an Agent", href: "/examples/workflows/calling-agent" },
    ]}
  />
  <CardItem
    title="RAG"
    links={[
      { title: "Chunk Text", href: "/examples/rag/chunk-text" },
      { title: "Chunk Markdown", href: "/examples/rag/chunk-markdown" },
      { title: "Chunk HTML", href: "/examples/rag/chunk-html" },
      { title: "Chunk JSON", href: "/examples/rag/chunk-json" },
      { title: "Embed Text Chunk", href: "/examples/rag/embed-text-chunk" },
      { title: "Embed Chunk Array", href: "/examples/rag/embed-chunk-array" },
      { title: "Adjust Chunk Size", href: "/examples/rag/adjust-chunk-size" },
      {
        title: "Adjust Chunk Delimiters",
        href: "/examples/rag/adjust-chunk-delimiters",
      },
      {
        title: "Embed Text with Cohere",
        href: "/examples/rag/embed-text-with-cohere",
      },
      {
        title: "Insert Embedding in Pinecone",
        href: "/examples/rag/insert-embedding-in-pinecone",
      },
      {
        title: "Insert Embedding in PgVector",
        href: "/examples/rag/insert-embedding-in-pgvector",
      },
      { title: "Retrieve Results", href: "/examples/rag/retrieve-results" },
      { title: "Basic RAG", href: "/examples/rag/basic-rag" },
      {
        title: "Optimizing Information Density",
        href: "/examples/rag/cleanup-rag",
      },
      { title: "Metadata Filtering", href: "/examples/rag/filter-rag" },
      { title: "Re-ranking Results", href: "/examples/rag/rerank-rag" },
      { title: "Chain of Thought Prompting", href: "/examples/rag/cot-rag" },
      {
        title: "Chain of Thought Prompting w/Workflow",
        href: "/examples/rag/cot-workflow-rag",
      },
      { title: "Graph RAG", href: "/examples/rag/graph-rag" },
    ]}
  />
</CardItems>


================================================================================
Source: src/pages/examples/llms/call-claude.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Generate Text with Claude

Many developers need to use different language models but don't want to learn multiple APIs. Mastra provides a unified interface for working with various LLM providers, handling the complexity of different API implementations. This example shows how to use Anthropic's Claude model through the same interface used for other providers.

```ts showLineNumbers copy
import { Mastra } from "@mastra/core";

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: "ANTHROPIC",
  name: "claude-3-5-sonnet-20241022",
});

const result = await llm.generate("Who invented the submarine?");

console.log(result.text);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/generate-text-with-claude"
  }
/>


================================================================================
Source: src/pages/examples/llms/call-google-gemini.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Generate Text with Gemini

Many developers need to use different language models but don't want to learn multiple APIs. Mastra provides a unified interface for working with various LLM providers, handling the complexity of different API implementations. This example shows how to use Google's Gemini model through the same interface used for other providers.

```ts showLineNumbers copy
import { Mastra } from '@mastra/core';

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: 'GOOGLE',
  name: 'gemini-1.5-flash',
  apiKey: process.env.GEMINI_API_KEY,
});

const result = await llm.generate('Who invented the submarine?');

console.log(result.text);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/generate-text-with-gemini'} />


================================================================================
Source: src/pages/examples/llms/describe-an-image.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Describing an Image

Vision-enabled language models can analyze both text and images, but combining them in a single prompt requires specific message formatting. This example demonstrates how to structure a multimodal request by combining a text question with an image URL in the message content array.

```ts showLineNumbers copy
import { Mastra } from '@mastra/core';

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: 'OPEN_AI',
  name: 'gpt-4-turbo',
});

const response = await llm.generate([
  {
    role: 'user',
    content: [
      {
        type: 'text',
        text: 'what is that black bold text at the top?',
      },
      {
        type: 'image',
        image: new URL(
          'https://upload.wikimedia.org/wikipedia/commons/thumb/0/03/491_BC_-_1902_AD_-_A_Long_Time_Between_Drinks.jpg/1000px-491_BC_-_1902_AD_-_A_Long_Time_Between_Drinks.jpg',
        ),
      },
    ],
  },
]);

console.log(response.text);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/generate-text-from-image'} />


================================================================================
Source: src/pages/examples/llms/generate-object-with-structured-output.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Generate Object With Structured Output

Sometimes you need the language model to return data in a specific format rather than free-form text. This example shows how to use Zod schemas to get structured JSON output from the model, making it easier to work with the response in your application.

```ts showLineNumbers copy
import { Mastra } from '@mastra/core';
import { z } from 'zod';

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: 'OPEN_AI',
  name: 'gpt-4o',
});

const recipeSchema = z.object({
  recipe: z.object({
    name: z.string(),
    ingredients: z.array(
      z.object({
        name: z.string(),
        amount: z.string(),
      }),
    ),
    steps: z.array(z.string()),
  }),
});

const result = await llm.generate('Generate a egusi recipe.', {
  output: recipeSchema,
});

console.log(JSON.stringify(result.object.recipe, null, 2));
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/generate-object-structured'} />


================================================================================
Source: src/pages/examples/llms/generate-text-from-pdf.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Generate Text from PDF

Some models support the `file` type. You can use this type to generate text from a PDF file. This example shows how to generate text from a PDF file using the `ANTHROPIC` provider.

```ts showLineNumbers copy
import { Mastra } from '@mastra/core';
import { readFileSync } from 'fs';
import path from 'path';

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: 'ANTHROPIC',
  name: 'claude-3-5-sonnet-20241022',
});


const buf = readFileSync(path.join(process.cwd(), './fridge-owners-manual.pdf'))

const response = await llm.generate([{
  role: "user",
  content: [
    {
      type: "file",
      mimeType: "application/pdf",
      data: buf
    },
    {
      type: "text",
      text: "Please confirm you can see this PDF file by saying 'YES I can see the PDF' and then tell me what's in it."
    }
  ]
}]);

console.log(response.text);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/generate-text-from-pdf'} />



================================================================================
Source: src/pages/examples/llms/generate-text.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Generate Text

When you need to get a quick answer from a language model, the simplest approach is to send a text prompt and receive a text response. This example shows how to initialize an LLM and generate text with a single line of code.

```ts showLineNumbers copy
import { Mastra } from '@mastra/core';

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: 'OPEN_AI',
  name: 'gpt-4o',
});

const response = await llm.generate('What is a wormhole? Explain briefly.');

console.log(response.text);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/generate-text'} />


================================================================================
Source: src/pages/examples/llms/stream-object-with-structured-output.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Stream Object with Structured Output

When you need structured data from a language model, waiting for the complete response can take time. By streaming the output, you can display partial results as they arrive, providing immediate feedback to users. This example shows how to stream JSON-formatted responses using a Zod schema.

```ts showLineNumbers copy
import { Mastra } from '@mastra/core';
import { z } from 'zod';

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: 'OPEN_AI',
  name: 'gpt-4o',
});

const recipeSchema = z.object({
  recipe: z.object({
    name: z.string(),
    ingredients: z.array(
      z.object({
        name: z.string(),
        amount: z.string(),
      }),
    ),
    steps: z.array(z.string()),
  }),
});

const result = await llm.stream('Generate a egusi recipe.', {
  output: recipeSchema,
});

for await (const chunk of result.textStream) {
  process.stdout.write(chunk);
}
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/stream-object-structured'} />


================================================================================
Source: src/pages/examples/llms/stream-text.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Stream Text

Text generation can sometimes take a long time to complete, especially when you're generating a couple of paragraphs. By streaming the response, you can display partial results as they arrive, providing immediate feedback to users. This example shows how to stream text responses in real-time.

```ts showLineNumbers copy
import { Mastra } from '@mastra/core';

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: 'OPEN_AI',
  name: 'gpt-4',
});

const response = await llm.stream('Tell me about christmas and it"s traditions');

for await (const chunk of response.textStream) {
  process.stdout.write(chunk);
}
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/stream-text'} />


================================================================================
Source: src/pages/examples/llms/use-a-system-prompt.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Generate Text with System Prompt

When interacting with language models, you can guide their behavior by providing initial instructions. A system prompt sets the overall context and behavior for the model before it processes any user messages. This example shows how to use system prompts to control model behavior.

```ts showLineNumbers
import { Mastra } from '@mastra/core';

const mastra = new Mastra();

const llm = mastra.LLM({
  provider: 'OPEN_AI',
  name: 'gpt-4',
});

const response = await llm.generate([
  { role: 'system', content: 'You are a helpful assistant.' },
  { role: 'user', content: 'What is the meaning of life?' },
]);

console.log(response.text);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/llms/generate-text-system-prompt'}
/>


================================================================================
Source: src/pages/examples/rag/adjust-chunk-delimiters.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Adjust Chunk Delimiters

When processing large documents, you may want to control how the text is split into smaller chunks. By default, documents are split on newlines, but you can customize this behavior to better match your content structure. This example shows how to specify a custom delimiter for chunking documents.

```tsx copy
import { MDocument } from "@mastra/rag";

const doc = MDocument.fromText("Your plain text content...");

const chunks = await doc.chunk({
  separator: "\n",
});
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/adjust-chunk-delimiters"
  }
/>


================================================================================
Source: src/pages/examples/rag/adjust-chunk-size.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Adjust Chunk Size

When processing large documents, you might need to adjust how much text is included in each chunk. By default, chunks are 1024 characters long, but you can customize this size to better match your content and memory requirements. This example shows how to set a custom chunk size when splitting documents.

```tsx copy
import { MDocument } from "@mastra/rag";

const doc = MDocument.fromText("Your plain text content...");

const chunks = await doc.chunk({
  size: 512,
});
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/adjust-chunk-size"
  }
/>


================================================================================
Source: src/pages/examples/rag/basic-rag.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Basic RAG

This example demonstrates how to implement a Retrieval-Augmented Generation (RAG) system using Mastra, OpenAI embeddings, and PGVector for vector storage.

## Overview

The system implements RAG using Mastra and OpenAI. Here's what it does:
1. Sets up a Mastra agent with GPT-4o-mini for response generation
2. Creates a vector query tool to manage vector store interactions
3. Chunks text documents into smaller segments
4. Creates embeddings for these chunks
5. Stores them in a PostgreSQL vector database
6. Retrieves relevant chunks based on queries using vector query tool
7. Generates context-aware responses using the Mastra agent

## Setup 

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
POSTGRES_CONNECTION_STRING=your_connection_string_here
```

### Dependencies

Then, import the necessary dependencies:

```typescript copy showLineNumbers filename="src/mastra/index.ts"
import { Mastra, Agent, EmbedManyResult } from '@mastra/core';
import { embed, MDocument, PgVector, createVectorQueryTool } from '@mastra/rag';
```

## Vector Query Tool Creation
Using createVectorQueryTool imported from @mastra/rag, you can create a tool that can query the vector database.
```typescript copy showLineNumbers{4} filename="src/mastra/index.ts"
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'embeddings',
  options: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002',
    maxRetries: 3,
  },
  topK: 3,
});
```

## Agent Configuration

Set up the Mastra agent that will handle the responses:

```typescript copy showLineNumbers{15} filename="src/mastra/index.ts"
export const ragAgent = new Agent({
  name: 'RAG Agent',
  instructions:
    'You are a helpful assistant that answers questions based on the provided context. Keep your answers concise and relevant.',
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o-mini',
  },
  tools: {
    vectorQueryTool,
  },
})
```

## Instantiate PgVector and Mastra

Instantiate PgVector and Mastra with all components:

```typescript copy showLineNumbers{28} filename="src/mastra/index.ts"
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);

export const mastra = new Mastra({
  agents: { ragAgent },
  vectors: { pgVector },
})

const agent = mastra.getAgent('ragAgent')
```

## Document Processing

Create a document and process it into chunks:

```typescript copy showLineNumbers{37} filename="src/mastra/index.ts"
const doc = MDocument.fromText(`The Impact of Climate Change on Global Agriculture...`)

const chunks = await doc.chunk({
  strategy: 'recursive',
  size: 512,
  overlap: 50,
  separator: '\n',
})
```

## Creating and Storing Embeddings

Generate embeddings for the chunks and store them in the vector database:

```typescript copy showLineNumbers{46} filename="src/mastra/index.ts"
const { embeddings } = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
}) as EmbedManyResult<string>

const vectorStore = mastra.getVector('pgVector');
await vectorStore.createIndex("embeddings", 1536)
await vectorStore.upsert(
  "embeddings",
  embeddings,
  chunks?.map((chunk: any) => ({ text: chunk.text }))
)
```

## Response Generation

Function to generate responses based on retrieved context:

```typescript copy showLineNumbers{60} filename="src/mastra/index.ts"
async function generateResponse(query: string) {
  const prompt = `
      Please answer the following question:
      ${query}

      Please base your answer only on the context provided in the tool. If the context doesn't 
      contain enough information to fully answer the question, please state that explicitly.
      `

  const completion = await agent.generate(prompt)
  return completion.text
}
```

## Example Usage

```typescript copy showLineNumbers{73} filename="src/mastra/index.ts"
async function answerQueries(queries: string[]) {
  for (const query of queries) {
    try {
      const answer = await generateResponse(query)
      console.log('\nQuery:', query)
      console.log('Response:', answer)
    } catch (error) {
      console.error(`Error processing query "${query}":`, error)
    }
  }
}

const queries = [
  "What are the main points in the article?",
  "How does temperature affect crop yields?",
  "What solutions are farmers implementing?",
  "What are the future challenges mentioned in the text?",
];

await answerQueries(queries)
```


<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/basic-rag'} />

================================================================================
Source: src/pages/examples/rag/chunk-html.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Chunk HTML

When working with HTML content, you often need to break it down into smaller, manageable pieces while preserving the document structure. The chunk method splits HTML content intelligently, maintaining the integrity of HTML tags and elements. This example shows how to chunk HTML documents for search or retrieval purposes.

```tsx copy
import { MDocument } from "@mastra/rag";

const html = `
<div>
    <h1>h1 content...</h1>
    <p>p content...</p>
</div>
`;

const doc = MDocument.fromHTML(html);

const chunks = await doc.chunk({
  headers: [
    ["h1", "Header 1"],
    ["p", "Paragraph"],
  ],
});

console.log(chunks);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/chunk-html"
  }
/>


================================================================================
Source: src/pages/examples/rag/chunk-json.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Chunk JSON

When working with JSON data, you need to split it into smaller pieces while preserving the object structure. The chunk method breaks down JSON content intelligently, maintaining the relationships between keys and values. This example shows how to chunk JSON documents for search or retrieval purposes.

```tsx copy
import { MDocument } from "@mastra/rag";

const testJson = {
  name: "John Doe",
  age: 30,
  email: "john.doe@example.com",
};

const doc = MDocument.fromJSON(JSON.stringify(testJson));

const chunks = await doc.chunk({
  maxSize: 100,
});

console.log(chunks);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/chunk-json"
  }
/>


================================================================================
Source: src/pages/examples/rag/chunk-markdown.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Chunk Markdown

Markdown is more information-dense than raw HTML, making it easier to work with for RAG pipelines. When working with markdown, you need to split it into smaller pieces while preserving headers and formatting. The `chunk` method handles Markdown-specific elements like headers, lists, and code blocks intelligently. This example shows how to chunk markdown documents for search or retrieval purposes.

```tsx copy
import { MDocument } from "@mastra/rag";

const doc = MDocument.fromMarkdown("# Your markdown content...");

const chunks = await doc.chunk();
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/chunk-markdown"
  }
/>


================================================================================
Source: src/pages/examples/rag/chunk-text.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Chunk Text

When working with large text documents, you need to break them down into smaller, manageable pieces for processing. The chunk method splits text content into segments that can be used for search, analysis, or retrieval. This example shows how to split plain text into chunks using default settings.

```tsx copy
import { MDocument } from "@mastra/rag";

const doc = MDocument.fromText("Your plain text content...");

const chunks = await doc.chunk();
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/chunk-text"
  }
/>


================================================================================
Source: src/pages/examples/rag/cleanup-rag.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Optimizing Information Density

This example demonstrates how to implement a Retrieval-Augmented Generation (RAG) system using Mastra, OpenAI embeddings, and PGVector for vector storage. 
The system uses an agent to clean the initial chunks to optimize information density and deduplicate data.

## Overview

The system implements RAG using Mastra and OpenAI, this time optimizing information density through LLM-based processing. Here's what it does:
1. Sets up two Mastra agents with GPT-4o-mini for response generation
2. Sets up another Mastra agent to handle cleaning up chunk data before vector storage
3. Creates a vector query tool to manage vector store interactions
4. Create a document chunking tool for agent to use to get chunks 
5. Chunks text documents into smaller segments
6. Takes those chunks and filters them to remove irrelevant or duplicate information
7. Creates embeddings for both the initial chunks and the updated chunks
8. Stores them both in a PostgreSQL vector database
9. Retrieves relevant chunks based on queries using vector query tool
10. Generates context-aware responses using the Mastra agents

## Setup 

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
POSTGRES_CONNECTION_STRING=your_connection_string_here
```

### Dependencies

Then, import the necessary dependencies:

```typescript copy showLineNumbers filename="src/mastra/index.ts"
import { Mastra, Agent, EmbedResult, EmbedManyResult } from '@mastra/core'
import { embed, MDocument, PgVector, createVectorQueryTool, createDocumentChunkerTool } from '@mastra/rag';
```

## Tool Creation

### Vector Query Tool
Using createVectorQueryTool imported from @mastra/rag, you can create a tool that can query the vector database.
```typescript copy showLineNumbers{4} filename="src/mastra/index.ts"
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'embeddings',
  options: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002',
    maxRetries: 3,
  },
});

const cleanedVectorQueryTool = createVectorQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'cleanedEmbeddings',
  options: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002',
    maxRetries: 3,
  },
});
```

### Chunk Tool
Using createDocumentChunkerTool imported from @mastra/rag, you can create a tool that chunks the document and sends the chunks to your agent.
```typescript copy showLineNumbers{24} filename="src/mastra/index.ts"
const doc = MDocument.fromText(yourText);

const documentChunkerTool = createDocumentChunkerTool({
  doc,
  params: {
    strategy: 'recursive',
    size: 256,
    overlap: 50,
    separator: '\n',
  },
});
```

## Agent Configuration

Set up three Mastra agents:

```typescript copy showLineNumbers{36} filename="src/mastra/index.ts"
export const ragAgentOne = new Agent({
  name: 'RAG Agent One',
  instructions:
    'You are a helpful assistant that answers questions based on the provided context. Keep your answers concise and relevant.',
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o-mini',
  },
  tools: {
    vectorQueryTool,
  },
});

export const ragAgentTwo = new Agent({
  name: 'RAG Agent Two',
  instructions:
    'You are a helpful assistant that answers questions based on the provided context. Keep your answers concise and relevant.',
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o-mini',
  },
  tools: {
    cleanedVectorQueryTool,
  },
});

export const ragAgentThree = new Agent({
  name: 'RAG Agent Three',
  instructions: 'You are a helpful assistant that processes, cleans, and labels data before storage.',
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o-mini',
  },
  tools: { documentChunkerTool },
});
```

## Instantiate PgVector and Mastra

Instantiate PgVector and Mastra with all components:

```typescript copy showLineNumbers{72} filename="src/mastra/index.ts"
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);

export const mastra = new Mastra({
  agents: { ragAgentOne, ragAgentTwo, ragAgentThree },
  vectors: { pgVector },
});
const dataAgentOne = mastra.getAgent('ragAgentOne');
const dataAgentTwo = mastra.getAgent('ragAgentTwo');
const processAgent = mastra.getAgent('ragAgentThree');
```

## Document Processing
Chunk the initial document clean them using the processAgent.
```typescript copy showLineNumbers{82} filename="src/mastra/index.ts"
const chunks = await doc.chunk({
  strategy: 'recursive',
  size: 256,
  overlap: 50,
  separator: '\n',
});

const chunkPrompt = `Take the chunks returned from the tool and clean them up according to the instructions provided. Make sure to filter out irrelevant information and remove duplicates.`;

const newChunks = await processAgent.generate(chunkPrompt);
const updatedDoc = MDocument.fromText(newChunks.text);

const updatedChunks = await updatedDoc.chunk({
  strategy: 'recursive',
  size: 256,
  overlap: 50,
  separator: '\n',
});
```

## Creating and Storing Embeddings

Generate and store both raw and cleaned embeddings:

```typescript copy showLineNumbers{101} filename="src/mastra/index.ts"
const { embeddings } = await embed(chunks, {
  provider: 'OPEN_AI',
  model: 'text-embedding-ada-002',
  maxRetries: 3,
}) as EmbedManyResult<string>;

const { embeddings: cleanedEmbeddings } = await embed(updatedChunks, {
  provider: 'OPEN_AI',
  model: 'text-embedding-ada-002',
  maxRetries: 3,
}) as EmbedManyResult<string>;

const vectorStore = mastra.getVector('pgVector');
await vectorStore.createIndex('embeddings', 1536);
await vectorStore.createIndex('cleanedEmbeddings', 1536);

await vectorStore.upsert(
  'embeddings',
  embeddings,
  chunks?.map((chunk: any) => ({ text: chunk.text })),
);

await vectorStore.upsert(
  'cleanedEmbeddings',
  cleanedEmbeddings,
  updatedChunks?.map((chunk: any) => ({ text: chunk.text })),
);
```

## Response Generation

Function to generate responses with index selection:

```typescript copy showLineNumbers{129} filename="src/mastra/index.ts"
async function generateResponse(query: string, agent: Agent) {
  const prompt = `
      Please answer the following question:
      ${query}

      Please base your answer only on the context provided in the tool with this index ${index}. 
      If the context doesn't contain enough information to fully answer the question, 
      please state that explicitly. 
      `;
  // Call the agent to generate a response
  const completion = await agent.generate(prompt);
  return completion.text;
}
```

## Example Usage

```typescript copy showLineNumbers{143} filename="src/mastra/index.ts"
async function answerQueries(queries: string[], agent: Agent) {
  for (const query of queries) {
    try {
      const answer = await generateResponse(query, agent);
      console.log('\nQuery:', query);
      console.log('Response:', answer);
    } catch (error) {
      console.error(`Error processing query "${query}":`, error);
    }
  }
}

const queries = [
  'What is the average temperature on Mars?',
  'What technologies are used in modern spacecraft?',
  'What are all the requirements for space settlements?',
  'What are all the dates mentioned related to space stations?',
  'What are all the mentions of sustainability in space settlements?',
];

// Compare responses between raw and cleaned embeddings
await answerQueries(queries, dataAgentOne);
await answerQueries(queries, dataAgentTwo);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/cleanup-rag'} />

================================================================================
Source: src/pages/examples/rag/cot-rag.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Chain of Thought Prompting

This example demonstrates how to implement a Retrieval-Augmented Generation (RAG) system using Mastra, OpenAI embeddings, and PGVector for vector storage, with an emphasis on chain-of-thought reasoning.

## Overview

The system implements RAG using Mastra and OpenAI with chain-of-thought prompting. Here's what it does:
1. Sets up a Mastra agent with GPT-4o-mini for response generation
2. Creates a vector query tool to manage vector store interactions
3. Chunks text documents into smaller segments
4. Creates embeddings for these chunks
5. Stores them in a PostgreSQL vector database
6. Retrieves relevant chunks based on queries using vector query tool
7. Generates context-aware responses using chain-of-thought reasoning

## Setup 

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
POSTGRES_CONNECTION_STRING=your_connection_string_here
```

### Dependencies

Then, import the necessary dependencies:

```typescript copy showLineNumbers filename="src/mastra/index.ts"
import { Mastra, Agent, EmbedManyResult } from '@mastra/core';
import { createVectorQueryTool, embed, MDocument, PgVector } from '@mastra/rag';
```

## Vector Query Tool Creation

Using createVectorQueryTool imported from @mastra/rag, you can create a tool that can query the vector database.
```typescript copy showLineNumbers{4} filename="src/mastra/index.ts"
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'embeddings',
  options: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002',
    maxRetries: 3,
  },
  topK: 3,
});
```

## Agent Configuration

Set up the Mastra agent with chain-of-thought prompting instructions:

```typescript copy showLineNumbers{15} filename="src/mastra/index.ts"
export const ragAgent = new Agent({
  name: 'RAG Agent',
  instructions: `You are a helpful assistant that answers questions based on the provided context.
Follow these steps for each response:

1. First, carefully analyze the retrieved context chunks and identify key information.
2. Break down your thinking process about how the retrieved information relates to the query.
3. Explain how you're connecting different pieces from the retrieved chunks.
4. Draw conclusions based only on the evidence in the retrieved context.
5. If the retrieved chunks don't contain enough information, explicitly state what's missing.

Format your response as:
THOUGHT PROCESS:
- Step 1: [Initial analysis of retrieved chunks]
- Step 2: [Connections between chunks]
- Step 3: [Reasoning based on chunks]

FINAL ANSWER:
[Your concise answer based on the retrieved context]`,
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o-mini',
  },
  tools: { contextTool },
})
```

## Instantiate PgVector and Mastra

Instantiate PgVector and Mastra with all components:

```typescript copy showLineNumbers{41} filename="src/mastra/index.ts"
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);

export const mastra = new Mastra({
  agents: { ragAgent },
  vectors: { pgVector },
})
const agent = mastra.getAgent('ragAgent')
```

## Document Processing

Create a document and process it into chunks:

```typescript copy showLineNumbers{49} filename="src/mastra/index.ts"
const doc = MDocument.fromText(`The Impact of Climate Change on Global Agriculture...`)

const chunks = await doc.chunk({
  strategy: 'recursive',
  size: 512,
  overlap: 50,
  separator: '\n',
})
```

## Creating and Storing Embeddings

Generate embeddings for the chunks and store them in the vector database:

```typescript copy showLineNumbers{58} filename="src/mastra/index.ts"
const { embeddings } = (await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
})) as EmbedManyResult<string>

const vectorStore = mastra.getVector('pgVector');
await vectorStore.createIndex("embeddings", 1536)
await vectorStore.upsert(
  "embeddings",
  embeddings,
  chunks?.map((chunk: any) => ({ text: chunk.text }))
)
```

## Response Generation with Chain-of-Thought

Function to generate responses using chain-of-thought reasoning:

```typescript copy showLineNumbers{72} filename="src/mastra/index.ts"
async function generateResponse(query: string) {
  const prompt = `
    Please answer the following question using chain-of-thought reasoning:
    ${query}

    Please base your answer only on the context provided in the tool. If the context doesn't 
    contain enough information to fully answer the question, please state that explicitly.
    Remember: Explain how you're using the retrieved information to reach your conclusions.
    `

  const completion = await agent.generate(prompt)
  return completion.text
}
```

## Example Usage

```typescript copy showLineNumbers{86} filename="src/mastra/index.ts"
async function answerQueries(queries: string[]) {
  for (const query of queries) {
    try {
      const answer = await generateResponse(query)
      console.log('\nQuery:', query)
      console.log('\nReasoning Chain + Retrieved Context Response:')
      console.log(answer)
      console.log('\n-------------------')
    } catch (error) {
      console.error(`Error processing query "${query}":`, error)
    }
  }
}

const queries = [
  "What are the main adaptation strategies for farmers?",
  "Analyze how temperature affects crop yields.",
  "What connections can you draw between climate change and food security?",
  "How are farmers implementing solutions to address climate challenges?",
  "What future implications are discussed for agriculture?",
];

await answerQueries(queries)
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/cot-rag'} />

================================================================================
Source: src/pages/examples/rag/cot-workflow-rag.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Chain of Thought Workflow RAG

This example demonstrates how to implement a Retrieval-Augmented Generation (RAG) system using Mastra, OpenAI embeddings, and PGVector for vector storage, with an emphasis on chain-of-thought reasoning using a step-by-step workflow.

## Overview

The system implements RAG using Mastra and OpenAI with chain-of-thought prompting through a defined workflow. Here's what it does:

1. Sets up a Mastra agent with GPT-4o-mini for response generation
2. Creates a vector query tool to manage vector store interactions
3. Defines a workflow with multiple steps for chain-of-thought reasoning
4. Processes and chunks text documents
5. Creates and stores embeddings in PostgreSQL
6. Generates responses through the workflow steps

## Setup

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
POSTGRES_CONNECTION_STRING=your_connection_string_here
```

### Dependencies

Import the necessary dependencies:

```typescript copy showLineNumbers filename="src/mastra/index.ts"
import { Mastra, Agent, EmbedManyResult, Step, Workflow } from "@mastra/core";
import { createVectorQueryTool, embed, MDocument, PgVector } from "@mastra/rag";
import { z } from "zod";
```

## Workflow Definition

First, define the workflow with its trigger schema:

```typescript copy showLineNumbers{5} filename="src/mastra/index.ts"
export const ragWorkflow = new Workflow({
  name: "rag-workflow",
  triggerSchema: z.object({
    query: z.string(),
  }),
});
```

## Vector Query Tool Creation

Create a tool for querying the vector database:

```typescript copy showLineNumbers{12} filename="src/mastra/index.ts"
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: "pgVector",
  indexName: "embeddings",
  options: {
    provider: "OPEN_AI",
    model: "text-embedding-ada-002",
    maxRetries: 3,
  },
  topK: 3,
});
```

## Agent Configuration

Set up the Mastra agent:

```typescript copy showLineNumbers{23} filename="src/mastra/index.ts"
export const ragAgent = new Agent({
  name: "RAG Agent",
  instructions: `You are a helpful assistant that answers questions based on the provided context.`,
  model: {
    provider: "OPEN_AI",
    name: "gpt-4o-mini",
  },
  tools: {
    vectorQueryTool,
  },
});
```

## Workflow Steps

The workflow is divided into multiple steps for chain-of-thought reasoning:

### 1. Context Analysis Step

```typescript copy showLineNumbers{35} filename="src/mastra/index.ts"
const analyzeContext = new Step({
  id: "analyzeContext",
  outputSchema: z.object({
    initialAnalysis: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    console.log("---------------------------");
    const ragAgent = mastra?.agents?.ragAgent;
    const query = context?.machineContext?.getStepPayload<{ query: string }>(
      "trigger",
    )?.query;

    const analysisPrompt = `${query} 1. First, carefully analyze the retrieved context chunks and identify key information.`;

    const analysis = await ragAgent?.generate(analysisPrompt);
    console.log(analysis?.text);
    return {
      initialAnalysis: analysis?.text ?? "",
    };
  },
});
```

### 2. Thought Breakdown Step

```typescript copy showLineNumbers{55} filename="src/mastra/index.ts"
const breakdownThoughts = new Step({
  id: "breakdownThoughts",
  outputSchema: z.object({
    breakdown: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    console.log("---------------------------");
    const ragAgent = mastra?.agents?.ragAgent;
    const analysis = context?.machineContext?.getStepPayload<{
      initialAnalysis: string;
    }>("analyzeContext")?.initialAnalysis;

    const connectionPrompt = `
      Based on the initial analysis: ${analysis}

      2. Break down your thinking process about how the retrieved information relates to the query.
    `;

    const connectionAnalysis = await ragAgent?.generate(connectionPrompt);
    console.log(connectionAnalysis?.text);
    return {
      breakdown: connectionAnalysis?.text ?? "",
    };
  },
});
```

### 3. Connection Step

```typescript copy showLineNumbers{78} filename="src/mastra/index.ts"
const connectPieces = new Step({
  id: "connectPieces",
  outputSchema: z.object({
    connections: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    console.log("---------------------------");
    const ragAgent = mastra?.agents?.ragAgent;
    const process = context?.machineContext?.getStepPayload<{
      breakdown: string;
    }>("breakdownThoughts")?.breakdown;
    const connectionPrompt = `
        Based on the breakdown: ${process}

        3. Explain how you're connecting different pieces from the retrieved chunks.
    `;

    const connections = await ragAgent?.generate(connectionPrompt);
    console.log(connections?.text);
    return {
      connections: connections?.text ?? "",
    };
  },
});
```

### 4. Conclusion Step

```typescript copy showLineNumbers{101} filename="src/mastra/index.ts"
const drawConclusions = new Step({
  id: "drawConclusions",
  outputSchema: z.object({
    conclusions: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    console.log("---------------------------");
    const ragAgent = mastra?.agents?.ragAgent;
    const evidence = context?.machineContext?.getStepPayload<{
      connections: string;
    }>("connectPieces")?.connections;
    const conclusionPrompt = `
        Based on the connections: ${evidence}

        4. Draw conclusions based only on the evidence in the retrieved context.
    `;

    const conclusions = await ragAgent?.generate(conclusionPrompt);
    console.log(conclusions?.text);
    return {
      conclusions: conclusions?.text ?? "",
    };
  },
});
```

### 5. Final Answer Step

```typescript copy showLineNumbers{124} filename="src/mastra/index.ts"
const finalAnswer = new Step({
  id: "finalAnswer",
  outputSchema: z.object({
    finalAnswer: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    console.log("---------------------------");
    const ragAgent = mastra?.agents?.ragAgent;
    const conclusions = context?.machineContext?.getStepPayload<{
      conclusions: string;
    }>("drawConclusions")?.conclusions;
    const answerPrompt = `
        Based on the conclusions: ${conclusions}
        Format your response as:
        THOUGHT PROCESS:
        - Step 1: [Initial analysis of retrieved chunks]
        - Step 2: [Connections between chunks]
        - Step 3: [Reasoning based on chunks]

        FINAL ANSWER:
        [Your concise answer based on the retrieved context]`;

    const finalAnswer = await ragAgent?.generate(answerPrompt);
    console.log(finalAnswer?.text);
    return {
      finalAnswer: finalAnswer?.text ?? "",
    };
  },
});
```

## Workflow Configuration

Connect all the steps in the workflow:

```typescript copy showLineNumbers{154} filename="src/mastra/index.ts"
ragWorkflow
  .step(analyzeContext)
  .then(breakdownThoughts)
  .then(connectPieces)
  .then(drawConclusions)
  .then(finalAnswer);

ragWorkflow.commit();
```

## Instantiate PgVector and Mastra

Instantiate PgVector and Mastra with all components:

```typescript copy showLineNumbers{198} filename="src/mastra/index.ts"
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);

export const mastra = new Mastra({
  agents: { ragAgent },
  vectors: { pgVector },
  workflows: { ragWorkflow },
});
```

## Document Processing

Process and chunks the document:

```typescript copy showLineNumbers{206} filename="src/mastra/index.ts"
const doc = MDocument.fromText(`Your document text here...`);

const chunks = await doc.chunk({
  strategy: "recursive",
  size: 512,
  overlap: 50,
  separator: "\n",
});
```

## Embedding Creation and Storage

Generate and store embeddings:

```typescript copy showLineNumbers{215} filename="src/mastra/index.ts"
const { embeddings } = (await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
})) as EmbedManyResult<string>;

const vectorStore = mastra.getVector("pgVector");
await vectorStore.createIndex("embeddings", 1536);
await vectorStore.upsert(
  "embeddings",
  embeddings,
  chunks?.map((chunk: any) => ({ text: chunk.text })),
);
```

## Response Generation

Function to generate responses using the workflow:

```typescript copy showLineNumbers{229} filename="src/mastra/index.ts"
async function generateResponse(query: string) {
  const prompt = `
    Please answer the following question:
    ${query}

    Please base your answer only on the context provided in the tool. If the context doesn't contain enough information to fully answer the question, please state that explicitly.
    `;

  const { runId, start } = ragWorkflow.createRun();

  const workflowResult = await start({
    triggerData: {
      query: prompt,
    },
  });

  return workflowResult;
}
```

## Example Usage

```typescript copy showLineNumbers{246} filename="src/mastra/index.ts"
const query = "What are the main benefits of telemedicine?";
console.log("\nQuery:", query);
const result = await generateResponse(query);
console.log("\nThought Process:");
console.log(result.results);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/cot-workflow-rag"
  }
/>


================================================================================
Source: src/pages/examples/rag/embed-chunk-array.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Embed Chunk Array

After chunking documents, you need to convert the text chunks into numerical vectors that can be used for similarity search. The `embed` method transforms text chunks into embeddings using your chosen provider and model. This example shows how to generate embeddings for an array of text chunks.

```tsx copy
import { MDocument, embed } from "@mastra/rag";

const doc = MDocument.fromText("Your text content...");

const chunks = await doc.chunk();

const { embeddings } = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
});
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/embed-chunk-array"
  }
/>


================================================================================
Source: src/pages/examples/rag/embed-text-chunk.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Embed Text Chunk

When working with individual text chunks, you need to convert them into numerical vectors for similarity search. The `embed` method transforms a single text chunk into an embedding using your chosen provider and model.

```tsx copy
import { MDocument, embed } from "@mastra/rag";

const doc = MDocument.fromText("Your text content...");

const chunks = await doc.chunk();

const { embedding } = await embed(chunks[0], {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
});
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/embed-text-chunk"
  }
/>


================================================================================
Source: src/pages/examples/rag/embed-text-with-cohere.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Embed Text with Cohere

When working with alternative embedding providers, you need a way to generate vectors that match your chosen model's specifications. The `embed` method supports multiple providers, allowing you to switch between different embedding services. This example shows how to generate embeddings using Cohere's embedding model.

```tsx copy
import { MDocument, embed } from "@mastra/rag";

const doc = MDocument.fromText("Your text content...");

const chunks = await doc.chunk();

const { embeddings } = await embed(chunks, {
  provider: "COHERE",
  model: "embed-english-v3.0",
  maxRetries: 3,
});
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/embed-text-with-cohere"
  }
/>


================================================================================
Source: src/pages/examples/rag/filter-rag.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Metadata Filtering

This example demonstrates how to implement a Retrieval-Augmented Generation (RAG) system using Mastra, OpenAI embeddings, and PGVector for vector storage.
This system uses metadata filters to search for relevant chunks in the vector store, reducing the amount of results returned.

## Overview

The system implements RAG using Mastra and OpenAI. Here's what it does:
1. Sets up a Mastra agent with GPT-4o-mini for response generation
2. Creates a vector query tool to manage vector store interactions
3. Chunks text documents into smaller segments
4. Creates embeddings for these chunks
5. Stores them in a PostgreSQL vector database
6. Retrieves relevant chunks based on queries using vector query tool
7. Generates context-aware responses using the Mastra agent

## Setup 

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
POSTGRES_CONNECTION_STRING=your_connection_string_here
```

### Dependencies

Then, import the necessary dependencies:

```typescript copy showLineNumbers filename="src/mastra/index.ts"
import { Mastra, Agent, EmbedResult, EmbedManyResult } from '@mastra/core'
import { createVectorQueryTool, embed, MDocument, PgVector } from '@mastra/rag';
```

## Vector Query Tool Creation
Using createVectorQueryTool imported from @mastra/rag, you can create a tool that can query the vector database.
```typescript copy showLineNumbers{4} filename="src/mastra/index.ts"
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'embeddings',
  options: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002',
    maxRetries: 3,
  },
  topK: 3,
  vectorFilterType: 'pg',
});
```

## Agent Configuration

Set up the Mastra agent that will handle the responses:

```typescript copy showLineNumbers{16} filename="src/mastra/index.ts"
export const ragAgent = new Agent({
  name: 'RAG Agent',
  instructions:
    'You are a helpful assistant that answers questions based on the provided context. Keep your answers concise and relevant.',
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o-mini',
  },
  tools: { vectorQueryTool },
});
```

## Instantiate PgVector and Mastra

Instantiate PgVector and Mastra with all components:

```typescript copy showLineNumbers{27} filename="src/mastra/index.ts"
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);

export const mastra = new Mastra({
  agents: { ragAgent },
  vectors: { pgVector },
})

const agent = mastra.getAgent('ragAgent')
```

## Document Processing

Create a document and process it into chunks:

```typescript copy showLineNumbers{36} filename="src/mastra/index.ts"
const doc = MDocument.fromText(`The Impact of Climate Change on Global Agriculture...`)

const chunks = await doc.chunk({
  strategy: 'recursive',
  size: 512,
  overlap: 50,
  separator: '\n',
  extract: {
    keywords: true,
  },
})
```

## Creating and Storing Embeddings

Generate embeddings for the chunks and store them in the vector database:

```typescript copy showLineNumbers{48} filename="src/mastra/index.ts"
const { embeddings } = (await embed(chunks, {
  provider: 'OPEN_AI',
  model: 'text-embedding-ada-002',
  maxRetries: 3,
})) as EmbedManyResult<string>;

const vectorStore = mastra.getVector('pgVector');
await vectorStore.createIndex('embeddings', 1536);
await vectorStore.upsert(
  'embeddings',
  embeddings,
  chunks?.map((chunk: any) => ({
    text: chunk.text,
    ...chunk.metadata,
  })),
);
```

## Response Generation

Function to generate responses based on retrieved context:

```typescript copy showLineNumbers{65} filename="src/mastra/index.ts"
async function generateResponse(
  query: string,
  filter: {
    keyword: string;
    operator: string;
    value: string;
  },
) {
  const { keyword, operator, value } = filter;
  const prompt = `
      Please answer the following question:
      ${query}

      Please base your answer only on the context provided in the tool using this keyword: ${keyword}, this operator: ${operator}, and this value: ${value}.
      If the context doesn't contain enough information to fully answer the question, please state that explicitly.
      `;

  // Call the agent to generate a response
  const completion = await agent.generate(prompt);

  return completion.text;
}
```

## Example Usage

```typescript copy showLineNumbers{87} filename="src/mastra/index.ts"
async function answerQueries(
  queries: {
    query: string;
    filter: {
      keyword: string;
      operator: string;
      value: string;
    };
  }[],
) {
  for (const { query, filter } of queries) {
    try {
      // Generate and log the response
      const answer = await generateResponse(query, filter);
      console.log('\nQuery:', query);
      console.log('Response:', answer);
    } catch (error) {
      console.error(`Error processing query "${query}":`, error);
    }
  }
}

const queries = [
  {
    query: 'What adaptation strategies are mentioned?',
    filter: {
      keyword: 'excerptKeywords',
      operator: 'ilike',
      value: `%adaptation%`,
    },
  },
  {
    query: 'How do temperatures affect crop yields specifically?',
    filter: { keyword: 'excerptKeywords', operator: 'ilike', value: `%crop%` },
  },
  {
    query: 'What are the future challenges?',
    filter: {
      keyword: 'excerptKeywords',
      operator: 'ilike',
      value: `%technologies%`,
    },
  },
];

await answerQueries(queries);
```


<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/filter-rag'} />

================================================================================
Source: src/pages/examples/rag/graph-rag.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Graph RAG

This example demonstrates how to implement a Retrieval-Augmented Generation (RAG) system using Mastra, OpenAI embeddings, and PGVector for vector storage.

## Overview

The system implements Graph RAG using Mastra and OpenAI. Here's what it does:
1. Sets up a Mastra agent with GPT-4o-mini for response generation
2. Creates a GraphRAG tool to manage vector store interactions and knowledge graph creation/traversal
3. Chunks text documents into smaller segments
4. Creates embeddings for these chunks
5. Stores them in a PostgreSQL vector database
6. Creates a knowledge graph of relevant chunks based on queries using GraphRAG tool
    - Tool returns results from vector store and creates knowledge graph 
    - Traverses knowledge graph using query
7. Generates context-aware responses using the Mastra agent

## Setup 

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
POSTGRES_CONNECTION_STRING=your_connection_string_here
```

### Dependencies

Then, import the necessary dependencies:

```typescript copy showLineNumbers filename="src/mastra/index.ts"
import { Mastra, Agent, EmbedManyResult } from '@mastra/core';
import { embed, MDocument, PgVector, createGraphRAGTool } from '@mastra/rag';
```

## GraphRAG Tool Creation
Using createGraphRAGTool imported from @mastra/rag, you can create a tool that queries the vector database and converts the results into a knowledge graph.
```typescript copy showLineNumbers{4} filename="src/mastra/index.ts"
const graphRagTool = createGraphRAGTool({
  vectorStoreName: 'pgVector',
  indexName: 'embeddings',
  options: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002',
    maxRetries: 3,
  },
  graphOptions: {
    dimension: 1536,
    threshold: 0.7,
  },
  topK: 5,
});
```

## Agent Configuration

Set up the Mastra agent that will handle the responses:

```typescript copy showLineNumbers{19} filename="src/mastra/index.ts"
export const ragAgent = new Agent({
  name: 'GraphRAG Agent',
  instructions: `You are a helpful assistant that answers questions based on the provided context. Format your answers as follows:

1. DIRECT FACTS: List only the directly stated facts from the text relevant to the question (2-3 bullet points)
2. CONNECTIONS MADE: List the relationships you found between different parts of the text (2-3 bullet points)
3. CONCLUSION: One sentence summary that ties everything together

Keep each section brief and focus on the most important points.`,
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o-mini',
  },
  tools: {
    graphRagTool,
  },
});
```

## Instantiate PgVector and Mastra

Instantiate PgVector and Mastra with all components:

```typescript copy showLineNumbers{37} filename="src/mastra/index.ts"
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);

export const mastra = new Mastra({
  agents: { ragAgent },
  vectors: { pgVector },
})

const agent = mastra.getAgent('ragAgent')
```

## Document Processing

Create a document and process it into chunks:

```typescript copy showLineNumbers{46} filename="src/mastra/index.ts"
const doc = MDocument.fromText(`Riverdale Heights: Community Development Study...`)

const chunks = await doc.chunk({
  strategy: 'recursive',
  size: 512,
  overlap: 50,
  separator: '\n',
})
```

## Creating and Storing Embeddings

Generate embeddings for the chunks and store them in the vector database:

```typescript copy showLineNumbers{55} filename="src/mastra/index.ts"
const { embeddings } = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
}) as EmbedManyResult<string>

const vectorStore = mastra.getVector('pgVector');
await vectorStore.createIndex("embeddings", 1536)
await vectorStore.upsert(
  "embeddings",
  embeddings,
  chunks?.map((chunk: any) => ({ text: chunk.text }))
)
```

## Response Generation

Function to generate responses based on retrieved context:

```typescript copy showLineNumbers{69} filename="src/mastra/index.ts"
async function generateResponse(query: string) {
  const prompt = `
    Please answer the following question using both semantic and graph-based context:
    ${query}

    Please base your answer only on the context provided in the tool. If the context doesn't contain enough information to fully answer the question, please state that explicitly.
    `;

  const completion = await agent.generate(prompt)
  return completion.text
}
```

## Example Usage

```typescript copy showLineNumbers{81} filename="src/mastra/index.ts"
async function answerQueries(queries: string[]) {
  for (const query of queries) {
    try {
      const answer = await generateResponse(query)
      console.log('\nQuery:', query)
      console.log('Response:', answer)
    } catch (error) {
      console.error(`Error processing query "${query}":`, error)
    }
  }
}

const queries = [
  "What are the direct and indirect effects of early railway decisions on Riverdale Heights' current state?",

  'How have changes in transportation infrastructure affected different generations of local businesses and community spaces?',

  'Compare how the Rossi family business and Thompson Steel Works responded to major infrastructure changes, and how their responses affected the community.',

  'Trace how the transformation of the Thompson Steel Works site has influenced surrounding businesses and cultural spaces from 1932 to present.',
];

await answerQueries(queries)
```


<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/basic-rag'} />

================================================================================
Source: src/pages/examples/rag/insert-embedding-in-pgvector.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Insert Embedding in PgVector

After generating embeddings, you need to store them in a database that supports vector similarity search. The `PgVector` class provides methods to create indexes and insert embeddings into PostgreSQL with the pgvector extension. This example shows how to store embeddings in a PostgreSQL database for later retrieval.

```tsx copy
import { MDocument, embed, PgVector } from "@mastra/rag";

const doc = MDocument.fromText("Your text content...");

const chunks = await doc.chunk();

const { embeddings } = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
});

const pgVector = new PgVector("postgresql://localhost:5432/mydb");

await pgVector.createIndex("test_index", 1536);

await pgVector.upsert(
  "test_index",
  embeddings,
  chunks?.map((chunk: any) => ({ text: chunk.text })),
);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/insert-embedding-in-pgvector"
  }
/>


================================================================================
Source: src/pages/examples/rag/insert-embedding-in-pinecone.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Insert Embedding in Pinecone

After generating embeddings, you need to store them in a vector database for similarity search. The `PineconeVector` class provides methods to create indexes and insert embeddings into Pinecone, a managed vector database service. This example shows how to store embeddings in Pinecone for later retrieval.

```tsx copy
import { MDocument, embed, PineconeVector } from '@mastra/rag';

const doc = MDocument.fromText('Your text content...');

const chunks = await doc.chunk();

const { embeddings } = await embed(chunks, {
  provider: 'OPEN_AI',
  model: 'text-embedding-ada-002',
  maxRetries: 3,
});

const pinecone = new PineconeVector('your-api-key');

await pinecone.createIndex('test_index', 1536);

await pinecone.upsert(
  'test_index',
  embeddings,
  chunks?.map(chunk => ({ text: chunk.text })),
);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/insert-embedding-in-pinecone'} />


================================================================================
Source: src/pages/examples/rag/rerank-rag.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Re-ranking Results

This example demonstrates how to implement a Retrieval-Augmented Generation (RAG) system with re-ranking using Mastra, OpenAI embeddings, and PGVector for vector storage.

## Overview

The system implements RAG with re-ranking using Mastra and OpenAI. Here's what it does:
1. Sets up a Mastra agent with GPT-4o-mini for response generation
2. Creates a vector query tool with re-ranking capabilities
3. Chunks text documents into smaller segments
4. Creates embeddings for these chunks
5. Stores them in a PostgreSQL vector database
6. Retrieves and re-ranks relevant chunks based on queries
7. Generates context-aware responses using the Mastra agent

## Setup 

### Environment Setup

Make sure to set up your environment variables:

```bash filename=".env"
POSTGRES_CONNECTION_STRING=your_connection_string_here
```

### Dependencies

Then, import the necessary dependencies:

```typescript copy showLineNumbers filename="src/mastra/index.ts"
import { Mastra, Agent, EmbedManyResult } from '@mastra/core';
import { embed, MDocument, PgVector, createVectorQueryTool } from '@mastra/rag';
```

## Vector Query Tool Creation with Re-ranking

Using createVectorQueryTool imported from @mastra/rag, you can create a tool that can query the vector database and re-rank results:

```typescript copy showLineNumbers{4} filename="src/mastra/index.ts"
const vectorQueryTool = createVectorQueryTool({
  vectorStoreName: 'pgVector',
  indexName: 'embeddings',
  options: {
    provider: 'OPEN_AI',
    model: 'text-embedding-ada-002',
    maxRetries: 3,
  },
  topK: 5,
  rerankOptions: {
    semanticProvider: 'agent',
    agentProvider: {
      provider: 'OPEN_AI',
      name: 'gpt-4o-mini',
    },
  },
});
```

## Agent Configuration

Set up the Mastra agent that will handle the responses:

```typescript copy showLineNumbers{22} filename="src/mastra/index.ts"
export const ragAgent = new Agent({
  name: 'RAG Agent',
  instructions:
    'You are a helpful assistant that answers questions based on the provided context. Keep your answers concise and relevant.',
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o-mini',
  },
  tools: {
    vectorQueryTool,
  },
})
```

## Instantiate PgVector and Mastra

Instantiate PgVector and Mastra with all components:

```typescript copy showLineNumbers{35} filename="src/mastra/index.ts"
const pgVector = new PgVector(process.env.POSTGRES_CONNECTION_STRING!);

export const mastra = new Mastra({
  agents: { ragAgent },
  vectors: { pgVector },
})

const agent = mastra.getAgent('ragAgent')
```

## Document Processing

Create a document and process it into chunks:

```typescript copy showLineNumbers{44} filename="src/mastra/index.ts"
const doc1 = MDocument.fromText(`
market data shows price resistance levels.
technical charts display moving averages.
support levels guide trading decisions.
breakout patterns signal entry points.
price action determines trade timing.

baseball cards show gradual value increase.
rookie cards command premium prices.
card condition affects resale value.
authentication prevents fake trading.
grading services verify card quality.

volume analysis confirms price trends.
sports cards track seasonal demand.
chart patterns predict movements.
mint condition doubles card worth.
resistance breaks trigger orders.
rare cards appreciate yearly.
`)

const chunks = await doc1.chunk({
  strategy: 'recursive',
  size: 150,
  overlap: 20,
  separator: '\n',
})
```

## Creating and Storing Embeddings

Generate embeddings for the chunks and store them in the vector database:

```typescript copy showLineNumbers{72} filename="src/mastra/index.ts"
const { embeddings } = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
}) as EmbedManyResult<string>

const vectorStore = mastra.getVector('pgVector');
await vectorStore.createIndex("embeddings", 1536)
await vectorStore.upsert(
  "embeddings",
  embeddings,
  chunks?.map((chunk: any) => ({ text: chunk.text }))
)
```

## Response Generation

Function to generate responses based on retrieved and re-ranked context:

```typescript copy showLineNumbers{86} filename="src/mastra/index.ts"
async function generateResponse(query: string) {
  const prompt = `
      Please answer the following question:
      ${query}

      Please base your answer only on the context provided in the tool. If the context doesn't 
      contain enough information to fully answer the question, please state that explicitly.
      `

  const completion = await agent.generate(prompt)
  return completion.text
}
```

## Example Usage

```typescript copy showLineNumbers{99} filename="src/mastra/index.ts"
async function answerQueries(queries: string[]) {
  for (const query of queries) {
    try {
      const answer = await generateResponse(query)
      console.log('\nQuery:', query)
      console.log('Response:', answer)
    } catch (error) {
      console.error(`Error processing query "${query}":`, error)
    }
  }
}

const queries = [
  "explain technical trading analysis",
  "explain trading card valuation",
  "how do you analyze market resistance",
];

await answerQueries(queries)
```

================================================================================
Source: src/pages/examples/rag/reranking-with-cohere.mdx
================================================================================

# Reranking with Cohere

When retrieving documents for RAG, initial vector similarity search may miss important semantic matches. 

Cohere's reranking service helps improve result relevance by reordering documents using multiple scoring factors.

```typescript 
const reranker = new Reranker({
  semanticProvider: "cohere",
  cohereApiKey: "your-api-key",
  weights: {
    semantic: 0.4,
    vector: 0.4,
    position: 0.2
  }
});

const results = await reranker.rerank({
  query: "deployment configuration",
  vectorStoreResults: searchResults,
  topK: 5
});
```

## Links

- [Reranker() reference](../reference/rag/reranker.mdx)
- [Retrieval docs](../docs/rag/retrieval.mdx)


================================================================================
Source: src/pages/examples/rag/retrieve-results.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Retrieve Results

After storing embeddings in a vector database, you need to query them to find similar content. The `query` method returns the most semantically similar chunks to your input embedding, ranked by relevance. This example shows how to retrieve similar chunks from a Pinecone vector database.

```tsx copy
import { MDocument, embed, PineconeVector } from "@mastra/rag";

const doc = MDocument.fromText("Your text content...");

const chunks = await doc.chunk();

const { embeddings } = await embed(chunks, {
  provider: "OPEN_AI",
  model: "text-embedding-ada-002",
  maxRetries: 3,
});

const pinecone = new PineconeVector("your-api-key");

await pinecone.createIndex("test_index", 1536);

await pinecone.upsert(
  "test_index",
  embeddings,
  chunks?.map((chunk: any) => ({ text: chunk.text })),
);

const results = await pinecone.query("test_index", embeddings[0], 10);

console.log(results);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/rag/retrieve-results"
  }
/>


================================================================================
Source: src/pages/examples/workflows/branching-paths.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Branching Paths

When processing data, you often need to take different actions based on intermediate results. This example shows how to create a workflow that splits into separate paths, where each path executes different steps based on the output of a previous step.

## Control Flow Diagram

This example shows how to create a workflow that splits into separate paths, where each path executes different steps based on the output of a previous step.

Here's the control flow diagram:

<img
  src="/subscribed-chains.png"
  alt="Diagram showing workflow with branching paths"
/>

## Creating the Steps

Let's start by creating the steps and initializing the workflow.

{/* prettier-ignore */}
```ts showLineNumbers copy
import { Step, Workflow } from "@mastra/core";
import { z } from "zod"

const stepOne = new Step({
  id: "stepOne",
  execute: async ({ context: { machineContext } }) => ({ 
    doubledValue: machineContext.triggerData.inputValue * 2
  })
});

const stepTwo = new Step({
  id: "stepTwo",
  execute: async ({ context: { machineContext } }) => ({ 
    isDivisibleByFive: machineContext.stepResults.stepOne.payload.doubledValue % 5 === 0
  })
});


const stepThree = new Step({
  id: "stepThree",
  execute: async ({ context: { machineContext } }) => ({ 
    incrementedValue: machineContext.stepResults.stepOne.payload.doubledValue + 1
  })
});

const stepFour = new Step({
  id: "stepFour",
  execute: async ({ context: { machineContext } }) => ({ 
    isDivisibleByThree: machineContext.stepResults.stepThree.payload.incrementedValue % 3 === 0
  })
});

// Build the workflow
const myWorkflow = new Workflow({
  name: "my-workflow",
  triggerSchema: z.object({
    inputValue: z.number(),
  }),
});
```

## Branching Paths and Chaining Steps

Now let's execute the workflow and see the results.

```ts showLineNumbers copy
myWorkflow
  .step(stepOne)
  .then(stepTwo)
  .after(stepOne)
  .step(stepThree)
  .then(stepFour)
  .commit();

const result = await myWorkflow.execute({ triggerData: { inputValue: 3 } });
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/workflow-with-branching-paths"
  }
/>


================================================================================
Source: src/pages/examples/workflows/calling-agent.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Calling an Agent From a Workflow

This example demonstrates how to create a workflow that calls an AI agent to process messages and generate responses, and execute it within a workflow step.

```ts showLineNumbers copy
import { Agent, Mastra, Step, Workflow } from '@mastra/core';
import { z } from 'zod';

const penguin = new Agent({
  name: 'agent skipper',
  instructions: `You are skipper from penguin of madagascar, reply as that`,
  model: {
    provider: 'OPEN_AI',
    name: 'gpt-4o',
  },
});

const newWorkflow = new Workflow({
  name: 'pass message to the workflow',
  triggerSchema: z.object({
    message: z.string(),
  }),
});

const replyAsSkipper = new Step({
  id: 'reply',
  outputSchema: z.object({
    reply: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    const kowalski = mastra?.agents?.penguin;

    const res = await kowalski?.generate(context.machineContext?.triggerData?.message);
    return { reply: res?.text || '' };
  },
});

newWorkflow.step(replyAsSkipper);
newWorkflow.commit();

const mastra = new Mastra({
  agents: { penguin },
  workflows: { newWorkflow },
});

const { runId, start } = await mastra
  .getWorkflow('newWorkflow')
  .createRun()

const runResult = await start({ triggerData: { message: 'Give me a run down of the mission to save private' } });

console.log(runResult.results);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/calling-agent-from-workflow'}
/>


================================================================================
Source: src/pages/examples/workflows/calling-llm.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Calling an LLM from a Workflow

This example demonstrates how to create a workflow that streams responses from an LLM, showing both real-time output and final text handling.

```ts showLineNumbers copy
import { Mastra, Step, Workflow } from '@mastra/core';
import { z } from 'zod';

const newWorkflow = new Workflow({
  name: 'pass message to the workflow',
  triggerSchema: z.object({
    message: z.string(),
  }),
});

const replyAsPenguin = new Step({
  id: 'reply',
  outputSchema: z.object({
    reply: z.string(),
  }),
  execute: async ({ context, mastra }) => {
    const penguinCharacter = mastra?.llm?.({
      provider: 'OPEN_AI',
      name: 'gpt-4o',
    });

    const res = await penguinCharacter?.stream(context.machineContext?.triggerData?.message);

    if (!res) {
      return { reply: '' };
    }

    for await (const chunk of res?.textStream) {
      process.stdout.write(chunk);
    }

    const text = await res.text;
    return { reply: text };
  },
});

newWorkflow.step(replyAsPenguin);
newWorkflow.commit();

const mastra = new Mastra({
  workflows: { newWorkflow },
});

const { runId = start } = mastra
  .getWorkflow('newWorkflow')
  .createRun();

await start({ triggerData: { message: 'Give me a speech as skipper from penguin of madagascar' } });
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/calling-llm-from-workflow'}
/>


================================================================================
Source: src/pages/examples/workflows/creating-a-workflow.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Creating a Workflow

A workflow allows you to define and execute sequences of operations in a structured path. This example
shows a workflow with a single step.

```ts showLineNumbers copy
import { Step, Workflow } from '@mastra/core';
import { z } from 'zod';

const myWorkflow = new Workflow({
  name: 'my-workflow',
  triggerSchema: z.object({
    input: z.number(),
  }),
});

const stepOne = new Step({
  id: 'stepOne',
  inputSchema: z.object({
    value: z.number(),
  }),
  outputSchema: z.object({
    doubledValue: z.number(),
  }),
  execute: async ({ context }) => {
    const doubledValue = context.machineContext?.triggerData?.input * 2;
    return { doubledValue };
  },
})

myWorkflow.step(
  stepOne
).commit();

const { runId, start } = myWorkflow.createRun({ 
  triggerData: { input: 90 } 
});

const res = await start();

console.log(res.results);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/create-workflow'} />


================================================================================
Source: src/pages/examples/workflows/cyclical-dependencies.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Workflow with Cyclical dependencies

Workflows support cyclical dependencies where steps can loop back based on conditions. The example below shows how to use conditional logic to create loops and handle repeated execution.

```ts showLineNumbers copy
import { Workflow, Step } from '@mastra/core';
import { z } from 'zod';

async function main() {
  const doubleValue = new Step({
    id: 'doubleValue',
    description: 'Doubles the input value',
    inputSchema: z.object({
      inputValue: z.number(),
    }),
    outputSchema: z.object({
      doubledValue: z.number(),
    }),
    execute: async ({ context }) => {
      const doubledValue = context.inputValue * 2;
      return { doubledValue };
    },
  });

  const incrementByOne = new Step({
    id: 'incrementByOne',
    description: 'Adds 1 to the input value',
    outputSchema: z.object({
      incrementedValue: z.number(),
    }),
    execute: async ({ context }) => {
      const valueToIncrement = context?.machineContext?.getStepPayload<{ firstValue: number }>('trigger')?.firstValue;
      if (!valueToIncrement) throw new Error('No value to increment provided');
      const incrementedValue = valueToIncrement + 1;
      return { incrementedValue };
    },
  });

  const cyclicalWorkflow = new Workflow({
    name: 'cyclical-workflow',
    triggerSchema: z.object({
      firstValue: z.number(),
    }),
  });

  cyclicalWorkflow
    .step(doubleValue, {
      variables: {
        inputValue: {
          step: 'trigger',
          path: 'firstValue',
        },
      },
    })
    .then(incrementByOne)
    .after(doubleValue)
    .step(doubleValue, {
      when: {
        ref: { step: doubleValue, path: 'doubledValue' },
        query: { $eq: 12 },
      },
      variables: {
        inputValue: {
          step: doubleValue,
          path: 'doubledValue',
        },
      },
    })
    .commit();

  const { runId, start } = cyclicalWorkflow.createRun();

  console.log('Run', runId);

  const res = await start({ triggerData: { firstValue: 6 } });

  console.log(res.results);
}

main();
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/workflow-with-cyclical-deps"
  }
/>


================================================================================
Source: src/pages/examples/workflows/parallel-steps.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Parallel Steps

When building AI applications, you often need to process multiple independent tasks simultaneously to improve efficiency.

## Control Flow Diagram

This example shows how to structure a workflow that executes steps in parallel, with each branch handling its own data flow and dependencies.

Here's the control flow diagram:

<img
  src="/parallel-chains.png"
  alt="Diagram showing workflow with parallel steps"
  width={600}
/>

## Creating the Steps

Let's start by creating the steps and initializing the workflow.

```ts showLineNumbers copy
import { Step, Workflow } from "@mastra/core";
import { z } from "zod";

const stepOne = new Step({
  id: "stepOne",
  execute: async ({ context: { machineContext } }) => ({
    doubledValue: machineContext.triggerData.inputValue * 2,
  }),
});

const stepTwo = new Step({
  id: "stepTwo",
  execute: async ({ context: { machineContext } }) => ({
    incrementedValue:
      machineContext.stepResults.stepOne.payload.doubledValue + 1,
  }),
});

const stepThree = new Step({
  id: "stepThree",
  execute: async ({ context: { machineContext } }) => ({
    tripledValue: machineContext.triggerData.inputValue * 3,
  }),
});

const stepFour = new Step({
  id: "stepFour",
  execute: async ({ context: { machineContext } }) => ({
    isEven: machineContext.stepResults.stepThree.payload.tripledValue % 2 === 0,
  }),
});

const myWorkflow = new Workflow({
  name: "my-workflow",
  triggerSchema: z.object({
    inputValue: z.number(),
  }),
});
```

## Chaining and Parallelizing Steps

Now we can add the steps to the workflow. Note the `.then()` method is used to chain the steps, but the `.step()` method is used to add the steps to the workflow.

```ts showLineNumbers copy
myWorkflow
  .step(stepOne)
  .then(stepTwo) // chain one
  .step(stepThree)
  .then(stepFour) // chain two
  .commit();

const result = await myWorkflow.execute({ triggerData: { inputValue: 3 } });
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/workflow-with-parallel-steps"
  }
/>


================================================================================
Source: src/pages/examples/workflows/sequential-steps.mdx
================================================================================

import { GithubLink } from "../../../components/github-link";

# Workflow with Sequential Steps

Workflow can be chained to run one after another in a specific sequence.

## Control Flow Diagram

This example shows how to chain workflow steps by using the `then` method demonstrating how to pass data between sequential steps and execute them in order.

Here's the control flow diagram:

<img
  src="/sequential-chains.png"
  alt="Diagram showing workflow with sequential steps"
  width={600}
/>

## Creating the Steps

Let's start by creating the steps and initializing the workflow.

```ts showLineNumbers copy
import { Step, Workflow } from "@mastra/core";
import { z } from "zod";

const stepOne = new Step({
  id: "stepOne",
  execute: async ({ context: { machineContext } }) => ({
    doubledValue: machineContext.triggerData.inputValue * 2,
  }),
});

const stepTwo = new Step({
  id: "stepTwo",
  execute: async ({ context: { machineContext } }) => ({
    incrementedValue:
      machineContext.stepResults.stepOne.payload.doubledValue + 1,
  }),
});

const stepThree = new Step({
  id: "stepThree",
  execute: async ({ context: { machineContext } }) => ({
    tripledValue:
      machineContext.stepResults.stepTwo.payload.incrementedValue * 3,
  }),
});

// Build the workflow
const myWorkflow = new Workflow({
  name: "my-workflow",
  triggerSchema: z.object({
    inputValue: z.number(),
  }),
});
```

## Chaining the Steps and Executing the Workflow

Now let's chain the steps together.

```ts showLineNumbers copy
// sequential steps
myWorkflow.step(stepOne).then(stepTwo).then(stepThree);

myWorkflow.commit();

const { runId, start } = myWorkflow.createRun();

const res = await start({ triggerData: { inputValue: 90 } });
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={
    "https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/workflow-with-sequential-steps"
  }
/>


================================================================================
Source: src/pages/examples/workflows/suspend-and-resume.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Workflow with Suspend and Resume

Workflow steps can be suspended and resumed at any point in the workflow execution. This example demonstrates how to suspend a workflow step and resume it later.

```ts showLineNumbers copy
import { Step, Workflow } from '@mastra/core';
import { z } from 'zod';

const stepOne = new Step({
  id: 'stepOne',
  outputSchema: z.object({
    doubledValue: z.number(),
  }),
  execute: async ({ context }) => {
    const doubledValue = context.triggerData.inputValue * 2;
    return { doubledValue };
  },
});

const stepTwo = new Step({
  id: 'stepTwo',
  outputSchema: z.object({
    incrementedValue: z.number(),
  }),
  execute: async ({ context, suspend }) => {
    const stepValue = context.stepResults.stepTwo.payload.secondValue || 0;
    
    const incrementedValue = context.stepResults.stepOne.payload.doubledValue + stepValue;

    if (incrementedValue < 100) {
      await suspend();
      return { incrementedValue: 0 };
    }
    return { incrementedValue };
  },
});

// Build the workflow
const myWorkflow = new Workflow({
  name: 'my-workflow',
  triggerSchema: z.object({
    inputValue: z.number(),
  }),
});

// run workflows in parallel
myWorkflow
  .step(stepOne)
  .then(stepTwo)
  .commit();

const { runId, start } = myWorkflow.createRun();

const res = await start({ triggerData: { inputValue: 90 } });

await myWorkflow.watch(runId, async ({ activePaths }) => {
    for (const path of activePaths) {
        const stepTwoStatus = context.stepResults?.stepTwo?.status;
        if (stepTwoStatus === 'suspended') {
            await myWorkflow.resume({
                runId,
                stepId: 'stepTwo',
                context: { secondValue: 100 },
            });
        }
    }
})
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink
  link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/workflow-with-parallel-steps'}
/>


================================================================================
Source: src/pages/examples/workflows/using-a-tool-as-a-step.mdx
================================================================================

import { GithubLink } from '../../../components/github-link';

# Tool as a Workflow step

This example demonstrates how to create and integrate a custom tool as a workflow step, showing how to define input/output schemas and implement the tool's execution logic.

```ts showLineNumbers copy
import { createTool, Workflow } from '@mastra/core';
import { z } from 'zod';

const crawlWebpage = createTool({
  id: 'Crawl Webpage',
  description: 'Crawls a webpage and extracts the text content',
  inputSchema: z.object({
    url: z.string().url(),
  }),
  outputSchema: z.object({
    rawText: z.string(),
  }),
  execute: async ({ context: { url } }) => {
    return { rawText: 'This is the text content of the webpage' };
  },
});

const contentWorkflow = new Workflow({ name: 'content-review' });

contentWorkflow.step(crawlWebpage).commit();

const { runId, start } = contentWorkflow.createRun();

const res = await start();

console.log(res.results);
```

<br />
<br />
<hr className="dark:border-[#404040] border-gray-300" />
<br />
<br />
<GithubLink link={'https://github.com/mastra-ai/mastra/blob/main/examples/basics/workflows/tool-as-workflow-step'} />


================================================================================
Source: src/pages/showcase/index.mdx
================================================================================

---
title: 'Showcase'
description: 'Check out these applications built with Mastra'
---

import { ShowcaseGrid } from '../../components/showcase-grid';

<ShowcaseGrid />
